{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC EDGAR Data Collection - Project Summary\n",
    "\n",
    "**Date:** October 6, 2025  \n",
    "**Objective:** Build a fraud detection system using SEC EDGAR filings to identify anomalies in 10-K Risk Factors and 8-K event disclosures\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "\n",
    "### Goal\n",
    "Develop an AI-powered fraud detection model that analyzes:\n",
    "- **10-K Item 1A (Risk Factors):** Year-over-year changes indicating emerging risks or boilerplate language\n",
    "- **8-K Event Filings:** Critical fraud indicators in Items 4.02, 5.02, 8.01, 2.01\n",
    "\n",
    "### Known Fraud Cases (Baseline)\n",
    "1. **Under Armour (UAA)** - Accounting fraud 2015-2017 (revenue manipulation)\n",
    "2. **Luckin Coffee (LK)** - Accounting fraud 2019-2020 (fabricated sales)\n",
    "3. **Nikola Corporation (NKLA)** - Securities fraud 2019-2020 (false product claims)\n",
    "\n",
    "### Dataset Scope\n",
    "- **Companies:** 23 (20 clean baseline + 3 fraud cases)\n",
    "- **Time Period:** 2015-2024 (10 years)\n",
    "- **Filing Types:** 10-K (annual) + 8-K (event-driven)\n",
    "- **Total Filings:** 4,194 (230 × 10-K, 3,964 × 8-K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Collection Implementation\n",
    "\n",
    "### 2.1 Technology Stack\n",
    "```python\n",
    "# Core libraries\n",
    "import sec_edgar_downloader  # SEC API wrapper with rate limiting\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "```\n",
    "\n",
    "### 2.2 Scrapers Built\n",
    "\n",
    "#### **10-K Scraper** (`src/data/edgar_10k_scraper.py`)\n",
    "- **Purpose:** Download annual 10-K reports for Item 1A risk factor analysis\n",
    "- **Date Range:** 2015-2024 (captures Under Armour fraud period 2015-2017)\n",
    "- **Rate Limiting:** Built-in via `sec-edgar-downloader` (10 req/sec)\n",
    "- **Output:** `data/raw/sec-edgar-filings/{CIK}/10-K/{ACCESSION}/`\n",
    "  - `full-submission.txt` - Complete filing\n",
    "  - `primary-document.html` - Main 10-K document\n",
    "\n",
    "**Results:**\n",
    "- **230 filings** across 23 companies\n",
    "- **Per-company breakdown:**\n",
    "  - Most companies: 11 filings (2015-2024, some filed multiple per year)\n",
    "  - Apple (AAPL): 10 filings\n",
    "  - Alphabet (GOOGL): 10 filings\n",
    "  - Disney (DIS): 6 filings (later IPO)\n",
    "  - Nikola (NKLA): 6 filings (IPO 2020)\n",
    "  - **Luckin Coffee (LK): 0 filings** (delisted after fraud exposure)\n",
    "\n",
    "#### **8-K Scraper** (`src/data/edgar_8k_scraper.py`)\n",
    "- **Purpose:** Download event-driven 8-K filings for fraud indicators\n",
    "- **Target Items:**\n",
    "  - **Item 4.02:** Non-reliance on financials (restatements) - **PRIMARY FRAUD INDICATOR**\n",
    "  - **Item 5.02:** Departure of directors/officers - governance red flag\n",
    "  - **Item 8.01:** Other events (investigations, lawsuits) - regulatory issues\n",
    "  - **Item 2.01:** Acquisition/disposition - M&A anomalies\n",
    "- **Date Range:** 2015-2024\n",
    "- **Output:** `data/raw/sec-edgar-filings/{CIK}/8-K/{ACCESSION}/`\n",
    "\n",
    "**Results:**\n",
    "- **3,964 filings** across 23 companies\n",
    "- **Fraud case coverage:**\n",
    "  - Under Armour (UAA): 141 filings\n",
    "  - Nikola (NKLA): 118 filings\n",
    "  - Luckin Coffee (LK): 0 filings (delisted before 2015)\n",
    "- **Highest volume:** Wells Fargo (869 filings - likely due to 2016 scandal)\n",
    "- **Download errors:** 3 temporary 503 errors (server unavailable), filings skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 File Structure\n",
    "\n",
    "```\n",
    "data/raw/sec-edgar-filings/\n",
    "├── {CIK}/                          # 10-digit company identifier\n",
    "│   ├── 10-K/\n",
    "│   │   └── {ACCESSION_NUMBER}/     # e.g., 0000012927-24-000010\n",
    "│   │       ├── full-submission.txt # Complete HTML/XML filing\n",
    "│   │       └── primary-document.html\n",
    "│   └── 8-K/\n",
    "│       └── {ACCESSION_NUMBER}/\n",
    "│           ├── full-submission.txt\n",
    "│           └── primary-document.html\n",
    "```\n",
    "\n",
    "**Storage:** ~2-3 GB total (uncompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Company Configuration\n",
    "\n",
    "Companies selected for sector diversity and fraud case representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load company list\n",
    "import json\n",
    "\n",
    "with open('../config/companies.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"Total companies: {len(config['companies'])}\\n\")\n",
    "\n",
    "# Show fraud cases\n",
    "fraud_cases = [c for c in config['companies'] if c.get('fraud_case', False)]\n",
    "print(\"Fraud Cases:\")\n",
    "for company in fraud_cases:\n",
    "    print(f\"  - {company['ticker']}: {company['name']} ({company['sector']})\")\n",
    "\n",
    "# Sector breakdown\n",
    "from collections import Counter\n",
    "sectors = Counter(c['sector'] for c in config['companies'])\n",
    "print(\"\\nSector Distribution:\")\n",
    "for sector, count in sectors.most_common():\n",
    "    print(f\"  {sector}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Download Statistics\n",
    "\n",
    "### 3.1 Load Download Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 10-K summary\n",
    "with open('../data/raw/download_summary.json', 'r') as f:\n",
    "    summary_10k = json.load(f)\n",
    "\n",
    "print(\"=== 10-K Download Summary ===\")\n",
    "print(f\"Companies: {summary_10k['companies']}\")\n",
    "print(f\"Target Years: {summary_10k['target_years']}\")\n",
    "print(f\"Total Filings: {summary_10k['total_filings']}\")\n",
    "print(f\"\\nTop 5 by filing count:\")\n",
    "import pandas as pd\n",
    "df_10k = pd.DataFrame(list(summary_10k['by_company'].items()), columns=['Ticker', 'Count'])\n",
    "print(df_10k.sort_values('Count', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 8-K summary\n",
    "with open('../data/raw/download_summary_8k.json', 'r') as f:\n",
    "    summary_8k = json.load(f)\n",
    "\n",
    "print(\"=== 8-K Download Summary ===\")\n",
    "print(f\"Total Filings: {summary_8k['total_filings']}\")\n",
    "print(f\"Critical Items: {summary_8k['critical_items']}\")\n",
    "print(f\"\\nTop 5 by filing count:\")\n",
    "df_8k = pd.DataFrame(list(summary_8k['by_company'].items()), columns=['Ticker', 'Count'])\n",
    "print(df_8k.sort_values('Count', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fraud Case Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_tickers = ['UAA', 'NKLA', 'LK']\n",
    "fraud_periods = {\n",
    "    'UAA': '2015-2017 (accounting fraud - revenue manipulation)',\n",
    "    'NKLA': '2019-2020 (securities fraud - false product claims)',\n",
    "    'LK': '2019-2020 (accounting fraud - fabricated sales, delisted)'\n",
    "}\n",
    "\n",
    "print(\"Fraud Case Filing Coverage:\\n\")\n",
    "for ticker in fraud_tickers:\n",
    "    count_10k = summary_10k['by_company'].get(ticker, 0)\n",
    "    count_8k = summary_8k['by_company'].get(ticker, 0)\n",
    "    print(f\"{ticker} - {fraud_periods[ticker]}\")\n",
    "    print(f\"  10-K: {count_10k} filings\")\n",
    "    print(f\"  8-K: {count_8k} filings\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Next Steps: Extraction & Analysis Pipeline\n",
    "\n",
    "### 4.1 Text Extraction (Week 1-2)\n",
    "Build extractors to parse specific sections:\n",
    "\n",
    "#### **10-K Risk Factor Extractor** (`src/data/risk_extractor.py`)\n",
    "- **Input:** `full-submission.txt` or `primary-document.html`\n",
    "- **Output:** Structured JSON with Item 1A text\n",
    "- **Challenges:**\n",
    "  - Inconsistent HTML/XML formatting across years\n",
    "  - Section headers vary (\"Item 1A\", \"ITEM 1A.\", \"Risk Factors\")\n",
    "  - Nested tables, multi-page risks\n",
    "- **Strategy:**\n",
    "  1. Try regex patterns for section headers\n",
    "  2. Fallback to BeautifulSoup HTML parsing\n",
    "  3. Manual validation on 10 sample filings\n",
    "\n",
    "#### **8-K Item Extractor** (`src/data/item_8k_extractor.py`)\n",
    "- **Input:** 8-K `full-submission.txt`\n",
    "- **Target Items:** 4.02, 5.02, 8.01, 2.01\n",
    "- **Output:** JSON with item type, text, filing date\n",
    "- **Fraud Indicators to Flag:**\n",
    "  - Item 4.02 keywords: \"restatement\", \"non-reliance\", \"material weakness\"\n",
    "  - Item 5.02 keywords: \"resignation\", \"terminated\", \"CFO\", \"CEO\"\n",
    "  - Item 8.01 keywords: \"investigation\", \"SEC\", \"DOJ\", \"subpoena\"\n",
    "\n",
    "### 4.2 Feature Engineering (Week 2-3)\n",
    "- **YoY Risk Factor Changes:**\n",
    "  - Semantic similarity (sentence-transformers)\n",
    "  - New/removed/modified risk detection\n",
    "  - Boilerplate vs. substantive scoring\n",
    "- **8-K Anomaly Scores:**\n",
    "  - Frequency of critical items (Item 4.02 = highest weight)\n",
    "  - Clustering of events (multiple 8-Ks in short period)\n",
    "  - Sentiment analysis on Item 8.01 text\n",
    "\n",
    "### 4.3 Modeling (Week 3-4)\n",
    "- **Baseline Models:**\n",
    "  - Logistic regression on engineered features\n",
    "  - Random forest for feature importance\n",
    "- **Advanced:**\n",
    "  - Fine-tuned BERT for risk factor classification\n",
    "  - Anomaly detection (isolation forest on 8-K patterns)\n",
    "- **Validation:**\n",
    "  - Train on 20 clean companies\n",
    "  - Test on UAA, NKLA (LK likely insufficient data)\n",
    "  - Success metric: Flag fraud cases 1-2 years before public disclosure\n",
    "\n",
    "### 4.4 Dashboard (Week 4)\n",
    "- **Streamlit app** (`dashboard/streamlit_app.py`)\n",
    "- Features:\n",
    "  - Company selector\n",
    "  - Timeline of 8-K filings (color-coded by risk)\n",
    "  - Risk factor diff viewer (YoY changes)\n",
    "  - Fraud probability score\n",
    "  - Download analysis report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. External Datasets Explored (Optional Supplements)\n",
    "\n",
    "### Kaggle & SRAF Datasets\n",
    "- **Limitation:** None include 8-K filings (critical for fraud detection)\n",
    "- **Use Case:** Supplement with parsed financial metrics (balance sheet, income statement)\n",
    "- **Sources:**\n",
    "  1. Kaggle: SEC EDGAR Company Facts (2009-2023)\n",
    "  2. SRAF Notre Dame: 10-X Summaries (1993-2024)\n",
    "\n",
    "**Decision:** Keep scraped data as primary source, optionally download external datasets to `data/external/` for enrichment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Current Status & To-Do\n",
    "\n",
    "### ✅ Completed\n",
    "- [x] Project structure setup\n",
    "- [x] Company selection (23 companies, 3 fraud cases)\n",
    "- [x] 10-K scraper implementation\n",
    "- [x] 8-K scraper implementation\n",
    "- [x] Downloaded 4,194 filings (2015-2024)\n",
    "- [x] Validated fraud case coverage\n",
    "\n",
    "### 🔄 In Progress\n",
    "- [ ] Explore Kaggle datasets for supplementary data\n",
    "\n",
    "### 📋 To-Do\n",
    "1. **Week 1-2: Extraction**\n",
    "   - [ ] Build 10-K Item 1A extractor\n",
    "   - [ ] Build 8-K Item extractor (4.02, 5.02, 8.01, 2.01)\n",
    "   - [ ] Validate extraction on 10 sample filings\n",
    "   - [ ] Store extracted text in `data/processed/`\n",
    "\n",
    "2. **Week 2-3: Feature Engineering**\n",
    "   - [ ] YoY risk factor change detection\n",
    "   - [ ] Boilerplate scoring\n",
    "   - [ ] 8-K anomaly features (frequency, clustering, sentiment)\n",
    "\n",
    "3. **Week 3-4: Modeling**\n",
    "   - [ ] Train baseline classifiers\n",
    "   - [ ] Fine-tune transformer models\n",
    "   - [ ] Validate on UAA, NKLA fraud cases\n",
    "\n",
    "4. **Week 4: Dashboard**\n",
    "   - [ ] Build Streamlit interactive UI\n",
    "   - [ ] Deploy locally for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Technical Notes\n",
    "\n",
    "### SEC API Compliance\n",
    "- **User-Agent Required:** Set in `.env` as `SEC_USER_AGENT`\n",
    "- **Rate Limit:** 10 requests/second (enforced by `sec-edgar-downloader`)\n",
    "- **Retry Logic:** Built-in for 503 errors\n",
    "\n",
    "### Data Quality Issues Encountered\n",
    "1. **Luckin Coffee (LK):** No filings (delisted after fraud exposure in 2020)\n",
    "2. **Disney (DIS):** Only 6 10-Ks (CIK may be incorrect or post-2018 filings)\n",
    "3. **3 × 503 Errors:** Temporary SEC server unavailability during download\n",
    "\n",
    "### Storage Optimization\n",
    "- Exclude `data/raw/` from Git (added to `.gitignore`)\n",
    "- Future: Compress old filings to `.gz` after extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. References\n",
    "\n",
    "### Fraud Cases\n",
    "- **Under Armour:** SEC charges (2021) - https://www.sec.gov/newsroom/press-releases/2021-78\n",
    "- **Luckin Coffee:** SEC settlement (2020) - https://www.sec.gov/newsroom/press-releases/2020-319\n",
    "- **Nikola:** Trevor Milton indictment (2021) - https://www.sec.gov/newsroom/press-releases/2021-141\n",
    "\n",
    "### Technical Resources\n",
    "- SEC EDGAR API: https://www.sec.gov/edgar/sec-api-documentation\n",
    "- `sec-edgar-downloader`: https://github.com/jadchaar/sec-edgar-downloader\n",
    "- SRAF Datasets: https://sraf.nd.edu/sec-edgar-data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
