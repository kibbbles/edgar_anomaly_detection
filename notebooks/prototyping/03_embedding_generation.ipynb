{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Generation - 2024 SEC Filings\n",
    "\n",
    "**Purpose:** Generate embeddings for contextually-chunked 2024 SEC filings\n",
    "\n",
    "**Input:** `output/processed_2024_500tok_contextual.json` (from Step 2)\n",
    "**Output:** `output/embeddings_2024_500tok_contextual.npy` (for RAPTOR clustering)\n",
    "\n",
    "---\n",
    "\n",
    "## Embedding Model: all-MiniLM-L6-v2\n",
    "\n",
    "**Why this model?**\n",
    "\n",
    "### 1. Research-Backed for RAG Systems\n",
    "\n",
    "**RAPTOR Paper (ICLR 2024):**\n",
    "> \"We use Sentence-BERT to generate embeddings for chunks\"\n",
    "- Source: https://arxiv.org/abs/2401.18059\n",
    "- Same model family we're using\n",
    "- Proven for hierarchical retrieval\n",
    "\n",
    "**Original Sentence-BERT Paper (2019):**\n",
    "> Reimers & Gurevych showed SBERT is 10,000x faster than BERT for similarity search\n",
    "- Source: https://arxiv.org/abs/1908.10084\n",
    "- Optimized for semantic similarity tasks\n",
    "- Foundation for modern embedding models\n",
    "\n",
    "**MTEB Benchmark (2022):**\n",
    "> all-MiniLM-L6-v2 ranks in top 20% across 58 embedding tasks\n",
    "- Source: https://arxiv.org/abs/2210.07316\n",
    "- Best performance-to-size ratio\n",
    "- Score: 56.26/100 on MTEB Leaderboard\n",
    "- Leaderboard: https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "### 2. Technical Specifications\n",
    "\n",
    "- **Model:** sentence-transformers/all-MiniLM-L6-v2\n",
    "- **Dimensions:** 384 (compact but effective)\n",
    "- **Parameters:** 22.7M (lightweight)\n",
    "- **Speed:** ~1000 sentences/second on CPU\n",
    "- **Context Window:** 512 tokens (sufficient for our 700-token chunks after subword tokenization)\n",
    "- **Training:** Trained on 1B+ sentence pairs\n",
    "\n",
    "### 3. Why Not Larger Models?\n",
    "\n",
    "**Considered alternatives:**\n",
    "\n",
    "| Model | Dims | Speed | Why Not? |\n",
    "|-------|------|-------|----------|\n",
    "| all-mpnet-base-v2 | 768 | Slower | 2x dimensions, marginal quality gain |\n",
    "| bge-large-en-v1.5 | 1024 | Much slower | 3x dimensions, overkill for our use case |\n",
    "| text-embedding-3-large | 3072 | API cost | OpenAI API = expensive + privacy concerns |\n",
    "\n",
    "**Decision:** all-MiniLM-L6-v2 offers best **speed × quality × cost** tradeoff\n",
    "\n",
    "### 4. Production Usage\n",
    "\n",
    "Used by:\n",
    "- LangChain (default embedding model)\n",
    "- LlamaIndex (recommended for document search)\n",
    "- Pinecone, Weaviate, ChromaDB (documentation examples)\n",
    "- 200M+ downloads on Hugging Face\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Expectations\n",
    "\n",
    "**Data to embed:**\n",
    "- ~2.9M chunks (26K filings × 111 chunks/filing)\n",
    "- Extended text: ~700 tokens per chunk (500 core + 100 context + header)\n",
    "\n",
    "**Estimated time:**\n",
    "- CPU: 1-2 hours (1000 chunks/second)\n",
    "- GPU (if available): 15-30 minutes\n",
    "\n",
    "**Memory usage:**\n",
    "- Model: 80 MB\n",
    "- Working memory: ~2-4 GB\n",
    "- Output embeddings: ~4.4 GB (2.9M × 384 × 4 bytes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Project root\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Paths\n",
    "INPUT_FILE = project_root / 'notebooks' / 'prototyping' / 'output' / 'processed_2024_500tok_contextual.json'\n",
    "OUTPUT_DIR = project_root / 'notebooks' / 'prototyping' / 'output'\n",
    "OUTPUT_FILE = OUTPUT_DIR / 'embeddings_2024_500tok_contextual.npy'\n",
    "\n",
    "print(f\"[INFO] Input file: {INPUT_FILE}\")\n",
    "print(f\"[INFO] Input exists: {INPUT_FILE.exists()}\")\n",
    "print(f\"[INFO] Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "if INPUT_FILE.exists():\n",
    "    file_size_mb = INPUT_FILE.stat().st_size / (1024*1024)\n",
    "    print(f\"[OK] Input file size: {file_size_mb:,.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sentence-transformers if needed\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"[OK] sentence-transformers already installed\")\n",
    "except ImportError:\n",
    "    print(\"[INFO] Installing sentence-transformers...\")\n",
    "    !pip install sentence-transformers\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"[OK] sentence-transformers installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Sentence Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Loading sentence-transformers/all-MiniLM-L6-v2...\")\n",
    "print(\"[INFO] This will download ~80 MB on first run\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load model (will download on first run)\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n[OK] Model loaded in {load_time:.2f} seconds\")\n",
    "print(f\"[INFO] Model details:\")\n",
    "print(f\"  - Model name: all-MiniLM-L6-v2\")\n",
    "print(f\"  - Embedding dimensions: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"  - Max sequence length: {model.max_seq_length} tokens\")\n",
    "print(f\"  - Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Processed Chunks\n",
    "\n",
    "**Important:** We use `text_for_embedding` field (the extended 700-token version with context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[INFO] Loading processed chunks from {INPUT_FILE.name}...\")\n",
    "\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"[OK] Loaded {len(data):,} filings\")\n",
    "\n",
    "# Extract texts for embedding\n",
    "embedding_texts = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for filing in data:\n",
    "    for chunk in filing['chunks']:\n",
    "        # Use 'text_for_embedding' - the extended version with context!\n",
    "        embedding_texts.append(chunk['text_for_embedding'])\n",
    "        \n",
    "        # Store metadata for later (ChromaDB step)\n",
    "        chunk_metadata.append({\n",
    "            'file_name': filing['file_name'],\n",
    "            'company': chunk['metadata']['company'],\n",
    "            'form_type': chunk['metadata']['form_type'],\n",
    "            'filing_date': chunk['metadata']['filing_date'],\n",
    "            'cik': chunk['metadata']['cik'],\n",
    "            'chunk_id': chunk['chunk_id'],\n",
    "            'chunk_index': chunk['metadata']['chunk_index'],\n",
    "            'core_tokens': chunk['metadata']['core_tokens'],\n",
    "            'extended_tokens': chunk['metadata']['extended_tokens']\n",
    "        })\n",
    "\n",
    "print(f\"\\n[OK] Extracted {len(embedding_texts):,} chunks for embedding\")\n",
    "print(f\"[INFO] Using 'text_for_embedding' field (extended with context)\")\n",
    "\n",
    "# Preview first chunk\n",
    "print(f\"\\n[Preview] First chunk text (first 400 chars):\")\n",
    "print(embedding_texts[0][:400])\n",
    "print(f\"\\n[Metadata]:\")\n",
    "print(json.dumps(chunk_metadata[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Embeddings\n",
    "\n",
    "**Process:**\n",
    "1. Batch encode all chunks (batch_size=32 for efficiency)\n",
    "2. Normalize embeddings (important for cosine similarity in clustering)\n",
    "3. Convert to numpy array\n",
    "4. Save to disk\n",
    "\n",
    "**Estimated time:** 1-2 hours for ~2.9M chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*80}\")\n",
    "print(f\"EMBEDDING GENERATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nStarting embedding generation...\")\n",
    "print(f\"  Total chunks: {len(embedding_texts):,}\")\n",
    "print(f\"  Batch size: 32\")\n",
    "print(f\"  Device: {model.device}\")\n",
    "print(f\"  Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nThis will take approximately 1-2 hours...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(\n",
    "    embedding_texts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,  # Important for cosine similarity!\n",
    "    device=None  # Auto-detect (use GPU if available)\n",
    ")\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EMBEDDING GENERATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGeneration time: {embedding_time:.2f} seconds ({embedding_time/60:.2f} minutes)\")\n",
    "print(f\"Speed: {len(embedding_texts) / embedding_time:.2f} chunks/second\")\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
    "print(f\"Expected: [{len(embedding_texts):,}, 384]\")\n",
    "print(f\"\\nEmbedding statistics:\")\n",
    "print(f\"  Min value: {embeddings.min():.6f}\")\n",
    "print(f\"  Max value: {embeddings.max():.6f}\")\n",
    "print(f\"  Mean: {embeddings.mean():.6f}\")\n",
    "print(f\"  Std: {embeddings.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate Embeddings\n",
    "\n",
    "**Quick sanity checks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[INFO] Running validation checks...\\n\")\n",
    "\n",
    "# Check 1: Correct shape\n",
    "assert embeddings.shape == (len(embedding_texts), 384), \"Incorrect embedding shape!\"\n",
    "print(\"[OK] Shape check passed\")\n",
    "\n",
    "# Check 2: No NaN or Inf values\n",
    "assert not np.isnan(embeddings).any(), \"NaN values found in embeddings!\"\n",
    "assert not np.isinf(embeddings).any(), \"Inf values found in embeddings!\"\n",
    "print(\"[OK] No NaN/Inf values\")\n",
    "\n",
    "# Check 3: Embeddings are normalized (L2 norm ≈ 1)\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "assert np.allclose(norms, 1.0, atol=1e-6), \"Embeddings not properly normalized!\"\n",
    "print(f\"[OK] Embeddings normalized (L2 norm = {norms.mean():.6f})\")\n",
    "\n",
    "# Check 4: Test similarity between similar chunks\n",
    "# Compare first chunk with itself (should be ~1.0)\n",
    "similarity = np.dot(embeddings[0], embeddings[0])\n",
    "print(f\"[OK] Self-similarity check: {similarity:.6f} (should be ~1.0)\")\n",
    "\n",
    "# Compare first two chunks (should be high if from same document)\n",
    "similarity_adjacent = np.dot(embeddings[0], embeddings[1])\n",
    "print(f\"[INFO] Adjacent chunk similarity: {similarity_adjacent:.6f}\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] All validation checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[INFO] Saving embeddings to {OUTPUT_FILE}...\")\n",
    "\n",
    "# Save as numpy array\n",
    "np.save(OUTPUT_FILE, embeddings)\n",
    "\n",
    "file_size_mb = OUTPUT_FILE.stat().st_size / (1024*1024)\n",
    "\n",
    "print(f\"[OK] Embeddings saved!\")\n",
    "print(f\"  File: {OUTPUT_FILE.name}\")\n",
    "print(f\"  Size: {file_size_mb:,.2f} MB\")\n",
    "\n",
    "# Also save metadata for convenience (ChromaDB will need this)\n",
    "metadata_file = OUTPUT_DIR / 'chunk_metadata_2024.json'\n",
    "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunk_metadata, f, indent=2)\n",
    "\n",
    "metadata_size_mb = metadata_file.stat().st_size / (1024*1024)\n",
    "print(f\"\\n[OK] Metadata saved!\")\n",
    "print(f\"  File: {metadata_file.name}\")\n",
    "print(f\"  Size: {metadata_size_mb:,.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*80}\")\n",
    "print(f\"EMBEDDING GENERATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nModel:\")\n",
    "print(f\"  Name: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"  Dimensions: 384\")\n",
    "print(f\"  Parameters: 22.7M\")\n",
    "print(f\"  Device: {model.device}\")\n",
    "\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  Input file: {INPUT_FILE.name}\")\n",
    "print(f\"  Total filings: {len(data):,}\")\n",
    "print(f\"  Total chunks: {len(embedding_texts):,}\")\n",
    "print(f\"  Avg chunks/filing: {len(embedding_texts) / len(data):.1f}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Generation time: {embedding_time:.2f} seconds ({embedding_time/60:.2f} minutes)\")\n",
    "print(f\"  Speed: {len(embedding_texts) / embedding_time:.2f} chunks/second\")\n",
    "print(f\"  Speed: {len(embedding_texts) / embedding_time * 60:.0f} chunks/minute\")\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Embeddings file: {OUTPUT_FILE.name}\")\n",
    "print(f\"  Embeddings size: {file_size_mb:,.2f} MB\")\n",
    "print(f\"  Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"  Metadata file: {metadata_file.name}\")\n",
    "print(f\"  Metadata size: {metadata_size_mb:,.2f} MB\")\n",
    "\n",
    "print(f\"\\nStorage breakdown:\")\n",
    "print(f\"  Per-chunk embedding: {384 * 4 / 1024:.2f} KB (384 dims × 4 bytes)\")\n",
    "print(f\"  Total embeddings: {file_size_mb:,.2f} MB\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Option A: Simple RAG - Load into ChromaDB for basic retrieval testing\")\n",
    "print(f\"  2. Option B: RAPTOR - Implement clustering (UMAP + GMM) and summarization\")\n",
    "print(f\"  3. Run experimental comparison: Baseline vs Simple RAG vs RAPTOR RAG\")\n",
    "print(f\"  4. Evaluate with RAGAS framework\")\n",
    "\n",
    "print(f\"\\nResearch citations:\")\n",
    "print(f\"  - Sentence-BERT: https://arxiv.org/abs/1908.10084\")\n",
    "print(f\"  - MTEB Benchmark: https://arxiv.org/abs/2210.07316\")\n",
    "print(f\"  - RAPTOR Paper: https://arxiv.org/abs/2401.18059\")\n",
    "print(f\"  - MTEB Leaderboard: https://huggingface.co/spaces/mteb/leaderboard\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
