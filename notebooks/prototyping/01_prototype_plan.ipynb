{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype Plan: RAPTOR RAG with 2024 SEC Filings\n",
    "\n",
    "## Overview\n",
    "This notebook outlines the step-by-step plan for prototyping the RAPTOR RAG system using:\n",
    "- **Data**: All 2024 SEC 10-K/10-Q filings (~26K filings)\n",
    "- **Models**: Test with both `gpt-oss` (13 GB) and `llama3-sec` (49 GB) via Ollama\n",
    "- **Goal**: Validate complete RAPTOR pipeline before scaling to full 51 GB dataset\n",
    "\n",
    "## Why 2024 Data Only?\n",
    "\n",
    "**Previous approach:** 1,375 sample filings spread across 1993-2024\n",
    "**New approach:** All ~26K filings from 2024\n",
    "\n",
    "**Rationale:**\n",
    "- **More data = better clustering**: 26K filings vs 1,375 samples (19x more data)\n",
    "- **Temporal consistency**: Same regulatory environment, accounting standards, economic conditions\n",
    "- **Better testing**: Can answer \"compare Apple vs Microsoft 2024 risks\" type queries\n",
    "- **Cleaner baseline**: Avoids format/style drift across 30+ years during prototyping\n",
    "- **Statistical robustness**: More documents for RAPTOR hierarchical clustering\n",
    "\n",
    "**Archive location:** Multi-year prototype archived in `archive_v1_multi_year/`\n",
    "\n",
    "---\n",
    "\n",
    "## Prototype Objectives\n",
    "\n",
    "1. **Process all 2024 filings** - Extract, clean, chunk 26K filings\n",
    "2. **Test RAPTOR hierarchical clustering** on substantial dataset\n",
    "3. **Compare model performance** - gpt-oss vs llama3-sec for summarization\n",
    "4. **Verify recursive summarization** quality (3 levels)\n",
    "5. **Build query interface** for retrieving and answering questions\n",
    "6. **Measure performance** (speed, quality, resource usage)\n",
    "7. **Identify issues** before production deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Data Scope\n",
    "\n",
    "**Source:** `data/external/10-X_C_2024.zip`\n",
    "- **Time period:** Full year 2024 (Q1-Q4)\n",
    "- **Total filings:** 26,018\n",
    "- **Compressed size:** 1.6 GB\n",
    "- **Estimated uncompressed:** 5-8 GB\n",
    "- **Form types:** 10-K, 10-Q, 10-K/A, 10-Q/A, 10-QT\n",
    "\n",
    "**Expected processing output:**\n",
    "- **Chunk size:** 500 tokens (validated optimal from FinGPT research)\n",
    "- **Total chunks:** ~2.9M (26K filings × ~111 chunks/filing)\n",
    "- **Embedding storage:** ~17 GB (1536-dim embeddings)\n",
    "- **Processing time:** 1-2 hours for chunking + embedding\n",
    "\n",
    "---\n",
    "\n",
    "## Current Status\n",
    "\n",
    "### ✅ Step 1: Archive Multi-Year Prototype\n",
    "- Moved previous work to `archive_v1_multi_year/`\n",
    "- Renamed files: `01_prototype_plan_multi_year.ipynb`, `02_text_processing_multi_year.ipynb`\n",
    "- Preserved all 12 chunk size outputs (200-8000 tokens)\n",
    "- Key finding validated: 500-1000 tokens optimal for SEC filings\n",
    "\n",
    "### ⏳ Step 2: Text Processing (IN PROGRESS)\n",
    "**Notebook:** `02_text_processing.ipynb`\n",
    "\n",
    "**Status:** Notebook created, ready to run\n",
    "\n",
    "**Tasks:**\n",
    "1. Extract all 26K filings from `10-X_C_2024.zip`\n",
    "2. Parse SRAF-XML format (metadata + clean text)\n",
    "3. Chunk at **500 tokens** with 50 token overlap (10%)\n",
    "4. Add contextual headers (company, CIK, form type, date)\n",
    "5. Export to `output/processed_2024_500tok.json`\n",
    "\n",
    "**Expected output:**\n",
    "- ~2.9M contextual chunks\n",
    "- Avg ~111 chunks per filing\n",
    "- JSON file size: ~6-8 GB\n",
    "\n",
    "**Estimated time:** 30-60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Remaining Pipeline Steps\n",
    "\n",
    "### Step 3: Embedding Generation ⏸️\n",
    "**Notebook:** `03_embedding_generation.ipynb` (to be created)\n",
    "\n",
    "**Tasks:**\n",
    "1. Load processed chunks from Step 2\n",
    "2. Load Sentence Transformers model (`all-MiniLM-L6-v2`)\n",
    "3. Generate embeddings for all ~2.9M chunks\n",
    "4. Store embeddings as NumPy array\n",
    "5. Measure embedding generation time and memory usage\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load processed chunks\n",
    "with open('output/processed_2024_500tok.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract all chunk texts\n",
    "all_chunks = []\n",
    "for filing in data:\n",
    "    for chunk in filing['chunks']:\n",
    "        all_chunks.append(chunk['text'])\n",
    "\n",
    "print(f\"[INFO] Generating embeddings for {len(all_chunks):,} chunks...\")\n",
    "\n",
    "# Generate embeddings in batches\n",
    "batch_size = 1000\n",
    "embeddings = model.encode(\n",
    "    all_chunks,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "# Save embeddings\n",
    "np.save('output/embeddings_2024_500tok.npy', embeddings)\n",
    "print(f\"[OK] Saved embeddings: shape {embeddings.shape}\")\n",
    "```\n",
    "\n",
    "**Output:** \n",
    "- NumPy array: shape `[~2.9M, 384]`\n",
    "- File size: ~4.4 GB\n",
    "\n",
    "**Success Criteria:**\n",
    "- All chunks embedded successfully\n",
    "- Dimension = 384 (all-MiniLM-L6-v2 output)\n",
    "- Generation time < 1 second per chunk\n",
    "- Memory usage < 16 GB during generation\n",
    "\n",
    "**Estimated time:** 1-2 hours (for 2.9M chunks)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: RAPTOR Hierarchical Clustering ⏸️\n",
    "**Notebook:** `04_raptor_clustering.ipynb` (to be created)\n",
    "\n",
    "**Tasks:**\n",
    "1. **Copy RAPTOR class from FinGPT** to `src/models/raptor.py`\n",
    "   - Source: [FinGPT rag.py](https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py)\n",
    "2. Load embeddings from Step 3\n",
    "3. Implement global clustering (UMAP → GMM)\n",
    "4. Implement local clustering within global clusters\n",
    "5. Determine optimal cluster count using BIC\n",
    "6. Visualize clusters (UMAP plot)\n",
    "7. Validate cluster coherence (manual review)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "from src.models.raptor import RaptorRAG\n",
    "import numpy as np\n",
    "\n",
    "# Load embeddings\n",
    "embeddings = np.load('output/embeddings_2024_500tok.npy')\n",
    "\n",
    "# Initialize RAPTOR\n",
    "raptor = RaptorRAG(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    llm_model=\"gpt-oss\",\n",
    "    ollama_host=\"localhost:11434\"\n",
    ")\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "clusters = raptor.cluster_embeddings(\n",
    "    embeddings=embeddings,\n",
    "    dim=10,  # UMAP dimensions\n",
    "    n_neighbors=10,\n",
    "    metric=\"cosine\"\n",
    ")\n",
    "\n",
    "print(f\"[OK] Formed {len(set(clusters))} clusters\")\n",
    "\n",
    "# Save cluster assignments\n",
    "np.save('output/clusters_2024_500tok.npy', clusters)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "- Cluster assignments for each chunk\n",
    "- Cluster metadata (size, topic keywords)\n",
    "- UMAP visualization\n",
    "\n",
    "**Success Criteria:**\n",
    "- Clusters are semantically coherent (manual review of 20+ clusters)\n",
    "- No single dominant cluster (>30% of chunks)\n",
    "- Reasonable cluster count (50-500 for 2.9M chunks)\n",
    "- Clear topic separation in UMAP plot\n",
    "\n",
    "**Estimated time:** 3-4 hours (larger dataset than multi-year)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Recursive Summarization (3 Levels) ⏸️\n",
    "**Notebook:** `05_raptor_summarization.ipynb` (to be created)\n",
    "\n",
    "**Tasks:**\n",
    "1. Load chunks and cluster assignments\n",
    "2. **Test both models:** gpt-oss vs llama3-sec\n",
    "3. Generate **Level 1 summaries** (per-chunk)\n",
    "4. Generate **Level 2 summaries** (cluster-level)\n",
    "5. Generate **Level 3 summaries** (document-level)\n",
    "6. Compare model quality (manual review)\n",
    "7. Measure summarization time for each model\n",
    "\n",
    "**Model Comparison:**\n",
    "\n",
    "| Model | Size | Speed | Quality (Expected) |\n",
    "|-------|------|-------|-------------------|\n",
    "| gpt-oss | 13 GB | Fast (~2-3 sec/chunk) | Good |\n",
    "| llama3-sec | 49 GB | Slower (~5-7 sec/chunk) | Excellent (fine-tuned for SEC) |\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "import ollama\n",
    "import json\n",
    "\n",
    "# Test both models\n",
    "models_to_test = ['gpt-oss', 'llama3-sec']\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n[INFO] Testing {model_name}...\")\n",
    "    \n",
    "    # Level 1: Summarize sample chunks (not all 2.9M - too expensive)\n",
    "    sample_chunks = all_chunks[:1000]  # Test on 1000 chunks\n",
    "    \n",
    "    level1_summaries = []\n",
    "    for i, chunk in enumerate(sample_chunks):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Processing chunk {i}/{len(sample_chunks)}...\")\n",
    "        \n",
    "        prompt = f\"\"\"Summarize this SEC filing excerpt in 2-3 sentences.\n",
    "        Focus on key financial metrics, risks, and business updates.\n",
    "        \n",
    "        Text:\n",
    "        {chunk}\n",
    "        \n",
    "        Summary:\"\"\"\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        level1_summaries.append(response['message']['content'])\n",
    "    \n",
    "    # Save summaries for comparison\n",
    "    with open(f'output/level1_summaries_{model_name}_sample.json', 'w') as f:\n",
    "        json.dump(level1_summaries, f, indent=2)\n",
    "    \n",
    "    print(f\"[OK] {model_name} summaries saved\")\n",
    "```\n",
    "\n",
    "**Output:** \n",
    "- 3-level summary hierarchy for each model\n",
    "- Quality comparison report\n",
    "- Performance metrics (time per chunk)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Level 1 summaries capture chunk essence (no hallucination)\n",
    "- Level 2 summaries coherently integrate themes\n",
    "- Level 3 summaries provide accurate document overview\n",
    "- llama3-sec outperforms gpt-oss in quality (expected)\n",
    "- 90%+ summaries factually accurate (manual review)\n",
    "\n",
    "**Estimated time:** 6-10 hours (testing both models on sample)\n",
    "\n",
    "**Note:** Don't summarize all 2.9M chunks yet - test on subset (1K-10K chunks) first\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Vector Database Setup (ChromaDB) ⏸️\n",
    "**Notebook:** `06_chromadb_setup.ipynb` (to be created)\n",
    "\n",
    "**Tasks:**\n",
    "1. Initialize ChromaDB\n",
    "2. Create collection for 2024 SEC filings\n",
    "3. Store chunks + embeddings + metadata\n",
    "4. Store summaries from chosen model\n",
    "5. Test similarity search\n",
    "6. Benchmark query performance\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "import chromadb\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Initialize ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"../../data/embeddings/chromadb\")\n",
    "\n",
    "# Create collection\n",
    "collection = client.create_collection(\n",
    "    name=\"sec_filings_2024_raptor\",\n",
    "    metadata={\"description\": \"2024 SEC 10-K/10-Q filings with RAPTOR (500 token chunks)\"}\n",
    ")\n",
    "\n",
    "# Load data\n",
    "with open('output/processed_2024_500tok.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "embeddings = np.load('output/embeddings_2024_500tok.npy')\n",
    "clusters = np.load('output/clusters_2024_500tok.npy')\n",
    "\n",
    "# Prepare for ChromaDB (in batches - 2.9M is large)\n",
    "batch_size = 5000\n",
    "chunk_idx = 0\n",
    "\n",
    "for batch_start in range(0, len(all_chunks), batch_size):\n",
    "    batch_end = min(batch_start + batch_size, len(all_chunks))\n",
    "    \n",
    "    batch_docs = all_chunks[batch_start:batch_end]\n",
    "    batch_embeddings = embeddings[batch_start:batch_end]\n",
    "    batch_ids = [f\"chunk_{i}\" for i in range(batch_start, batch_end)]\n",
    "    \n",
    "    # Add batch metadata here...\n",
    "    \n",
    "    collection.add(\n",
    "        embeddings=batch_embeddings.tolist(),\n",
    "        documents=batch_docs,\n",
    "        ids=batch_ids\n",
    "    )\n",
    "    \n",
    "    if batch_start % 50000 == 0:\n",
    "        print(f\"[Progress] Added {batch_end:,} / {len(all_chunks):,} chunks\")\n",
    "\n",
    "print(f\"[OK] Added {len(all_chunks):,} chunks to ChromaDB\")\n",
    "```\n",
    "\n",
    "**Output:** ChromaDB database in `data/embeddings/chromadb/`\n",
    "\n",
    "**Success Criteria:**\n",
    "- All 2.9M chunks stored successfully\n",
    "- Semantic search returns relevant results\n",
    "- Query time < 2 seconds for top-10 retrieval\n",
    "- Metadata correctly attached\n",
    "\n",
    "**Estimated time:** 3-4 hours (large dataset)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7: RAG Query Interface ⏸️\n",
    "**Notebook:** `07_rag_query_interface.ipynb` (to be created)\n",
    "\n",
    "**Tasks:**\n",
    "1. Build complete query pipeline: retrieve → augment → generate\n",
    "2. Implement cluster-aware retrieval (RAPTOR)\n",
    "3. Test with both models (gpt-oss vs llama3-sec)\n",
    "4. Test diverse sample queries\n",
    "5. Evaluate answer quality\n",
    "6. Measure end-to-end latency\n",
    "7. Compare RAPTOR vs simple RAG\n",
    "\n",
    "**Test Queries (2024-specific):**\n",
    "1. \"What are the main risk factors disclosed in 2024 10-Ks?\"\n",
    "2. \"Compare revenue trends across tech companies in 2024.\"\n",
    "3. \"What cybersecurity risks did companies disclose in 2024?\"\n",
    "4. \"How did companies describe AI impacts in their 2024 filings?\"\n",
    "5. \"What supply chain issues were mentioned in Q1 2024?\"\n",
    "6. \"Summarize regulatory risks disclosed in 2024.\"\n",
    "7. \"What are common executive compensation structures in 2024?\"\n",
    "8. \"How did companies discuss inflation in 2024 filings?\"\n",
    "9. \"What climate-related risks were disclosed in 2024?\"\n",
    "10. \"Compare Apple vs Microsoft 2024 risk factors.\"\n",
    "\n",
    "**Success Criteria:**\n",
    "- Answers factually accurate (90%+ on manual review)\n",
    "- Citations reference correct filings\n",
    "- End-to-end query time < 10 seconds\n",
    "- Cluster-aware retrieval improves context\n",
    "- No hallucinations\n",
    "\n",
    "**Estimated time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Metrics to Track\n",
    "\n",
    "### Measure and Record:\n",
    "\n",
    "1. **Data Processing** (Step 2)\n",
    "   - Text extraction: 26K filings\n",
    "   - Chunking time\n",
    "   - Total chunks: ~2.9M\n",
    "   - Output file size\n",
    "\n",
    "2. **Embedding Generation** (Step 3)\n",
    "   - Embeddings per second\n",
    "   - Total embedding time\n",
    "   - Memory usage peak\n",
    "\n",
    "3. **RAPTOR Clustering** (Step 4)\n",
    "   - Clustering time (UMAP + GMM)\n",
    "   - Number of clusters formed\n",
    "   - Cluster size distribution\n",
    "   - Coherence score (manual)\n",
    "\n",
    "4. **Summarization** (Step 5)\n",
    "   - **gpt-oss:** Time per chunk, quality score\n",
    "   - **llama3-sec:** Time per chunk, quality score\n",
    "   - Model comparison results\n",
    "   - Winner for production use\n",
    "\n",
    "5. **Query Performance** (Step 7)\n",
    "   - Retrieval time (ChromaDB)\n",
    "   - LLM generation time (both models)\n",
    "   - End-to-end latency\n",
    "   - Answer quality (1-5 scoring)\n",
    "   - RAPTOR vs simple RAG comparison\n",
    "\n",
    "### Expected Performance:\n",
    "- Total processing time: 8-15 hours for 26K filings\n",
    "- Query response time: < 10 seconds\n",
    "- Memory usage: < 32 GB RAM during processing\n",
    "\n",
    "---\n",
    "\n",
    "## Validation Checklist\n",
    "\n",
    "### Before Moving to Full Dataset:\n",
    "\n",
    "**Data Quality**\n",
    "- [ ] All 26K filings processed successfully (>95%)\n",
    "- [ ] Metadata correctly parsed\n",
    "- [ ] Chunks maintain semantic coherence\n",
    "- [ ] No major data quality issues\n",
    "\n",
    "**RAPTOR System**\n",
    "- [ ] Clustering produces interpretable topics\n",
    "- [ ] Level 1-3 summaries accurate and useful\n",
    "- [ ] Hierarchical structure adds value\n",
    "- [ ] Cluster coherence validated\n",
    "\n",
    "**RAG Pipeline**\n",
    "- [ ] ChromaDB stores/retrieves correctly\n",
    "- [ ] Similarity search returns relevant chunks\n",
    "- [ ] LLM generates accurate answers\n",
    "- [ ] Citations work correctly\n",
    "\n",
    "**Performance**\n",
    "- [ ] Query latency acceptable (< 10 sec)\n",
    "- [ ] Memory usage within limits (< 32 GB)\n",
    "- [ ] No crashes or errors\n",
    "- [ ] System stable under load\n",
    "\n",
    "**Quality**\n",
    "- [ ] 20+ query responses verified accurate\n",
    "- [ ] Edge cases tested (long filings, unusual formats)\n",
    "- [ ] RAPTOR outperforms simple RAG\n",
    "- [ ] Model comparison complete (choose winner)\n",
    "\n",
    "---\n",
    "\n",
    "## Model Selection Decision\n",
    "\n",
    "**After Step 5, choose production model:**\n",
    "\n",
    "**If gpt-oss is sufficient:**\n",
    "- Pros: Faster (2-3 sec/chunk), smaller (13 GB), good quality\n",
    "- Cons: Not fine-tuned for SEC filings\n",
    "- Use case: Quick prototyping, resource-constrained\n",
    "\n",
    "**If llama3-sec is better:**\n",
    "- Pros: Fine-tuned for SEC, excellent quality, better understanding\n",
    "- Cons: Slower (5-7 sec/chunk), larger (49 GB), more resources\n",
    "- Use case: Production deployment, best quality needed\n",
    "\n",
    "**Decision criteria:**\n",
    "1. Manual quality review (10+ summaries each model)\n",
    "2. Accuracy on test queries\n",
    "3. Hallucination rate\n",
    "4. Speed vs quality tradeoff\n",
    "5. Available compute resources\n",
    "\n",
    "---\n",
    "\n",
    "## Success Definition\n",
    "\n",
    "**Prototype is successful if:**\n",
    "\n",
    "1. **End-to-end pipeline runs** on 26K 2024 filings (>95% success rate)\n",
    "2. **RAPTOR clustering coherent** (manual validation of 30+ clusters)\n",
    "3. **Summaries accurate** at all 3 levels (90%+ accuracy)\n",
    "4. **Query responses correct** and well-cited (85%+ accuracy)\n",
    "5. **Performance acceptable** (< 10 sec query, < 32 GB RAM)\n",
    "6. **Clear model winner** identified for production\n",
    "7. **Measurable improvement** over simple RAG\n",
    "\n",
    "**If successful → proceed to full dataset (51 GB) + EC2 deployment**\n",
    "\n",
    "**If issues found → iterate on 2024 data until resolved**\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps After Prototype\n",
    "\n",
    "### If Prototype Succeeds:\n",
    "\n",
    "1. **Document lessons learned**\n",
    "   - Optimal parameters (chunk size: 500, UMAP dims, etc.)\n",
    "   - Winning model (gpt-oss vs llama3-sec)\n",
    "   - Performance characteristics\n",
    "   - Known issues and workarounds\n",
    "\n",
    "2. **Containerize pipeline**\n",
    "   - Ollama container with chosen model\n",
    "   - RAPTOR API (FastAPI)\n",
    "   - ChromaDB container\n",
    "   - Open WebUI\n",
    "   - Docker Compose setup\n",
    "\n",
    "3. **Prepare for EC2 deployment**\n",
    "   - Provision r6i.4xlarge (128 GB RAM, 16 vCPUs)\n",
    "   - Attach 500 GB EBS volume\n",
    "   - Security group configuration\n",
    "   - GPU instance if needed for embeddings\n",
    "\n",
    "4. **Scale to full dataset**\n",
    "   - Process all 51 GB (1993-2024)\n",
    "   - Generate embeddings for full dataset\n",
    "   - Full RAPTOR clustering\n",
    "   - Production ChromaDB setup\n",
    "\n",
    "5. **Deploy to production**\n",
    "   - Launch Docker containers on EC2\n",
    "   - Configure Open WebUI\n",
    "   - Set up monitoring\n",
    "   - Create backup strategy\n",
    "\n",
    "### Timeline:\n",
    "- **Step 2 (Processing):** 30-60 min\n",
    "- **Step 3 (Embeddings):** 1-2 hours\n",
    "- **Step 4 (Clustering):** 3-4 hours\n",
    "- **Step 5 (Summarization):** 6-10 hours\n",
    "- **Step 6 (ChromaDB):** 3-4 hours\n",
    "- **Step 7 (Query Interface):** 3-4 hours\n",
    "- **Validation:** 1-2 days\n",
    "- **Total:** 3-4 days for prototype\n",
    "\n",
    "**Then:**\n",
    "- **Docker Setup:** 2-3 days\n",
    "- **EC2 Deployment:** 1 week\n",
    "- **Full Dataset Processing:** 2-3 weeks\n",
    "\n",
    "**Total time to production: 4-5 weeks**\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Key Files:\n",
    "- `02_text_processing.ipynb` - Text extraction and chunking (ready to run)\n",
    "- `archive_v1_multi_year/` - Previous multi-year prototype\n",
    "- `data/external/10-X_C_2024.zip` - 2024 SEC filings source\n",
    "\n",
    "### To Be Created:\n",
    "- `03_embedding_generation.ipynb`\n",
    "- `04_raptor_clustering.ipynb`\n",
    "- `05_raptor_summarization.ipynb`\n",
    "- `06_chromadb_setup.ipynb`\n",
    "- `07_rag_query_interface.ipynb`\n",
    "- `src/models/raptor.py`\n",
    "\n",
    "### References:\n",
    "- [FinGPT RAPTOR](https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py)\n",
    "- [ChromaDB Docs](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Ollama Python](https://github.com/ollama/ollama-python)\n",
    "\n",
    "---\n",
    "\n",
    "**Status:** ✅ Archive Complete | ⏳ Ready for Step 2 (Text Processing)\n",
    "\n",
    "**Last Updated:** 2025-10-15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
