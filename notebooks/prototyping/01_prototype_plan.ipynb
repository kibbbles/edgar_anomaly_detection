{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Production Deployment: RAPTOR RAG with SEC EDGAR Filings (2024 Complete)\n\n## Overview\nProduction RAPTOR RAG system deployment on AWS EC2:\n- **Data**: All SEC 10-K/10-Q filings for 2024 (26,014 files) ‚úÖ CHUNKED\n- **Infrastructure**: AWS EC2 t3.xlarge (8 vCPUs, 64 GB RAM) - All processing and deployment\n- **Models**: Sentence Transformers for embeddings, Ollama for LLM queries\n- **Goal**: Production-ready RAG system with hierarchical retrieval via Open WebUI\n\n---\n\n## Current Status (2025-10-24)\n\n### ‚úÖ Infrastructure Deployed\n**AWS EC2 Instance \"secAI\":**\n- **Instance Type**: t3.xlarge\n- **vCPUs**: 8\n- **RAM**: 64 GB\n- **OS**: Ubuntu 24.04 LTS\n- **Public IP**: 35.175.134.36\n- **SSH Access**: Configured (key-based auth)\n- **Data Directory**: `/app/data/`\n\n**Deployed Services:**\n- Docker: edgar-chunking image (8.4 GB)\n- Open WebUI: Configured for deployment\n- Security groups: SSH access configured\n\n### ‚úÖ 2024 Data Chunking COMPLETE\n\n**EC2 File Structure:**\n```\n/app/data/\n‚îú‚îÄ‚îÄ edgar/extracted/2024/\n‚îÇ   ‚îú‚îÄ‚îÄ QTR1/ (6,337 .txt files)   ‚úÖ CHUNKED\n‚îÇ   ‚îú‚îÄ‚îÄ QTR2/ (7,247 .txt files)   ‚úÖ CHUNKED\n‚îÇ   ‚îú‚îÄ‚îÄ QTR3/ (6,248 .txt files)   ‚úÖ CHUNKED\n‚îÇ   ‚îî‚îÄ‚îÄ QTR4/ (6,182 .txt files)   ‚úÖ CHUNKED\n‚îÇ\n‚îú‚îÄ‚îÄ processed/2024/\n‚îÇ   ‚îú‚îÄ‚îÄ QTR1/ (6,337 JSON files)   ‚úÖ COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ QTR2/ (7,247 JSON files)   ‚úÖ COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ QTR3/ (6,248 JSON files)   ‚úÖ COMPLETE\n‚îÇ   ‚îî‚îÄ‚îÄ QTR4/ (6,182 JSON files)   ‚úÖ COMPLETE\n‚îÇ\n‚îî‚îÄ‚îÄ embeddings/\n    ‚îî‚îÄ‚îÄ test/ (next: 3 test files)\n```\n\n**2024 Processing Results:**\n\n| Quarter | Files | Chunks | Tokens | Status |\n|---------|-------|--------|--------|--------|\n| Q1 | 6,337 | 1,235,886 | 616,372,446 | ‚úÖ Complete |\n| Q2 | 7,247 | 584,914 | 290,670,736 | ‚úÖ Complete |\n| Q3 | 6,248 | 522,716 | 259,802,386 | ‚úÖ Complete |\n| Q4 | 6,182 | 498,682 | 247,809,543 | ‚úÖ Complete |\n| **TOTAL** | **26,014** | **2,842,198** | **1,414,655,111** | ‚úÖ Complete |\n\n**Processing completed:** 2025-10-24 using Docker container `edgar-chunking`\n**Total processing time:** ~4 hours (sequential quarterly processing)\n\n---\n\n## Next Phase: Embedding Generation\n\n### Test Files (Q4 2024):\n1. `20241024_10-Q_edgar_data_1318605_0001628280-24-043486.txt`\n2. `20241030_10-Q_edgar_data_789019_0000950170-24-118967.txt`\n3. `20241101_10-K_edgar_data_320193_0000320193-24-000123.txt`\n\n### Embedding Model: `multi-qa-mpnet-base-dot-v1` (768-dim)\n\n**Selection Rationale:**\n- **High-dimensional (768)** for precise retrieval of exact wording\n- **Trained for Q&A** tasks - perfect for \"find X in filings\" queries\n- **Preserves jargon** - financial/legal term distinctions maintained\n- **No overfitting concerns** - pre-trained model, inference only\n\n**Why NOT lower-dimensional models:**\n- 384-dim (`all-MiniLM-L6-v2`): Loses nuance needed for legal/financial precision\n- Use case requires exact wording retrieval, not general semantic similarity\n- 2x storage cost (8.6GB vs 4.3GB) worth the quality improvement\n\n### Storage Impact:\n- **2.8M chunks √ó 768 dims √ó 4 bytes = ~8.6 GB**\n- Acceptable for EC2 EBS volume\n- Enables precise retrieval for complex financial queries\n\n---\n\n## Implementation Pipeline\n\n### Phase 1: Data Processing ‚úÖ COMPLETE\n1. ‚úÖ Extract all 2024 filings from ZIP archives\n2. ‚úÖ Process with Docker chunking container\n3. ‚úÖ 500-token chunks using tiktoken\n4. ‚úÖ Metadata extraction (CIK, company, form, date)\n5. ‚úÖ JSON output: 26,014 files with 2.8M chunks\n\n### Phase 2: Embedding Generation üîÑ IN PROGRESS\n1. Create `embedding_generator.py` script\n2. Test on 3 files (validation)\n3. Scale to full 2024 (26,014 files)\n4. Store in `/app/data/embeddings/2024/`\n\n### Phase 3: RAPTOR Implementation ‚è∏Ô∏è\n1. Hierarchical clustering (UMAP + GMM)\n2. Recursive summarization (3 levels via Ollama)\n3. ChromaDB setup and ingestion\n4. Cluster validation\n\n### Phase 4: Deployment ‚è∏Ô∏è\n1. Open WebUI integration\n2. ChromaDB retrieval pipeline\n3. LLM query interface\n4. End-to-end testing\n\n---\n\n## Technical Specifications\n\n### Chunking Strategy (Completed)\n- **Core chunk:** 500 tokens (tiktoken)\n- **No contextual window:** Direct chunking for baseline\n- **Metadata:** CIK, company name, form type, filing date\n- **Rationale:** Simpler baseline; can add contextual embeddings later if needed\n\n### Embedding Model (Selected)\n- **Model:** sentence-transformers/multi-qa-mpnet-base-dot-v1\n- **Dimensions:** 768\n- **Parameters:** 420M\n- **Training:** Question-answering tasks\n- **Similarity:** Dot-product (faster than cosine)\n\n### EC2 Infrastructure\n- **Instance:** t3.xlarge\n- **RAM:** 64 GB\n- **Storage:** EBS volume at `/app/data/`\n- **Docker:** edgar-chunking (8.4 GB image)\n\n---\n\n## Research Citations\n\n**Embedding Selection:**\n- **Sentence-BERT (2019):** https://arxiv.org/abs/1908.10084\n- **MPNet (2020):** https://arxiv.org/abs/2004.09297\n- **MTEB Benchmark (2022):** https://arxiv.org/abs/2210.07316\n\n**RAPTOR:**\n- **RAPTOR Paper (2024):** https://arxiv.org/abs/2401.18059\n\n**Tools:**\n- **ChromaDB:** https://docs.trychroma.com/\n- **Ollama:** https://ollama.com/\n- **Open WebUI:** https://github.com/open-webui/open-webui\n\n---\n\n**Last Updated:** 2025-10-24\n\n**Status:** üîÑ Chunking Complete ‚Üí Starting Embedding Generation"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}