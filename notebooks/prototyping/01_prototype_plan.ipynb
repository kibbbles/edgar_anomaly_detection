{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Prototype Plan: RAPTOR RAG with Sample Data + gpt-oss\n\n## Overview\nThis notebook outlines the step-by-step plan for prototyping the RAPTOR RAG system using:\n- **Sample data**: 1,375 SEC 10-K/10-Q filings (already processed)\n- **Model**: gpt-oss (13 GB) running locally via Ollama\n- **Goal**: Validate the complete RAPTOR pipeline before scaling to full 51 GB dataset\n\n---\n\n## Prototype Objectives\n\n1. ✅ **Data processing pipeline validated** - 1,375 filings processed (see `02_text_processing.ipynb`)\n2. **Test RAPTOR hierarchical clustering** on sample documents\n3. **Verify recursive summarization** quality with gpt-oss\n4. **Build query interface** for retrieving and answering questions\n5. **Measure performance** (speed, quality, resource usage)\n6. **Identify issues** before production deployment\n\n---\n\n## Current Status\n\n### ✅ Steps 1-2: COMPLETED (see `02_text_processing.ipynb`)\n\n**What's been done:**\n- ✅ Processed 1,375 sample SEC filings from `eda/samples/`\n- ✅ Extracted metadata (CIK, company name, filing date, form type)\n- ✅ Cleaned text (removed SRAF wrappers, HTML/XML tags)\n- ✅ Tested **12 different chunk sizes** (200-8000 tokens)\n- ✅ Created contextual chunks with document metadata\n- ✅ Exported 12 JSON files (one per chunk size) + comparison CSV\n\n**Key Results:**\n- Total files: 1,375 filings\n- Chunk sizes tested: 200, 300, 400, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 8000 tokens\n- FinGPT optimal range confirmed: 500-1000 tokens\n- Files available: `output/processed_samples_{size}tok.json`\n\n**Sample Statistics (500 tokens - FinGPT optimal):**\n- Total chunks: 153,207\n- Avg chunks/filing: 111.4\n- Storage requirement: 897.70 MB (embeddings)\n\n**Next decision:** Choose optimal chunk size(s) for remainder of pipeline (Step 3 onwards)\n\n---\n\n## Remaining Pipeline Steps\n\n### Step 3: Embedding Generation ⏳ NEXT\n**Notebook**: `03_embedding_generation.ipynb` (to be created)\n\n**Tasks**:\n1. Select chunk size(s) for embedding (recommended: 500, 1000, 2000 tokens)\n2. Load Sentence Transformers model (`all-MiniLM-L6-v2`)\n3. Generate embeddings for selected chunk size(s)\n4. Store embeddings with chunk IDs\n5. Measure embedding generation time and memory usage\n\n**Implementation**:\n```python\nfrom sentence_transformers import SentenceTransformer\nimport json\nimport numpy as np\nfrom pathlib import Path\n\n# Load model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Load processed chunks (choose chunk size)\nchunk_size = 500  # or 1000, 2000\nwith open(f'output/processed_samples_{chunk_size}tok.json', 'r') as f:\n    data = json.load(f)\n\n# Extract all chunk texts\nall_chunks = []\nfor filing in data:\n    for chunk in filing['chunks']:\n        all_chunks.append(chunk['text'])\n\n# Generate embeddings\nprint(f\"Generating embeddings for {len(all_chunks)} chunks...\")\nembeddings = model.encode(all_chunks, show_progress_bar=True)\n\n# Save embeddings\nnp.save(f'output/embeddings_{chunk_size}tok.npy', embeddings)\nprint(f\"Saved embeddings: shape {embeddings.shape}\")\n```\n\n**Output**: NumPy array with embeddings (shape: [num_chunks, 384])\n\n**Success Criteria**:\n- Embeddings generated for all chunks\n- Dimension = 384 (all-MiniLM-L6-v2 output size)\n- Generation time < 1 second per chunk\n- Memory usage < 8 GB during generation\n\n**Estimated time**: 30-60 minutes (for 153K chunks at 500 tokens)\n\n---\n\n### Step 4: RAPTOR Hierarchical Clustering ⏸️\n**Notebook**: `04_raptor_clustering.ipynb` (to be created)\n\n**Tasks**:\n1. **Copy RAPTOR class from FinGPT** to `src/models/raptor.py`\n   - Source: https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n2. Load embeddings from Step 3\n3. Implement global clustering (UMAP → GMM)\n4. Implement local clustering within global clusters\n5. Determine optimal cluster count using BIC (Bayesian Information Criterion)\n6. Visualize clusters (t-SNE or UMAP plot)\n7. Validate cluster coherence (manually review sample chunks from each cluster)\n\n**Implementation**:\n```python\nfrom src.models.raptor import RaptorRAG\nimport numpy as np\n\n# Load embeddings\nembeddings = np.load('output/embeddings_500tok.npy')\n\n# Initialize RAPTOR\nraptor = RaptorRAG(\n    embedding_model=\"all-MiniLM-L6-v2\",\n    llm_model=\"gpt-oss\",\n    ollama_host=\"localhost:11434\"\n)\n\n# Perform hierarchical clustering\nclusters = raptor.cluster_embeddings(\n    embeddings=embeddings,\n    dim=10,  # UMAP dimensions\n    n_neighbors=10,  # UMAP neighbors\n    metric=\"cosine\"  # Distance metric\n)\n\nprint(f\"Formed {len(set(clusters))} clusters\")\n\n# Analyze cluster distribution\nfrom collections import Counter\ncluster_counts = Counter(clusters)\nprint(f\"Cluster sizes: min={min(cluster_counts.values())}, \"\n      f\"max={max(cluster_counts.values())}, \"\n      f\"avg={sum(cluster_counts.values())/len(cluster_counts):.1f}\")\n```\n\n**Output**: \n- Cluster assignments for each chunk\n- Cluster metadata (size, topic keywords)\n- Visualization plots\n\n**Success Criteria**:\n- Clusters are semantically coherent (manual review of 10+ clusters)\n- No single dominant cluster (>50% of chunks)\n- Cluster count reasonable (10-100 clusters for 153K chunks)\n- Clear topic separation visible in UMAP plot\n\n**Estimated time**: 2-3 hours\n\n---\n\n### Step 5: Recursive Summarization (3 Levels) ⏸️\n**Notebook**: `05_raptor_summarization.ipynb` (to be created)\n\n**Tasks**:\n1. Load chunks and cluster assignments from Steps 3-4\n2. Generate **Level 1 summaries** (per-chunk summaries using gpt-oss)\n3. Generate **Level 2 summaries** (cluster-level summaries)\n4. Generate **Level 3 summaries** (document-level summaries)\n5. Test summarization quality (manual review of samples)\n6. Measure summarization time and gpt-oss performance\n\n**Implementation**:\n```python\nimport ollama\nimport json\n\n# Initialize Ollama client\nollama_client = ollama.Client()\n\n# Level 1: Summarize individual chunks\ndef summarize_chunk(chunk_text):\n    prompt = f\"\"\"Summarize the following SEC filing excerpt in 2-3 sentences.\n    Focus on key financial metrics, risks, and business updates.\n    \n    Text:\n    {chunk_text}\n    \n    Summary:\"\"\"\n    \n    response = ollama_client.chat(\n        model=\"gpt-oss\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response['message']['content']\n\n# Process all chunks\nlevel1_summaries = []\nfor i, chunk in enumerate(all_chunks):\n    if i % 100 == 0:\n        print(f\"Processing chunk {i}/{len(all_chunks)}...\")\n    summary = summarize_chunk(chunk)\n    level1_summaries.append(summary)\n\n# Save Level 1 summaries\nwith open('output/level1_summaries_500tok.json', 'w') as f:\n    json.dump(level1_summaries, f, indent=2)\n\n\n# Level 2: Summarize clusters\ndef summarize_cluster(cluster_summaries):\n    combined = \"\\n\\n\".join(cluster_summaries[:10])  # Limit to 10 chunks per cluster\n    prompt = f\"\"\"Create a cohesive summary integrating these related SEC filing sections.\n    \n    Sections:\n    {combined}\n    \n    Integrated summary:\"\"\"\n    \n    response = ollama_client.chat(\n        model=\"gpt-oss\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response['message']['content']\n\n# Group summaries by cluster\ncluster_groups = {}\nfor idx, cluster_id in enumerate(clusters):\n    if cluster_id not in cluster_groups:\n        cluster_groups[cluster_id] = []\n    cluster_groups[cluster_id].append(level1_summaries[idx])\n\n# Summarize each cluster\nlevel2_summaries = {}\nfor cluster_id, summaries in cluster_groups.items():\n    print(f\"Summarizing cluster {cluster_id} ({len(summaries)} chunks)...\")\n    level2_summaries[cluster_id] = summarize_cluster(summaries)\n\n# Save Level 2 summaries\nwith open('output/level2_summaries_500tok.json', 'w') as f:\n    json.dump(level2_summaries, f, indent=2)\n\n\n# Level 3: Document-level summary\ndef summarize_document(filing_data):\n    # Get all cluster summaries for this filing\n    relevant_clusters = [level2_summaries[cid] for cid in filing_data['cluster_ids']]\n    combined = \"\\n\\n\".join(relevant_clusters)\n    \n    prompt = f\"\"\"Create a high-level executive summary of this SEC filing.\n    \n    Key themes:\n    {combined}\n    \n    Executive summary:\"\"\"\n    \n    response = ollama_client.chat(\n        model=\"gpt-oss\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response['message']['content']\n\n# Generate Level 3 summaries for each filing\nlevel3_summaries = []\nfor filing in data:\n    filing_summary = summarize_document(filing)\n    level3_summaries.append({\n        'file_name': filing['file_name'],\n        'company': filing['metadata']['company'],\n        'summary': filing_summary\n    })\n\n# Save Level 3 summaries\nwith open('output/level3_summaries_500tok.json', 'w') as f:\n    json.dump(level3_summaries, f, indent=2)\n```\n\n**Output**: 3-level summary hierarchy stored in JSON files\n\n**Success Criteria**:\n- Level 1 summaries capture chunk essence without hallucination\n- Level 2 summaries coherently integrate cluster themes\n- Level 3 summary provides accurate document overview\n- Summarization time < 5 seconds per chunk with gpt-oss\n- Manual review: 90%+ summaries are factually accurate\n\n**Estimated time**: 4-6 hours (depending on gpt-oss speed for 153K chunks)\n\n**Note**: This is the most time-consuming step. Consider processing subset first (e.g., 100 filings) to validate approach.\n\n---\n\n### Step 6: Vector Database Setup (ChromaDB) ⏸️\n**Notebook**: `06_chromadb_setup.ipynb` (to be created)\n\n**Tasks**:\n1. Install and initialize ChromaDB\n2. Create collection for SEC filings\n3. Store chunks + embeddings + metadata\n4. Store all 3 levels of summaries as additional documents\n5. Test similarity search\n6. Benchmark query performance\n\n**Implementation**:\n```python\nimport chromadb\nfrom chromadb.config import Settings\nimport json\nimport numpy as np\n\n# Initialize ChromaDB\nclient = chromadb.PersistentClient(path=\"../../data/embeddings/chromadb\")\n\n# Create collection\ncollection = client.create_collection(\n    name=\"sec_filings_raptor_500tok\",\n    metadata={\"description\": \"SEC 10-K/10-Q filings with RAPTOR summaries (500 token chunks)\"}\n)\n\n# Load data\nwith open('output/processed_samples_500tok.json', 'r') as f:\n    data = json.load(f)\n\nembeddings = np.load('output/embeddings_500tok.npy')\nclusters = np.load('output/clusters_500tok.npy')\n\nwith open('output/level1_summaries_500tok.json', 'r') as f:\n    level1_summaries = json.load(f)\n\n# Prepare documents and metadata\ndocuments = []\nmetadatas = []\nids = []\n\nchunk_idx = 0\nfor filing in data:\n    for chunk in filing['chunks']:\n        documents.append(chunk['text'])\n        metadatas.append({\n            'company': filing['metadata']['company'],\n            'cik': filing['metadata']['cik'],\n            'form_type': filing['metadata']['form_type'],\n            'filing_date': filing['metadata']['filing_date'],\n            'file_name': filing['file_name'],\n            'chunk_index': chunk['chunk_id'],\n            'cluster_id': int(clusters[chunk_idx]),\n            'level1_summary': level1_summaries[chunk_idx]\n        })\n        ids.append(f\"chunk_{chunk_idx}\")\n        chunk_idx += 1\n\n# Add to ChromaDB (in batches for efficiency)\nbatch_size = 1000\nfor i in range(0, len(documents), batch_size):\n    print(f\"Adding batch {i//batch_size + 1}/{len(documents)//batch_size + 1}...\")\n    batch_end = min(i + batch_size, len(documents))\n    \n    collection.add(\n        embeddings=embeddings[i:batch_end].tolist(),\n        documents=documents[i:batch_end],\n        metadatas=metadatas[i:batch_end],\n        ids=ids[i:batch_end]\n    )\n\nprint(f\"Added {len(documents)} chunks to ChromaDB\")\n\n# Test query\nresults = collection.query(\n    query_texts=[\"What are the company's main risks?\"],\n    n_results=5\n)\n\nprint(\"Top 5 results:\")\nfor i, doc in enumerate(results['documents'][0]):\n    meta = results['metadatas'][0][i]\n    print(f\"{i+1}. {meta['company']} ({meta['form_type']}) - {meta['filing_date']}\")\n    print(f\"   {doc[:200]}...\")\n```\n\n**Output**: ChromaDB database in `data/embeddings/chromadb/`\n\n**Success Criteria**:\n- All 153K chunks stored successfully in ChromaDB\n- Semantic search returns relevant results (manual verification)\n- Query time < 1 second for top-5 retrieval\n- Metadata correctly attached to all chunks\n\n**Estimated time**: 1-2 hours\n\n---\n\n### Step 7: RAG Query Interface ⏸️\n**Notebook**: `07_rag_query_interface.ipynb` (to be created)\n\n**Tasks**:\n1. Build complete query pipeline: retrieve → augment → generate\n2. Implement cluster-aware retrieval (RAPTOR's key differentiator)\n3. Test with diverse sample queries\n4. Evaluate answer quality (manual review)\n5. Measure end-to-end latency\n6. Compare RAPTOR vs. simple RAG (baseline)\n\n**Implementation**:\n```python\nimport chromadb\nimport ollama\nimport json\n\n# Load ChromaDB\nclient = chromadb.PersistentClient(path=\"../../data/embeddings/chromadb\")\ncollection = client.get_collection(name=\"sec_filings_raptor_500tok\")\n\n# Load cluster summaries\nwith open('output/level2_summaries_500tok.json', 'r') as f:\n    cluster_summaries = json.load(f)\n\ndef query_raptor(query, top_k=5, use_cluster_context=True):\n    \"\"\"\n    RAPTOR RAG query with cluster-aware retrieval\n    \"\"\"\n    # Step 1: Retrieve relevant chunks from ChromaDB\n    results = collection.query(\n        query_texts=[query],\n        n_results=top_k\n    )\n    \n    retrieved_chunks = results['documents'][0]\n    metadata = results['metadatas'][0]\n    \n    # Step 2: Get cluster summaries for retrieved chunks (RAPTOR advantage)\n    if use_cluster_context:\n        cluster_ids = list(set([m['cluster_id'] for m in metadata]))\n        cluster_context = [cluster_summaries[str(cid)] for cid in cluster_ids]\n    \n    # Step 3: Build context (chunks + cluster summaries)\n    context_parts = []\n    \n    # Add retrieved chunks\n    context_parts.append(\"[Retrieved Excerpts]\")\n    for i, chunk in enumerate(retrieved_chunks[:3]):  # Use top 3 chunks\n        meta = metadata[i]\n        context_parts.append(f\"\\nSource: {meta['company']} ({meta['form_type']}) {meta['filing_date']}\")\n        context_parts.append(chunk[:1000])  # Limit chunk size\n    \n    # Add cluster summaries (RAPTOR's hierarchical context)\n    if use_cluster_context:\n        context_parts.append(\"\\n\\n[Thematic Context from Clusters]\")\n        for summary in cluster_context[:2]:  # Use top 2 cluster summaries\n            context_parts.append(summary)\n    \n    context = \"\\n\".join(context_parts)\n    \n    # Step 4: Generate answer with gpt-oss\n    prompt = f\"\"\"You are analyzing SEC filings. Use the context below to answer the question.\n    Provide citations to specific companies and filing dates.\n    \n    Context:\n    {context}\n    \n    Question: {query}\n    \n    Answer:\"\"\"\n    \n    response = ollama.chat(\n        model=\"gpt-oss\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    return {\n        \"answer\": response['message']['content'],\n        \"sources\": [\n            {\n                \"company\": m['company'],\n                \"form_type\": m['form_type'],\n                \"filing_date\": m['filing_date'],\n                \"file_name\": m['file_name']\n            }\n            for m in metadata\n        ],\n        \"retrieved_chunks\": retrieved_chunks,\n        \"cluster_context\": cluster_context if use_cluster_context else []\n    }\n\n# Test queries\ntest_queries = [\n    \"What are the main risk factors disclosed in these filings?\",\n    \"Summarize revenue trends across companies.\",\n    \"What cybersecurity risks are mentioned?\",\n    \"How do companies describe their competitive positions?\",\n    \"What legal proceedings are disclosed?\",\n    \"What are the main environmental and regulatory risks?\",\n    \"How have companies disclosed their intellectual property?\",\n    \"What supply chain risks are mentioned?\",\n    \"Describe compensation structures for executives.\",\n    \"What customer concentration risks exist?\"\n]\n\nprint(\"Testing RAPTOR RAG Query Interface\\n\")\nprint(\"=\"*80)\n\nfor i, query in enumerate(test_queries, 1):\n    print(f\"\\n[Query {i}] {query}\")\n    print(\"-\"*80)\n    \n    result = query_raptor(query, top_k=5)\n    \n    print(f\"Answer: {result['answer'][:300]}...\")\n    print(f\"\\nSources ({len(result['sources'])} filings):\")\n    for source in result['sources'][:3]:\n        print(f\"  - {source['company']} ({source['form_type']}) {source['filing_date']}\")\n    print(\"=\"*80)\n```\n\n**Test Queries** (Diverse types):\n1. \"What are the main risk factors disclosed in these filings?\"\n2. \"Summarize revenue trends across companies.\"\n3. \"What cybersecurity risks are mentioned?\"\n4. \"How do companies describe their competitive positions?\"\n5. \"What legal proceedings are disclosed?\"\n6. \"What are the main environmental and regulatory risks?\"\n7. \"How have companies disclosed their intellectual property?\"\n8. \"What supply chain risks are mentioned?\"\n9. \"Describe compensation structures for executives.\"\n10. \"What customer concentration risks exist?\"\n\n**Output**: Query results with answers, sources, and context\n\n**Success Criteria**:\n- Answers are factually accurate (90%+ accuracy on manual review)\n- Citations correctly reference source filings (company, form, date)\n- End-to-end query time < 10 seconds\n- Cluster-aware retrieval provides better context than simple RAG (A/B test)\n- No hallucinations (facts not in source documents)\n\n**Estimated time**: 2-3 hours\n\n---\n\n## Performance Metrics to Track\n\n### Measure and Record:\n\n1. **Data Processing** ✅ (Complete)\n   - Text extraction: 1,375 filings processed\n   - Chunking: 12 sizes tested (200-8000 tokens)\n   - Total chunks: 153,207 (at 500 tokens)\n\n2. **Embedding Generation** ⏳\n   - Embeddings per second\n   - Total embedding time\n   - Memory usage peak\n\n3. **RAPTOR Clustering** ⏸️\n   - UMAP + GMM clustering time\n   - Number of clusters formed\n   - Cluster size distribution\n   - Coherence score (manual)\n\n4. **Summarization** ⏸️\n   - Time per chunk summary (Level 1)\n   - Time per cluster summary (Level 2)\n   - Time per document summary (Level 3)\n   - Total summarization time\n   - gpt-oss tokens used\n\n5. **Query Performance** ⏸️\n   - Retrieval time (ChromaDB)\n   - LLM generation time (gpt-oss)\n   - End-to-end query latency\n   - Answer quality (manual scoring 1-5)\n   - RAPTOR vs simple RAG comparison\n\n### Expected Performance (Sample Data):\n- Total processing time: 8-12 hours for 1,375 filings (most time in summarization)\n- Query response time: < 10 seconds\n- Memory usage: < 16 GB RAM during processing\n\n---\n\n## Validation Checklist\n\n### Before Moving to Full Dataset:\n\n**Data Quality** ✅\n- [x] Text extraction works across different filing formats (HTML/XML/SGML)\n- [x] Metadata correctly parsed for all sample files\n- [x] Chunks maintain semantic coherence (12 sizes tested)\n\n**RAPTOR System** ⏸️\n- [ ] Clustering produces interpretable topic groups\n- [ ] Level 1-3 summaries are accurate and useful\n- [ ] Hierarchical structure provides value over flat retrieval\n\n**RAG Pipeline** ⏸️\n- [ ] ChromaDB stores and retrieves embeddings correctly\n- [ ] Similarity search returns relevant chunks\n- [ ] gpt-oss generates accurate, citation-backed answers\n\n**Performance** ⏸️\n- [ ] Query latency acceptable (< 10 seconds)\n- [ ] Memory usage within limits (< 16 GB for sample data)\n- [ ] No crashes or errors during processing\n\n**Quality** ⏸️\n- [ ] Manually verify 10+ query responses for accuracy\n- [ ] Test edge cases (very long filings, unusual formats)\n- [ ] Compare RAPTOR vs. simple RAG on same queries (ablation study)\n\n---\n\n## Issues to Watch For\n\n### Common Problems:\n\n1. **Chunking Issues** ✅ (Resolved in Step 2)\n   - Tested 12 chunk sizes to find optimal\n   - 10% overlap preserves context\n   - Contextual headers add document metadata\n\n2. **Clustering Problems** ⏸️\n   - Too many/too few clusters → Use BIC to optimize\n   - Incoherent cluster themes → Manual review and iterate\n   - Single dominant cluster → Adjust UMAP parameters\n\n3. **Summarization Quality** ⏸️\n   - Hallucinations (gpt-oss inventing facts) → Add \"only use provided text\" to prompts\n   - Missing key information → Test different prompt templates\n   - Summaries too generic → Adjust prompt specificity\n\n4. **Retrieval Accuracy** ⏸️\n   - Irrelevant chunks retrieved → Test different embedding models\n   - Missing obvious relevant content → Check embedding quality\n   - Poor semantic matching → Consider query expansion\n\n5. **Performance Bottlenecks** ⏸️\n   - Slow embedding generation → Use GPU if available\n   - Slow LLM summarization → Consider batching or faster model\n   - High memory usage → Process in smaller batches\n\n---\n\n## Success Definition\n\n**Prototype is successful if:**\n\n1. **End-to-end pipeline runs without errors** on 1,375 sample filings\n2. **RAPTOR clustering produces coherent themes** (manual validation of 20+ clusters)\n3. **Summaries accurately capture content** at all 3 levels (90%+ accuracy)\n4. **Query responses are factually correct** and well-cited (80%+ accuracy)\n5. **Performance is acceptable** (< 10 sec query time, < 16 GB RAM)\n6. **System provides measurable value** over simple keyword search or basic RAG\n\n**If successful → proceed to Phase 4 (full dataset + EC2 deployment)**\n\n**If issues found → iterate on prototype until resolved**\n\n---\n\n## Next Steps After Prototype\n\n### If Prototype Succeeds:\n\n1. **Document lessons learned** and optimal parameters\n   - Optimal chunk size: 500-1000 tokens (to be confirmed)\n   - RAPTOR clustering params (UMAP dim, GMM threshold)\n   - gpt-oss prompt templates that work best\n   \n2. **Set up Docker Compose** for local development\n   - Ollama container with gpt-oss\n   - RAPTOR API container (FastAPI)\n   - Open WebUI container\n   \n3. **Containerize the pipeline** (Ollama + RAPTOR API + WebUI)\n   - Create Dockerfile for RAPTOR service\n   - Create docker-compose.dev.yml\n   \n4. **Test Docker setup** with sample data\n   - Ensure reproducibility\n   - Validate performance in containers\n   \n5. **Prepare for EC2 deployment** (provision instance, configure)\n   - Provision r6i.4xlarge (128 GB RAM)\n   - Attach 500 GB EBS volume\n   \n6. **Scale to full 51 GB dataset** on EC2\n   - Process all filings (not just samples)\n   - Generate embeddings for full dataset\n   \n7. **Deploy llama3-sec** for production quality\n   - Pull 49 GB llama3-sec model\n   - Re-run summarization with llama3-sec\n   - Compare quality vs gpt-oss\n\n### Timeline:\n- **Step 3 (Embeddings)**: 1-2 hours\n- **Step 4 (Clustering)**: 2-3 hours\n- **Step 5 (Summarization)**: 4-6 hours (most time-consuming)\n- **Step 6 (ChromaDB)**: 1-2 hours\n- **Step 7 (Query Interface)**: 2-3 hours\n- **Validation & Iteration**: 1-2 days\n- **Total for Steps 3-7**: 2-3 days\n\n**Then:**\n- **Docker Setup**: 2-3 days\n- **EC2 Deployment**: 1 week\n- **Full Dataset Processing**: 1-2 weeks\n\n**Total estimated time: 3-4 weeks from now to production**\n\n---\n\n## Resources\n\n### Key Files:\n- ✅ `02_text_processing.ipynb` - Data extraction and chunking (COMPLETE)\n- ✅ `output/processed_samples_{size}tok.json` - 12 chunk size variants\n- ✅ `output/chunk_size_comparison.csv` - Chunk size statistics\n- ⏸️ `src/models/raptor.py` - RAPTOR implementation (to be created)\n- ⏸️ `data/embeddings/chromadb/` - Vector database (to be created)\n\n### Available Data:\n- 1,375 processed SEC filings\n- 12 chunk size variants (200-8000 tokens)\n- Optimal range identified: 500-1000 tokens per FinGPT research\n\n### References:\n- FinGPT RAPTOR: https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n- ChromaDB Docs: https://docs.trychroma.com/\n- Sentence Transformers: https://www.sbert.net/\n- Ollama Python: https://github.com/ollama/ollama-python\n\n---\n\n**Status**: ✅ Steps 1-2 Complete | ⏳ Ready for Step 3 (Embedding Generation)\n\n**Last Updated**: 2025-10-14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}