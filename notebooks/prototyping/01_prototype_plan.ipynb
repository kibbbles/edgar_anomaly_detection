{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Prototype Plan: RAPTOR RAG with 2024 SEC Filings\n\n## Overview\nThis notebook outlines the step-by-step plan for prototyping the RAPTOR RAG system using:\n- **Data**: All 2024 SEC 10-K/10-Q filings (26,014 filings processed)\n- **Models**: Test with both `gpt-oss` (13 GB) and another model via Ollama\n- **Goal**: Validate complete RAPTOR pipeline before scaling to full 51 GB dataset\n\n## Why 2024 Data Only?\n\n**Previous approach:** 1,375 sample filings spread across 1993-2024\n**New approach:** All 26K filings from 2024\n\n**Rationale:**\n- **More data = better clustering**: 26K filings vs 1,375 samples (19x more data)\n- **Temporal consistency**: Same regulatory environment, accounting standards, economic conditions\n- **Better testing**: Can answer \"compare Apple vs Microsoft 2024 risks\" type queries\n- **Cleaner baseline**: Avoids format/style drift across 30+ years during prototyping\n- **Statistical robustness**: More documents for RAPTOR hierarchical clustering\n\n**Archive location:** Multi-year prototype archived in `archive_v1_multi_year/`\n\n---\n\n## Prototype Objectives\n\n1. ✅ **Process all 2024 filings** - Extract, clean, chunk 26K filings (COMPLETE)\n2. **Test RAPTOR hierarchical clustering** on substantial dataset\n3. **Compare model performance** for summarization\n4. **Verify recursive summarization** quality (3 levels)\n5. **Build query interface** for retrieving and answering questions\n6. **Measure performance** (speed, quality, resource usage)\n7. **Identify issues** before production deployment\n\n---\n\n## Data Scope\n\n**Source:** `data/external/10-X_C_2024.zip`\n- **Time period:** Full year 2024 (Q1-Q4)\n- **Total filings:** 26,014 (processed successfully)\n- **Compressed size:** 1,611.88 MB\n- **Form types:** 10-K, 10-Q, 10-K/A, 10-Q/A, 10-QT\n\n**Actual processing output:**\n- **Chunk size:** 500 tokens core + 100 token context (Anthropic method)\n- **Total chunks:** 2,725,171 (2.7M)\n- **Avg chunks/filing:** 104.8\n- **Output file:** `processed_2024_500tok_contextual.json` (14,957 MB)\n- **Processing time:** 42.1 minutes\n\n---\n\n## Current Status\n\n### ✅ Step 1: Archive Multi-Year Prototype\n- Moved previous work to `archive_v1_multi_year/`\n- Renamed files: `01_prototype_plan_multi_year.ipynb`, `02_text_processing_multi_year.ipynb`\n- Preserved all 12 chunk size outputs (200-8000 tokens)\n- Key finding validated: 500-1000 tokens optimal for SEC filings\n\n### ✅ Step 2: Text Processing (COMPLETE)\n**Notebook:** `02_text_processing.ipynb`\n\n**Status:** Successfully completed on 2025-10-16\n\n**Results:**\n1. ✅ Extracted 26,014 filings from `10-X_C_2024.zip` (100% success rate, 0 errors)\n2. ✅ Parsed SRAF-XML format (metadata + clean text)\n3. ✅ Chunked at **500 tokens** with **contextual chunking** (100 token context window)\n4. ✅ Added contextual headers (company, CIK, form type, date)\n5. ✅ Exported to `output/processed_2024_500tok_contextual.json`\n\n**Actual output:**\n- **Total chunks: 2,725,171** (2.7M chunks)\n- **Avg chunks/filing: 104.8**\n- **JSON file size: 14,957 MB (~15 GB)**\n- **Processing time: 42.1 minutes**\n- **Processing rate: 10.3 files/second**\n\n**Token statistics:**\n- Total document tokens: 1,356,067,955 (1.36 billion)\n- Core tokens (stored): 1,356,067,955\n- Extended tokens (embedded): 1,625,920,116 (1.63 billion)\n- Context overhead: 19.9% (very efficient!)\n- Avg tokens/filing: 52,128\n- Min tokens/filing: 850\n- Max tokens/filing: 1,071,003\n\n**Contextual chunking configuration:**\n- Core chunk: 500 tokens (stored)\n- Context window: 100 tokens (50 before + 50 after)\n- Extended chunk: ~700 tokens (embedded)\n- Research backing: Anthropic Contextual Retrieval (35-49% improvement)\n\n**Key improvements over original plan:**\n- Used contextual chunking instead of simple overlap (Anthropic method)\n- 2.7M chunks vs expected 2.9M (more efficient chunking)\n- No overlap needed (context window provides continuity)\n- Each chunk has both `text` (core) and `text_for_embedding` (extended)\n\n---\n\n## Remaining Pipeline Steps\n\n### Step 3: Embedding Generation ⏸️\n**Notebook:** `03_embedding_generation.ipynb` (CREATED)\n\n**Tasks:**\n1. Load processed chunks from Step 2\n2. Load Sentence Transformers model (`all-MiniLM-L6-v2`)\n3. Generate embeddings for all 2.7M chunks using `text_for_embedding` field\n4. Store embeddings as NumPy array\n5. Measure embedding generation time and memory usage\n\n**Expected output:** \n- NumPy array: shape `[2,725,171, 384]`\n- File size: ~4.2 GB\n- Estimated time: 1-2 hours (for 2.7M chunks)\n\n**Why all-MiniLM-L6-v2:**\n- RAPTOR paper uses Sentence-BERT (same family)\n- Top 20% on MTEB benchmark (56.26/100)\n- 384 dimensions (compact but effective)\n- ~1000 chunks/second on CPU\n- 200M+ downloads, production-proven\n\n---\n\n### Step 4: RAPTOR Hierarchical Clustering ⏸️\n**Notebook:** `04_raptor_clustering.ipynb` (to be created)\n\n**Tasks:**\n1. Load embeddings from Step 3\n2. Implement global clustering (UMAP → GMM)\n3. Implement local clustering within global clusters\n4. Determine optimal cluster count using BIC\n5. Visualize clusters (UMAP plot)\n6. Validate cluster coherence (manual review)\n\n**Expected output:**\n- Cluster assignments for each chunk\n- Cluster metadata (size, topic keywords)\n- UMAP visualization\n\n**Success Criteria:**\n- Clusters are semantically coherent (manual review of 20+ clusters)\n- No single dominant cluster (>30% of chunks)\n- Reasonable cluster count (50-500 for 2.7M chunks)\n- Clear topic separation in UMAP plot\n\n**Estimated time:** 3-4 hours\n\n---\n\n### Step 5: Recursive Summarization (3 Levels) ⏸️\n**Notebook:** `05_raptor_summarization.ipynb` (to be created)\n\n**Tasks:**\n1. Load chunks and cluster assignments\n2. Test model for summarization\n3. Generate **Level 1 summaries** (per-chunk)\n4. Generate **Level 2 summaries** (cluster-level)\n5. Generate **Level 3 summaries** (document-level)\n6. Validate quality (manual review)\n7. Measure summarization time\n\n**Note:** Test on subset (1K-10K chunks) first before scaling to all 2.7M\n\n**Estimated time:** 6-10 hours (testing on sample)\n\n---\n\n### Step 6: Vector Database Setup (ChromaDB) ⏸️\n**Notebook:** `06_chromadb_setup.ipynb` (to be created)\n\n**Tasks:**\n1. Initialize ChromaDB\n2. Create collection for 2024 SEC filings\n3. Store chunks + embeddings + metadata (2.7M chunks)\n4. Store summaries from chosen model\n5. Test similarity search\n6. Benchmark query performance\n\n**Success Criteria:**\n- All 2.7M chunks stored successfully\n- Semantic search returns relevant results\n- Query time < 2 seconds for top-10 retrieval\n- Metadata correctly attached\n\n**Estimated time:** 3-4 hours\n\n---\n\n### Step 7: RAG Query Interface ⏸️\n**Notebook:** `07_rag_query_interface.ipynb` (to be created)\n\n**Tasks:**\n1. Build complete query pipeline: retrieve → augment → generate\n2. Implement cluster-aware retrieval (RAPTOR)\n3. Test with evaluation questions\n4. Evaluate answer quality with RAGAS\n5. Measure end-to-end latency\n6. Compare RAPTOR vs simple RAG\n\n**Test approach:**\n- **Baseline test:** 5 questions, no RAG context\n- **Simple RAG test:** 5 questions, basic retrieval\n- **RAPTOR RAG test:** 5 questions, hierarchical retrieval\n- **Evaluation:** RAGAS metrics (faithfulness, relevancy, precision, recall)\n\n**Success Criteria:**\n- Answers factually accurate (90%+ on manual review)\n- Citations reference correct filings\n- End-to-end query time < 10 seconds\n- RAPTOR shows improvement over simple RAG\n\n**Estimated time:** 3-4 hours\n\n---\n\n## Performance Metrics Achieved\n\n### Step 2 Results (Text Processing):\n- ✅ Filings processed: 26,014 / 26,014 (100%)\n- ✅ Error rate: 0%\n- ✅ Processing time: 42.1 minutes\n- ✅ Rate: 10.3 files/second\n- ✅ Total chunks: 2,725,171\n- ✅ Output size: 14,957 MB\n\n---\n\n## Validation Checklist\n\n### Before Moving to Full Dataset:\n\n**Data Quality**\n- [x] All 26K filings processed successfully (100%)\n- [x] Metadata correctly parsed\n- [x] Chunks maintain semantic coherence\n- [x] No major data quality issues\n\n**RAPTOR System**\n- [ ] Clustering produces interpretable topics\n- [ ] Level 1-3 summaries accurate and useful\n- [ ] Hierarchical structure adds value\n- [ ] Cluster coherence validated\n\n**RAG Pipeline**\n- [ ] ChromaDB stores/retrieves correctly\n- [ ] Similarity search returns relevant chunks\n- [ ] LLM generates accurate answers\n- [ ] Citations work correctly\n\n**Performance**\n- [ ] Query latency acceptable (< 10 sec)\n- [ ] Memory usage within limits (< 32 GB)\n- [ ] No crashes or errors\n- [ ] System stable under load\n\n**Quality**\n- [ ] 5+ query responses verified accurate\n- [ ] Edge cases tested (long filings, unusual formats)\n- [ ] RAPTOR outperforms simple RAG\n- [ ] Model evaluation complete\n\n---\n\n## Success Definition\n\n**Prototype is successful if:**\n\n1. ✅ **End-to-end pipeline runs** on 26K 2024 filings (100% success rate achieved)\n2. [ ] **RAPTOR clustering coherent** (manual validation of 20+ clusters)\n3. [ ] **Summaries accurate** at all 3 levels (90%+ accuracy)\n4. [ ] **Query responses correct** and well-cited (85%+ accuracy)\n5. [ ] **Performance acceptable** (< 10 sec query, < 32 GB RAM)\n6. [ ] **Measurable improvement** over simple RAG\n\n**If successful → proceed to full dataset (51 GB) + EC2 deployment**\n\n**If issues found → iterate on 2024 data until resolved**\n\n---\n\n## Resources\n\n### Completed Files:\n- ✅ `02_text_processing.ipynb` - Text extraction and chunking (DONE)\n- ✅ `03_embedding_generation.ipynb` - Embedding generation notebook (CREATED)\n- ✅ `output/processed_2024_500tok_contextual.json` - Processed chunks (15 GB)\n\n### To Be Created:\n- `04_raptor_clustering.ipynb`\n- `05_raptor_summarization.ipynb`\n- `06_chromadb_setup.ipynb`\n- `07_rag_query_interface.ipynb`\n- `src/models/raptor.py`\n\n### References:\n- [Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)\n- [RAPTOR Paper](https://arxiv.org/abs/2401.18059)\n- [Sentence-BERT Paper](https://arxiv.org/abs/1908.10084)\n- [MTEB Benchmark](https://arxiv.org/abs/2210.07316)\n- [ChromaDB Docs](https://docs.trychroma.com/)\n- [Sentence Transformers](https://www.sbert.net/)\n- [Ollama Python](https://github.com/ollama/ollama-python)\n\n---\n\n**Status:** ✅ Step 2 Complete | ⏳ Ready for Step 3 (Embedding Generation)\n\n**Last Updated:** 2025-10-16"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}