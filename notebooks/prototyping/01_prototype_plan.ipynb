{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Plan: RAPTOR RAG with SEC EDGAR Filings (1993-2024)\n",
    "\n",
    "## Overview\n",
    "This notebook documents the full production RAPTOR RAG system deployment:\n",
    "- **Data**: All SEC 10-K/10-Q filings (1993-2024) - ~51GB total, expanding from initial 2024 prototype\n",
    "- **Infrastructure**: AWS EC2 g6.2xlarge (GPU-accelerated) - All processing, embedding, and deployment\n",
    "- **Models**: Ollama with FinGPT-v3 and other financial LLMs\n",
    "- **Goal**: Production-ready RAG system with hierarchical retrieval via Open WebUI\n",
    "\n",
    "## Evolution: Prototype ‚Üí Production\n",
    "\n",
    "**Initial Prototype Plan (Archived):**\n",
    "- 2024 only (26,014 filings)\n",
    "- Local PC processing ‚Üí AWS EC2 embedding ‚Üí handoff\n",
    "- Test RAPTOR before scaling\n",
    "\n",
    "**Current Production Reality:**\n",
    "- **2021-2024 uploaded** (6.6GB compressed, ~100K+ filings)\n",
    "- **Target: 1993-2024** (full 31-year dataset, ~300K+ filings)\n",
    "- **All operations on AWS EC2** (processing + embedding + RAPTOR + deployment)\n",
    "- **Open WebUI deployed** (currently port 3000 during troubleshooting, target port 8080)\n",
    "\n",
    "---\n",
    "\n",
    "## Current Status (2025-10-17)\n",
    "\n",
    "### ‚úÖ Infrastructure Deployed\n",
    "**AWS EC2 Instance \"secai\":**\n",
    "- **Instance Type**: g6.2xlarge (GPU-accelerated)\n",
    "- **vCPUs**: 8\n",
    "- **RAM**: 32 GB\n",
    "- **GPU**: 1x NVIDIA L4 (24 GB GPU memory)\n",
    "- **OS**: Ubuntu 24.04 LTS\n",
    "- **Public IP**: 35.175.134.36\n",
    "- **SSH Access**: Configured (key-based auth)\n",
    "- **Data Directory**: `/app/data/edgar/`\n",
    "\n",
    "**Deployed Services:**\n",
    "- Open WebUI: Running in Docker (currently port 3000, troubleshooting Ollama connectivity, will move to port 8080)\n",
    "- Security groups: Configured for authorized access (port 8080 already allowed)\n",
    "- Admin account: dev@onyxgs.com (local auth, new users require admin approval)\n",
    "\n",
    "### ‚úÖ Data Uploaded to EC2\n",
    "**Location:** `/app/data/edgar/`\n",
    "- ‚úÖ 10-X_C_2021.zip (1.6GB)\n",
    "- ‚úÖ 10-X_C_2022.zip (1.8GB)\n",
    "- ‚úÖ 10-X_C_2023.zip (1.7GB)\n",
    "- ‚úÖ 10-X_C_2024.zip (1.6GB)\n",
    "- ‚úÖ 2024/ (unzipped, ready for processing)\n",
    "- üîÑ 1993-2020 data (uploading in progress)\n",
    "\n",
    "**Total uploaded:** 6.6GB compressed (~25-30GB uncompressed when all extracted)\n",
    "\n",
    "### ‚úÖ Local Prototype Completed (Reference Only)\n",
    "**Note:** This was done locally for testing and is archived. Production uses AWS EC2.\n",
    "\n",
    "**Completed on local PC:**\n",
    "- ‚úÖ Text processing: 26,014 filings ‚Üí 2.7M chunks (Step 2 complete)\n",
    "- ‚úÖ Output: `processed_2024_500tok_contextual.json` (15GB)\n",
    "- ‚è≥ Embedding generation: Aborted locally (too resource-intensive)\n",
    "\n",
    "**Key learnings from local prototype:**\n",
    "- 500-token chunks optimal for SEC filings\n",
    "- Contextual chunking (Anthropic method) works well\n",
    "- 42 minutes to process 26K filings\n",
    "- 15GB output manageable for transfer\n",
    "\n",
    "---\n",
    "\n",
    "## Production Pipeline (AWS EC2)\n",
    "\n",
    "### Phase 1: Data Processing üîÑ\n",
    "**Status:** Starting\n",
    "\n",
    "**Tasks:**\n",
    "1. Process unzipped 2024 data on EC2\n",
    "2. Run `run_02_processing.py` adapted for EC2 paths\n",
    "3. Generate chunks with contextual embedding\n",
    "4. Repeat for 2021-2023 data\n",
    "5. Scale to 1993-2020 once uploaded\n",
    "\n",
    "**Expected output per year:**\n",
    "- ~26K filings/year √ó 4 years = ~104K filings\n",
    "- ~2.7M chunks/year √ó 4 years = ~11M chunks\n",
    "- ~15GB JSON/year √ó 4 years = ~60GB processed data\n",
    "\n",
    "**Location:** `/app/data/processed/`\n",
    "\n",
    "### Phase 2: Embedding Generation ‚è∏Ô∏è\n",
    "**Status:** Awaiting Phase 1 completion\n",
    "\n",
    "**Tasks:**\n",
    "1. Load processed chunks on EC2\n",
    "2. Run `run_03_embeddings.py` on EC2\n",
    "3. Generate embeddings using sentence-transformers (GPU-accelerated on NVIDIA L4)\n",
    "4. Store embeddings + metadata\n",
    "\n",
    "**Expected output:**\n",
    "- ~11M embeddings (384 dims each)\n",
    "- ~17GB embeddings file (.npy)\n",
    "- Metadata JSON (~2GB)\n",
    "\n",
    "**Performance estimate with GPU:**\n",
    "- NVIDIA L4 24GB GPU should process ~5,000-10,000 chunks/second\n",
    "- Total time for 11M chunks: ~20-40 minutes (vs. 3+ hours on CPU)\n",
    "\n",
    "**Location:** `/app/data/embeddings/`\n",
    "\n",
    "### Phase 3: RAPTOR Implementation ‚è∏Ô∏è\n",
    "**Status:** Awaiting Phase 2\n",
    "\n",
    "**Tasks:**\n",
    "1. Hierarchical clustering (UMAP + GMM)\n",
    "2. Recursive summarization (3 levels via Ollama)\n",
    "3. ChromaDB setup and ingestion\n",
    "4. Cluster validation\n",
    "\n",
    "### Phase 4: Deployment ‚è∏Ô∏è\n",
    "**Status:** Infrastructure ready, awaiting data pipeline\n",
    "\n",
    "**Completed:**\n",
    "- ‚úÖ Open WebUI Docker container deployed\n",
    "- ‚úÖ Port 8080 security rules configured\n",
    "- ‚úÖ Admin account configured (dev@onyxgs.com, password: 644e;1C6ig,o)\n",
    "- üîÑ Currently running on port 3000 while troubleshooting Ollama connection\n",
    "\n",
    "**Remaining:**\n",
    "- Fix Ollama connection to Open WebUI (adjusting Docker run command)\n",
    "- Move to port 8080 once connection working\n",
    "- Load ChromaDB with embeddings + summaries\n",
    "- Configure RAG query pipeline\n",
    "- Test end-to-end queries\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Workflow\n",
    "\n",
    "### Infrastructure & Deployment\n",
    "1. ‚úÖ AWS EC2 g6.2xlarge instance provisioned\n",
    "2. ‚úÖ Open WebUI Docker container deployed\n",
    "3. ‚úÖ Admin account configured\n",
    "4. üîÑ Ollama connection troubleshooting (currently port 3000)\n",
    "5. ‚è∏Ô∏è Finalize on port 8080 once connection stable\n",
    "6. ‚è∏Ô∏è ChromaDB setup and ingestion\n",
    "7. ‚è∏Ô∏è RAG pipeline integration\n",
    "\n",
    "### Data Pipeline\n",
    "1. ‚úÖ Upload 2021-2024 data (6.6GB)\n",
    "2. üîÑ Upload remaining 1993-2020 data (in progress)\n",
    "3. ‚è∏Ô∏è Process all years into chunks\n",
    "4. ‚è∏Ô∏è Generate embeddings for all chunks (GPU-accelerated)\n",
    "5. ‚è∏Ô∏è Implement RAPTOR clustering (UMAP + GMM)\n",
    "6. ‚è∏Ô∏è Recursive summarization via Ollama\n",
    "\n",
    "### Testing & Validation\n",
    "- ‚è∏Ô∏è Query interface testing\n",
    "- ‚è∏Ô∏è Evaluation with RAGAS\n",
    "- ‚è∏Ô∏è Performance benchmarking\n",
    "- ‚è∏Ô∏è Documentation\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Specifications\n",
    "\n",
    "### AWS EC2 Instance\n",
    "- **Type**: g6.2xlarge (GPU-accelerated compute)\n",
    "- **vCPUs**: 8 cores\n",
    "- **RAM**: 32 GB DDR5\n",
    "- **GPU**: 1x NVIDIA L4 Tensor Core GPU\n",
    "  - GPU Memory: 24 GB GDDR6\n",
    "  - CUDA Cores: 7,424\n",
    "  - Tensor Cores: 232 (4th generation)\n",
    "  - FP32 Performance: 30.3 TFLOPS\n",
    "  - Optimized for: AI inference, embedding generation, LLM serving\n",
    "- **OS**: Ubuntu 24.04 LTS\n",
    "- **Location**: `/app/` (project root)\n",
    "- **Network**: Enhanced networking enabled\n",
    "\n",
    "### Chunking Strategy (Proven from Local Prototype)\n",
    "- **Core chunk:** 500 tokens (stored)\n",
    "- **Context window:** 100 tokens (50 before + 50 after)\n",
    "- **Extended chunk:** ~700 tokens (embedded)\n",
    "- **Method:** Anthropic Contextual Retrieval\n",
    "- **Overhead:** 19.9% (efficient)\n",
    "\n",
    "### Embedding Model\n",
    "- **Model:** sentence-transformers/all-MiniLM-L6-v2\n",
    "- **Dimensions:** 384\n",
    "- **Speed (CPU):** ~1000 chunks/second\n",
    "- **Speed (GPU - NVIDIA L4):** ~5,000-10,000 chunks/second (estimated)\n",
    "- **Normalized:** Yes (L2 norm = 1.0)\n",
    "\n",
    "### Infrastructure\n",
    "- **Docker:** Open WebUI + Ollama containers\n",
    "- **Access:** SSH via key-based auth\n",
    "- **Web UI:** Port 8080 (target), currently 3000 during setup, admin: dev@onyxgs.com\n",
    "\n",
    "### Data Scale (Full Production)\n",
    "**When complete (1993-2024):**\n",
    "- **Filings:** ~300,000+ (31 years)\n",
    "- **Chunks:** ~30-40 million\n",
    "- **Embeddings:** ~45-55GB\n",
    "- **Processed data:** ~180-200GB\n",
    "- **ChromaDB:** ~250GB total (with summaries)\n",
    "\n",
    "---\n",
    "\n",
    "## Archived Local Prototype Results\n",
    "\n",
    "### Step 2: Text Processing (Local - Reference Only)\n",
    "**Status:** ‚úÖ Completed locally on 2025-10-16\n",
    "\n",
    "**Results (2024 data only):**\n",
    "- Filings processed: 26,014 / 26,014 (100%)\n",
    "- Total chunks: 2,725,171\n",
    "- Processing time: 42.1 minutes\n",
    "- Output: `processed_2024_500tok_contextual.json` (15GB)\n",
    "\n",
    "**Token statistics:**\n",
    "- Total tokens: 1.36 billion (core)\n",
    "- Extended tokens: 1.63 billion (for embedding)\n",
    "- Context overhead: 19.9%\n",
    "- Avg tokens/filing: 52,128\n",
    "\n",
    "**Note:** This local prototype validated the approach. Production processing happens on AWS EC2 for all years.\n",
    "\n",
    "---\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "### Phase 1-2 (Processing + Embedding)\n",
    "- [ ] All 1993-2024 data processed successfully\n",
    "- [ ] Embeddings generated for all chunks\n",
    "- [ ] No data loss or corruption\n",
    "- [ ] Reasonable processing time (<1 week total)\n",
    "- [ ] GPU utilization optimized (>80% during embedding generation)\n",
    "\n",
    "### Phase 3 (RAPTOR)\n",
    "- [ ] Clustering produces coherent topics\n",
    "- [ ] 3-level summarization accurate\n",
    "- [ ] Hierarchical structure adds value\n",
    "- [ ] Manual validation passes\n",
    "\n",
    "### Phase 4 (Deployment)\n",
    "- [ ] Open WebUI connects to Ollama\n",
    "- [ ] Service running on port 8080\n",
    "- [ ] ChromaDB retrieval works correctly\n",
    "- [ ] Query latency < 10 seconds\n",
    "- [ ] System stable under load\n",
    "\n",
    "### Overall System\n",
    "- [ ] Answers factually accurate (90%+)\n",
    "- [ ] RAPTOR outperforms simple RAG\n",
    "- [ ] Citations reference correct filings\n",
    "- [ ] RAGAS evaluation scores high\n",
    "\n",
    "---\n",
    "\n",
    "## Next Immediate Steps\n",
    "\n",
    "1. **Infrastructure:**\n",
    "   - Fix Ollama connection to Open WebUI (adjust Docker run command)\n",
    "   - Verify Docker containers healthy\n",
    "   - Move to port 8080 once stable\n",
    "   - Verify GPU accessibility for embedding generation\n",
    "\n",
    "2. **Data Processing:**\n",
    "   - Complete upload of 1993-2020 data\n",
    "   - Transfer `run_02_processing.py` to EC2\n",
    "   - Modify paths for `/app/data/edgar/2024/`\n",
    "   - Run processing on 2024 data (already unzipped)\n",
    "   - Monitor and validate output\n",
    "\n",
    "3. **GPU Optimization:**\n",
    "   - Install CUDA drivers and toolkit\n",
    "   - Configure PyTorch/TensorFlow for GPU\n",
    "   - Test GPU-accelerated embedding generation\n",
    "   - Benchmark performance vs. CPU\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Completed Files (Local Prototype):\n",
    "- ‚úÖ `02_text_processing.ipynb` - Local processing (archived)\n",
    "- ‚úÖ `run_02_processing.py` - Script (to be adapted for EC2)\n",
    "- ‚úÖ `03_embedding_generation.ipynb` - Embedding notebook\n",
    "- ‚úÖ `run_03_embeddings.py` - Script (to be run on EC2 with GPU)\n",
    "\n",
    "### To Be Created:\n",
    "- `04_raptor_clustering.ipynb`\n",
    "- `05_raptor_summarization.ipynb`\n",
    "- `06_chromadb_setup.ipynb`\n",
    "- `07_rag_query_interface.ipynb`\n",
    "\n",
    "### References:\n",
    "- [Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)\n",
    "- [RAPTOR Paper](https://arxiv.org/abs/2401.18059)\n",
    "- [Sentence-BERT Paper](https://arxiv.org/abs/1908.10084)\n",
    "- [MTEB Benchmark](https://arxiv.org/abs/2210.07316)\n",
    "- [ChromaDB Docs](https://docs.trychroma.com/)\n",
    "- [Ollama](https://ollama.com/)\n",
    "- [Open WebUI](https://github.com/open-webui/open-webui)\n",
    "- [AWS g6 Instances](https://aws.amazon.com/ec2/instance-types/g6/)\n",
    "- [NVIDIA L4 GPU](https://www.nvidia.com/en-us/data-center/l4/)\n",
    "\n",
    "---\n",
    "\n",
    "**Status:** üîÑ Production Deployment In Progress\n",
    "\n",
    "**Last Updated:** 2025-10-17\n",
    "\n",
    "**Key Change:** Transitioned from prototype to full production deployment on AWS EC2 g6.2xlarge with GPU acceleration for embedding generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
