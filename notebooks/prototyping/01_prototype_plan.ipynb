{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Production Deployment: RAPTOR RAG with SEC EDGAR Filings (2024 Complete)\n\n## Overview\nProduction RAPTOR RAG system deployment on AWS EC2:\n- **Data**: All SEC 10-K/10-Q filings for 2024 (26,014 files)\n- **Infrastructure**: AWS EC2 t3.xlarge (8 vCPUs, 64 GB RAM) - All processing and deployment\n- **Models**: Sentence Transformers for embeddings, Ollama for LLM queries\n- **Goal**: Production-ready RAG system with hierarchical retrieval via Open WebUI\n\n---\n\n## Current Status (2025-10-24)\n\n### ✅ Infrastructure Deployed\n**AWS EC2 Instance \"secAI\":**\n- **Instance Type**: t3.xlarge\n- **vCPUs**: 8\n- **RAM**: 64 GB\n- **OS**: Ubuntu 24.04 LTS\n- **Public IP**: 35.175.134.36\n- **SSH Access**: Configured (key-based auth)\n- **Data Directory**: `/app/data/`\n\n**Deployed Services:**\n- Docker: edgar-chunking image (8.4 GB)\n- Open WebUI: Configured for deployment\n- Security groups: SSH access configured\n\n### ✅ 2024 Data Chunking COMPLETE (Q1)\n\n**Method:** NVIDIA's 15% overlap chunking (no LLM context generation)\n**Rationale:** Anthropic's LLM method too slow (~11 days for all 2024); NVIDIA's overlap method fast (~4 hours total) with good retrieval quality\n\n**EC2 File Structure:**\n```\n/app/data/\n├── edgar/extracted/2024/\n│   ├── QTR1/ (6,337 .txt files)   ✅ READY\n│   ├── QTR2/ (7,247 .txt files)   ✅ READY\n│   ├── QTR3/ (6,248 .txt files)   ✅ READY\n│   └── QTR4/ (6,182 .txt files)   ✅ READY\n│\n├── processed/2024/\n│   ├── QTR1/ (6,337 .json files)  ✅ CHUNKED (15% overlap)\n│   ├── QTR2/                      ⏸️ Pending\n│   ├── QTR3/                      ⏸️ Pending\n│   └── QTR4/                      ⏸️ Pending\n│\n└── embeddings/\n    ├── test_q1/                   ✅ TEST (3 companies, 286 chunks)\n    │   ├── embeddings.parquet     (1.5 MB - 286 rows × 768 dims)\n    │   └── metadata.parquet       (3.3 KB - 286 rows)\n    ├── 2024/\n    │   ├── QTR1/                  ⏸️ Next (after test validation)\n    │   ├── QTR2/                  ⏸️ Pending\n    │   ├── QTR3/                  ⏸️ Pending\n    │   └── QTR4/                  ⏸️ Pending\n```\n\n**2024 Processing Progress:**\n\n| Quarter | Files | Chunks (est.) | Chunking Status | Embedding Status |\n|---------|-------|---------------|-----------------|------------------|\n| Q1 | 6,337 | ~108,000 | ✅ Complete | 🔄 Test (3 files) |\n| Q2 | 7,247 | ~123,000 | ⏸️ Pending | ⏸️ Pending |\n| Q3 | 6,248 | ~106,000 | ⏸️ Pending | ⏸️ Pending |\n| Q4 | 6,182 | ~105,000 | ⏸️ Pending | ⏸️ Pending |\n| **TOTAL** | **26,014** | **~442,000** | **Q1 done** | **Test done** |\n\n**Test Embedding Results (Q1 - 3 companies):**\n- **Companies**: Tesla (CIK 1318605), Microsoft (CIK 789019), Apple (CIK 320193)\n- **Files**: Tesla 10-K (171 chunks), Microsoft 10-Q (87 chunks), Apple 10-Q (28 chunks)\n- **Total**: 286 chunks embedded\n- **Time**: 2min 40sec\n- **Output**: `/app/data/embeddings/test_q1/` (embeddings.parquet + metadata.parquet)\n- **Performance**: ~95 chunks/min\n- **Estimated Q1 full**: ~19 hours for 108,000 chunks\n\n---\n\n## Implementation Details\n\n### Chunking Strategy: NVIDIA's 15% Overlap Method\n\n**Configuration:**\n- **Core chunk**: 500 tokens (tiktoken `cl100k_base`)\n- **Overlap**: 15% = 75 tokens\n- **Step size**: 425 tokens (500 - 75)\n- **Format**: JSON per filing with chunks array\n\n**Why 15% overlap?**\n- Research-backed: NVIDIA papers recommend 10-20% overlap for optimal retrieval\n- No LLM overhead: Anthropic's contextual method too slow (11 days vs 4 hours)\n- Good retrieval: Overlap provides context without requiring LLM summarization\n- Fast processing: ~4 hours for all 26,014 files\n\n**Example chunk overlap:**\n```\nChunk 0: tokens 0-499    (500 tokens)\nChunk 1: tokens 425-924  (500 tokens, 75 token overlap with Chunk 0)\nChunk 2: tokens 850-1349 (500 tokens, 75 token overlap with Chunk 1)\n```\n\n### Embedding Model: `multi-qa-mpnet-base-dot-v1` (768-dim)\n\n**Selection Rationale:**\n- **High-dimensional (768)** for precise retrieval of exact wording\n- **Trained for Q&A** tasks - perfect for \"find X in filings\" queries\n- **Preserves jargon** - financial/legal term distinctions maintained\n- **Dot-product similarity** - faster than cosine similarity\n- **No overfitting concerns** - pre-trained model, inference only\n\n**Why NOT lower-dimensional models:**\n- 384-dim (`all-MiniLM-L6-v2`): Loses nuance needed for legal/financial precision\n- Use case requires exact wording retrieval, not general semantic similarity\n- Storage cost acceptable for precision gain\n\n**Parquet Storage Format:**\n- **embeddings.parquet**: N rows × 768 columns (one row per chunk)\n- **metadata.parquet**: N rows × 2 columns (file_name, chunk_id)\n- **Why Parquet?**: Columnar format, efficient compression, vector DB ready\n- **Not individual JSON files**: 1 consolidated file for all chunks per quarter\n\n### Storage Estimates:\n- **Chunked JSON**: 26,014 files × ~1 MB avg = ~26 GB\n- **Embeddings**: 442,000 chunks × 768 dims × 4 bytes = ~1.4 GB (compressed in Parquet)\n- **Total**: ~27.4 GB for all 2024 data\n- **EC2 EBS**: Sufficient capacity\n\n---\n\n## Implementation Pipeline\n\n### Phase 1: Data Processing ✅ Q1 COMPLETE, Q2-Q4 PENDING\n1. ✅ Extract all 2024 filings from ZIP archives\n2. ✅ Chunk Q1 with 15% overlap (NVIDIA method)\n   - Core chunk: 500 tokens (tiktoken)\n   - Overlap: 75 tokens (15%)\n   - No LLM context generation\n3. ✅ Metadata extraction (CIK, company, form, date)\n4. 🔄 JSON output: Q1 complete (6,337 files), Q2-Q4 pending\n\n### Phase 2: Embedding Generation 🔄 TEST COMPLETE, Q1 FULL PENDING\n1. ✅ Create `embedding_generator.py` script\n2. ✅ Test on 3 Q1 files (Tesla, Microsoft, Apple - 286 chunks)\n3. ⏸️ Scale to full Q1 (6,337 files, ~108,000 chunks - est. 19 hours)\n4. ⏸️ Scale to Q2-Q4\n5. ⏸️ Store in `/app/data/embeddings/2024/QTR{1-4}/`\n\n### Phase 3: RAPTOR Implementation ⏸️\n1. Hierarchical clustering (UMAP + GMM)\n2. Recursive summarization (3 levels via Ollama)\n3. ChromaDB setup and ingestion\n4. Cluster validation\n\n### Phase 4: Deployment ⏸️\n1. Open WebUI integration\n2. ChromaDB retrieval pipeline\n3. LLM query interface\n4. End-to-end testing\n\n---\n\n## Docker Configuration\n\n### `docker-compose.chunking.yml` Volume Mounts:\n```yaml\nvolumes:\n  - /app/data/edgar:/app/data/edgar:ro          # Read-only input (extracted .txt)\n  - /app/data/processed:/app/data/processed     # Write output (chunked .json)\n  - /app/data/embeddings:/app/data/embeddings   # Embeddings output (.parquet)\n  - ./src:/app/src                              # Source code (development)\n```\n\n**Key Points:**\n- All three data directories mounted for persistence\n- Embeddings directory properly mounted (fixed issue where embeddings were lost)\n- Files persist on EC2 host filesystem after container removal (`--rm` flag)\n\n---\n\n## Technical Specifications\n\n### EC2 Infrastructure\n- **Instance:** t3.xlarge\n- **RAM:** 64 GB\n- **Storage:** EBS volume at `/app/data/`\n- **Docker:** edgar-chunking (8.4 GB image)\n- **Python environment**: Inside Docker container only\n\n### Commands Reference\n\n**Chunking (Q2-Q4 pending):**\n```bash\n# Q2\ndocker compose -f docker-compose.chunking.yml run --rm chunking \\\n  python -m src.data.text_processor \\\n  --input /app/data/edgar/extracted/2024/QTR2 \\\n  --output /app/data/processed/2024/QTR2 \\\n  --chunk-size 500\n\n# Q3, Q4 similar\n```\n\n**Embedding generation:**\n```bash\n# Full Q1\ndocker compose -f docker-compose.chunking.yml run --rm chunking \\\n  python -m src.models.embedding_generator \\\n  --input /app/data/processed/2024/QTR1 \\\n  --output /app/data/embeddings/2024/QTR1 \\\n  --files $(ls /app/data/processed/2024/QTR1/*.json)\n```\n\n**Monitoring:**\n```bash\n# Count chunked files\nls -1 /app/data/processed/2024/QTR1/*.json 2>/dev/null | wc -l\n\n# Check embedding files\nls -lh /app/data/embeddings/test_q1/\n\n# Check embedding directory size\ndu -sh /app/data/embeddings/test_q1/\n```\n\n---\n\n## Research Citations\n\n**Chunking Strategy:**\n- **NVIDIA Retrieval QA (2023)**: Overlap-based chunking for improved retrieval\n- **Recommended overlap**: 10-20% for balance between context and efficiency\n\n**Embedding Selection:**\n- **Sentence-BERT (2019):** https://arxiv.org/abs/1908.10084\n- **MPNet (2020):** https://arxiv.org/abs/2004.09297\n- **MTEB Benchmark (2022):** https://arxiv.org/abs/2210.07316\n\n**RAPTOR:**\n- **RAPTOR Paper (2024):** https://arxiv.org/abs/2401.18059\n\n**Tools:**\n- **ChromaDB:** https://docs.trychroma.com/\n- **Ollama:** https://ollama.com/\n- **Open WebUI:** https://github.com/open-webui/open-webui\n- **Sentence Transformers:** https://www.sbert.net/\n\n---\n\n**Last Updated:** 2025-10-24\n\n**Status:** ✅ Q1 chunking complete → ✅ Test embeddings complete (3 companies) → Next: Full Q1 embedding generation"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}