{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Production Plan: RAPTOR RAG with SEC EDGAR Filings (1993-2024)\n\n## Overview\nThis notebook documents the full production RAPTOR RAG system deployment:\n- **Data**: All SEC 10-K/10-Q filings (1993-2024) - ~51GB total, expanding from initial 2024 prototype\n- **Infrastructure**: AWS EC2 (secai instance) - All processing, embedding, and deployment\n- **Models**: Ollama with FinGPT-v3 and other financial LLMs\n- **Goal**: Production-ready RAG system with hierarchical retrieval via Open WebUI\n\n## Evolution: Prototype ‚Üí Production\n\n**Initial Prototype Plan (Archived):**\n- 2024 only (26,014 filings)\n- Local PC processing ‚Üí AWS EC2 embedding ‚Üí handoff\n- Test RAPTOR before scaling\n\n**Current Production Reality:**\n- **2021-2024 uploaded** (6.6GB compressed, ~100K+ filings)\n- **Target: 1993-2024** (full 31-year dataset, ~300K+ filings)\n- **All operations on AWS EC2** (processing + embedding + RAPTOR + deployment)\n- **Open WebUI deployed** (currently port 3000 during troubleshooting, target port 8080)\n\n---\n\n## Current Status (2025-10-17)\n\n### ‚úÖ Infrastructure Deployed\n- AWS EC2 instance \"secai\" (Ubuntu 24.04)\n- SSH access configured (35.175.134.36)\n- Data directory: `/app/data/edgar/`\n- Open WebUI: Running in Docker (currently port 3000, troubleshooting Ollama connectivity, will move to port 8080)\n- Security groups: Configured for authorized access (port 8080 already allowed)\n- Admin account: dev@onyxgs.com (local auth, new users require admin approval)\n\n### ‚úÖ Data Uploaded to EC2\n**Location:** `/app/data/edgar/`\n- ‚úÖ 10-X_C_2021.zip (1.6GB)\n- ‚úÖ 10-X_C_2022.zip (1.8GB)\n- ‚úÖ 10-X_C_2023.zip (1.7GB)\n- ‚úÖ 10-X_C_2024.zip (1.6GB)\n- ‚úÖ 2024/ (unzipped, ready for processing)\n- ‚è≥ 1993-2020 data (to be uploaded)\n\n**Total uploaded:** 6.6GB compressed (~25-30GB uncompressed when all extracted)\n\n### ‚úÖ Local Prototype Completed (Reference Only)\n**Note:** This was done locally for testing and is archived. Production uses AWS EC2.\n\n**Completed on local PC:**\n- ‚úÖ Text processing: 26,014 filings ‚Üí 2.7M chunks (Step 2 complete)\n- ‚úÖ Output: `processed_2024_500tok_contextual.json` (15GB)\n- ‚è≥ Embedding generation: Aborted locally (too resource-intensive)\n\n**Key learnings from local prototype:**\n- 500-token chunks optimal for SEC filings\n- Contextual chunking (Anthropic method) works well\n- 42 minutes to process 26K filings\n- 15GB output manageable for transfer\n\n---\n\n## Production Pipeline (AWS EC2)\n\n### Phase 1: Data Processing üîÑ\n**Status:** Starting\n\n**Tasks:**\n1. Process unzipped 2024 data on EC2\n2. Run `run_02_processing.py` adapted for EC2 paths\n3. Generate chunks with contextual embedding\n4. Repeat for 2021-2023 data\n5. Scale to 1993-2020 once uploaded\n\n**Expected output per year:**\n- ~26K filings/year √ó 4 years = ~104K filings\n- ~2.7M chunks/year √ó 4 years = ~11M chunks\n- ~15GB JSON/year √ó 4 years = ~60GB processed data\n\n**Location:** `/app/data/processed/`\n\n### Phase 2: Embedding Generation ‚è∏Ô∏è\n**Status:** Awaiting Phase 1 completion\n\n**Tasks:**\n1. Load processed chunks on EC2\n2. Run `run_03_embeddings.py` on EC2\n3. Generate embeddings using sentence-transformers\n4. Store embeddings + metadata\n\n**Expected output:**\n- ~11M embeddings (384 dims each)\n- ~17GB embeddings file (.npy)\n- Metadata JSON (~2GB)\n\n**Location:** `/app/data/embeddings/`\n\n### Phase 3: RAPTOR Implementation ‚è∏Ô∏è\n**Status:** Awaiting Phase 2\n\n**Tasks:**\n1. Hierarchical clustering (UMAP + GMM)\n2. Recursive summarization (3 levels via Ollama)\n3. ChromaDB setup and ingestion\n4. Cluster validation\n\n### Phase 4: Deployment ‚è∏Ô∏è\n**Status:** Infrastructure ready, awaiting data pipeline\n\n**Completed:**\n- ‚úÖ Open WebUI Docker container deployed\n- ‚úÖ Port 8080 security rules configured\n- ‚úÖ Admin account configured (dev@onyxgs.com, password: 644e;1C6ig,o)\n- üîÑ Currently running on port 3000 while troubleshooting Ollama connection\n\n**Remaining:**\n- Fix Ollama connection to Open WebUI (adjusting Docker run command)\n- Move to port 8080 once connection working\n- Load ChromaDB with embeddings + summaries\n- Configure RAG query pipeline\n- Test end-to-end queries\n\n---\n\n## Implementation Workflow\n\n### Infrastructure & Deployment\n1. ‚úÖ AWS EC2 instance provisioned\n2. ‚úÖ Open WebUI Docker container deployed\n3. ‚úÖ Admin account configured\n4. üîÑ Ollama connection troubleshooting (currently port 3000)\n5. ‚è∏Ô∏è Finalize on port 8080 once connection stable\n6. ‚è∏Ô∏è ChromaDB setup and ingestion\n7. ‚è∏Ô∏è RAG pipeline integration\n\n### Data Pipeline\n1. ‚úÖ Upload 2021-2024 data (6.6GB)\n2. ‚è∏Ô∏è Upload remaining 1993-2020 data\n3. üîÑ Process all years into chunks\n4. üîÑ Generate embeddings for all chunks\n5. ‚è∏Ô∏è Implement RAPTOR clustering (UMAP + GMM)\n6. ‚è∏Ô∏è Recursive summarization via Ollama\n\n### Testing & Validation\n- ‚è∏Ô∏è Query interface testing\n- ‚è∏Ô∏è Evaluation with RAGAS\n- ‚è∏Ô∏è Performance benchmarking\n- ‚è∏Ô∏è Documentation\n\n---\n\n## Technical Specifications\n\n### Chunking Strategy (Proven from Local Prototype)\n- **Core chunk:** 500 tokens (stored)\n- **Context window:** 100 tokens (50 before + 50 after)\n- **Extended chunk:** ~700 tokens (embedded)\n- **Method:** Anthropic Contextual Retrieval\n- **Overhead:** 19.9% (efficient)\n\n### Embedding Model\n- **Model:** sentence-transformers/all-MiniLM-L6-v2\n- **Dimensions:** 384\n- **Speed:** ~1000 chunks/second (CPU), faster on GPU\n- **Normalized:** Yes (L2 norm = 1.0)\n\n### Infrastructure\n- **Instance:** AWS EC2 \"secai\" (Ubuntu 24.04)\n- **Location:** `/app/` (project root)\n- **Docker:** Open WebUI + Ollama containers\n- **Access:** SSH via key-based auth\n- **Web UI:** Port 8080 (target), currently 3000 during setup, admin: dev@onyxgs.com\n\n### Data Scale (Full Production)\n**When complete (1993-2024):**\n- **Filings:** ~300,000+ (31 years)\n- **Chunks:** ~30-40 million\n- **Embeddings:** ~45-55GB\n- **Processed data:** ~180-200GB\n- **ChromaDB:** ~250GB total (with summaries)\n\n---\n\n## Archived Local Prototype Results\n\n### Step 2: Text Processing (Local - Reference Only)\n**Status:** ‚úÖ Completed locally on 2025-10-16\n\n**Results (2024 data only):**\n- Filings processed: 26,014 / 26,014 (100%)\n- Total chunks: 2,725,171\n- Processing time: 42.1 minutes\n- Output: `processed_2024_500tok_contextual.json` (15GB)\n\n**Token statistics:**\n- Total tokens: 1.36 billion (core)\n- Extended tokens: 1.63 billion (for embedding)\n- Context overhead: 19.9%\n- Avg tokens/filing: 52,128\n\n**Note:** This local prototype validated the approach. Production processing happens on AWS EC2 for all years.\n\n---\n\n## Success Criteria\n\n### Phase 1-2 (Processing + Embedding)\n- [ ] All 1993-2024 data processed successfully\n- [ ] Embeddings generated for all chunks\n- [ ] No data loss or corruption\n- [ ] Reasonable processing time (<1 week total)\n\n### Phase 3 (RAPTOR)\n- [ ] Clustering produces coherent topics\n- [ ] 3-level summarization accurate\n- [ ] Hierarchical structure adds value\n- [ ] Manual validation passes\n\n### Phase 4 (Deployment)\n- [ ] Open WebUI connects to Ollama\n- [ ] Service running on port 8080\n- [ ] ChromaDB retrieval works correctly\n- [ ] Query latency < 10 seconds\n- [ ] System stable under load\n\n### Overall System\n- [ ] Answers factually accurate (90%+)\n- [ ] RAPTOR outperforms simple RAG\n- [ ] Citations reference correct filings\n- [ ] RAGAS evaluation scores high\n\n---\n\n## Next Immediate Steps\n\n1. **Infrastructure:**\n   - Fix Ollama connection to Open WebUI (adjust Docker run command)\n   - Verify Docker containers healthy\n   - Move to port 8080 once stable\n\n2. **Data Processing:**\n   - Transfer `run_02_processing.py` to EC2\n   - Modify paths for `/app/data/edgar/2024/`\n   - Run processing on 2024 data (already unzipped)\n   - Monitor and validate output\n\n3. **Data Upload:**\n   - Upload remaining EDGAR data (1993-2020)\n   - Plan batch processing strategy for 31-year dataset\n\n---\n\n## Resources\n\n### Completed Files (Local Prototype):\n- ‚úÖ `02_text_processing.ipynb` - Local processing (archived)\n- ‚úÖ `run_02_processing.py` - Script (to be adapted for EC2)\n- ‚úÖ `03_embedding_generation.ipynb` - Embedding notebook\n- ‚úÖ `run_03_embeddings.py` - Script (to be run on EC2)\n\n### To Be Created:\n- `04_raptor_clustering.ipynb`\n- `05_raptor_summarization.ipynb`\n- `06_chromadb_setup.ipynb`\n- `07_rag_query_interface.ipynb`\n\n### References:\n- [Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)\n- [RAPTOR Paper](https://arxiv.org/abs/2401.18059)\n- [Sentence-BERT Paper](https://arxiv.org/abs/1908.10084)\n- [MTEB Benchmark](https://arxiv.org/abs/2210.07316)\n- [ChromaDB Docs](https://docs.trychroma.com/)\n- [Ollama](https://ollama.com/)\n- [Open WebUI](https://github.com/open-webui/open-webui)\n\n---\n\n**Status:** üîÑ Production Deployment In Progress\n\n**Last Updated:** 2025-10-17\n\n**Key Change:** Transitioned from prototype to full production deployment on AWS EC2 with complete 1993-2024 dataset."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}