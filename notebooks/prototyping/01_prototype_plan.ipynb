{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Production Deployment: RAPTOR RAG with SEC EDGAR Filings (2024 Complete)\n\n## Overview\nProduction RAPTOR RAG system deployment on AWS EC2:\n- **Data**: All SEC 10-K/10-Q filings for 2024 (26,014 files) üîÑ RE-CHUNKING WITH CONTEXTUAL SUMMARIES\n- **Infrastructure**: AWS EC2 t3.xlarge (8 vCPUs, 64 GB RAM) - All processing and deployment\n- **Models**: Sentence Transformers for embeddings, Ollama for LLM contextual summaries and queries\n- **Goal**: Production-ready RAG system with hierarchical retrieval via Open WebUI\n\n---\n\n## Current Status (2025-10-24)\n\n### ‚úÖ Infrastructure Deployed\n**AWS EC2 Instance \"secAI\":**\n- **Instance Type**: t3.xlarge\n- **vCPUs**: 8\n- **RAM**: 64 GB\n- **OS**: Ubuntu 24.04 LTS\n- **Public IP**: 35.175.134.36\n- **SSH Access**: Configured (key-based auth)\n- **Data Directory**: `/app/data/`\n\n**Deployed Services:**\n- Docker: edgar-chunking image (8.4 GB)\n- Open WebUI: Configured for deployment\n- Security groups: SSH access configured\n\n### üîÑ 2024 Data Re-Chunking IN PROGRESS\n\n**Previous chunking (DELETED):** Simple 500-token chunks without contextual summaries\n**New approach:** Anthropic's contextual retrieval method with LLM-generated summaries\n\n**EC2 File Structure:**\n```\n/app/data/\n‚îú‚îÄ‚îÄ edgar/extracted/2024/\n‚îÇ   ‚îú‚îÄ‚îÄ QTR1/ (6,337 .txt files)   ‚úÖ READY\n‚îÇ   ‚îú‚îÄ‚îÄ QTR2/ (7,247 .txt files)   ‚úÖ READY\n‚îÇ   ‚îú‚îÄ‚îÄ QTR3/ (6,248 .txt files)   ‚úÖ READY\n‚îÇ   ‚îî‚îÄ‚îÄ QTR4/ (6,182 .txt files)   ‚úÖ READY\n‚îÇ\n‚îú‚îÄ‚îÄ processed/2024/\n‚îÇ   ‚îî‚îÄ‚îÄ (empty - previous data deleted)\n‚îÇ\n‚îî‚îÄ‚îÄ embeddings/\n    ‚îî‚îÄ‚îÄ test/ (next: 3 test files after re-chunking)\n```\n\n**2024 Target Processing (with LLM context generation):**\n\n| Quarter | Files | Expected Time | Status |\n|---------|-------|---------------|--------|\n| Q1 | 6,337 | ~2-3 hours | ‚è∏Ô∏è Pending |\n| Q2 | 7,247 | ~2-3 hours | ‚è∏Ô∏è Pending |\n| Q3 | 6,248 | ~2-3 hours | ‚è∏Ô∏è Pending |\n| Q4 | 6,182 | ~2-3 hours | ‚è∏Ô∏è Pending |\n| **TOTAL** | **26,014** | **~8-12 hours** | ‚è∏Ô∏è Pending |\n\n**Note:** LLM context generation adds processing time but provides 35-49% better retrieval accuracy\n\n---\n\n## Next Phase: Contextual Chunking\n\n### Implementation: Anthropic's Contextual Retrieval Method\n\n**What we're doing:**\nFor each 500-token chunk, generate a 50-100 token LLM summary explaining what the chunk discusses in relation to the full document.\n\n**Example:**\n- **Chunk text:** \"The company's revenue grew by 3% over the previous quarter\"\n- **LLM-generated context:** \"This chunk is from ACME Corp's Q2 2023 10-Q filing discussing quarterly revenue performance. Previous quarter revenue was $314M.\"\n- **Stored:** Original chunk (500 tokens) + context summary (50-100 tokens)\n- **Embedded:** [context + chunk] for better retrieval\n\n**Expected improvement:** 35-49% reduction in retrieval failures (per Anthropic research)\n\n### Test Files (Q4 2024):\n1. `20241024_10-Q_edgar_data_1318605_0001628280-24-043486.txt`\n2. `20241030_10-Q_edgar_data_789019_0000950170-24-118967.txt`\n3. `20241101_10-K_edgar_data_320193_0000320193-24-000123.txt`\n\n### Embedding Model: `multi-qa-mpnet-base-dot-v1` (768-dim)\n\n**Selection Rationale:**\n- **High-dimensional (768)** for precise retrieval of exact wording\n- **Trained for Q&A** tasks - perfect for \"find X in filings\" queries\n- **Preserves jargon** - financial/legal term distinctions maintained\n- **No overfitting concerns** - pre-trained model, inference only\n\n**Why NOT lower-dimensional models:**\n- 384-dim (`all-MiniLM-L6-v2`): Loses nuance needed for legal/financial precision\n- Use case requires exact wording retrieval, not general semantic similarity\n- 2x storage cost (8.6GB vs 4.3GB) worth the quality improvement\n\n### Storage Impact:\n- **2.8M chunks √ó 768 dims √ó 4 bytes = ~8.6 GB** (embeddings)\n- **26,014 JSON files with contextual summaries** (~20-25 GB estimated)\n- Acceptable for EC2 EBS volume\n- Enables precise retrieval for complex financial queries\n\n---\n\n## Implementation Pipeline\n\n### Phase 1: Data Processing üîÑ IN PROGRESS\n1. ‚úÖ Extract all 2024 filings from ZIP archives\n2. üîÑ Re-chunk with LLM-generated contextual summaries\n   - Core chunk: 500 tokens (tiktoken)\n   - LLM context: 50-100 tokens per chunk\n   - Model: qwen2.5:1.5b via Ollama\n   - Embedded chunk: [context + core chunk]\n3. ‚úÖ Metadata extraction (CIK, company, form, date)\n4. ‚è∏Ô∏è JSON output: 26,014 files with 2.8M contextualized chunks\n\n### Phase 2: Embedding Generation ‚è∏Ô∏è\n1. ‚úÖ Create `embedding_generator.py` script\n2. ‚è∏Ô∏è Test on 3 files (validation)\n3. ‚è∏Ô∏è Scale to full 2024 (26,014 files)\n4. ‚è∏Ô∏è Store in `/app/data/embeddings/2024/`\n\n### Phase 3: RAPTOR Implementation ‚è∏Ô∏è\n1. Hierarchical clustering (UMAP + GMM)\n2. Recursive summarization (3 levels via Ollama)\n3. ChromaDB setup and ingestion\n4. Cluster validation\n\n### Phase 4: Deployment ‚è∏Ô∏è\n1. Open WebUI integration\n2. ChromaDB retrieval pipeline\n3. LLM query interface\n4. End-to-end testing\n\n---\n\n## Technical Specifications\n\n### Chunking Strategy (Updated - Anthropic's Method)\n- **Core chunk:** 500 tokens (tiktoken)\n- **Contextual summary:** 50-100 tokens (LLM-generated via Ollama)\n- **LLM model:** qwen2.5:1.5b (fast, efficient)\n- **Prompt template:** \"Provide a brief, factual summary (50-100 tokens) explaining what this chunk discusses in relation to the full [FORM_TYPE] filing\"\n- **Stored:** Both original chunk AND contextualized chunk\n- **Embedded:** [context + chunk] for better semantic search\n- **Metadata:** CIK, company name, form type, filing date\n- **Rationale:** 35-49% better retrieval accuracy vs. non-contextual chunks\n\n### Embedding Model (Selected)\n- **Model:** sentence-transformers/multi-qa-mpnet-base-dot-v1\n- **Dimensions:** 768\n- **Parameters:** 420M\n- **Training:** Question-answering tasks\n- **Similarity:** Dot-product (faster than cosine)\n\n### EC2 Infrastructure\n- **Instance:** t3.xlarge\n- **RAM:** 64 GB\n- **Storage:** EBS volume at `/app/data/`\n- **Docker:** edgar-chunking (8.4 GB image)\n- **Ollama:** Running inside Docker for LLM context generation\n\n---\n\n## Research Citations\n\n**Contextual Retrieval:**\n- **Anthropic Contextual Retrieval (2024):** https://www.anthropic.com/news/contextual-retrieval\n- **Key finding:** 35% fewer retrieval failures with contextual embeddings, 49% with BM25 hybrid\n\n**Embedding Selection:**\n- **Sentence-BERT (2019):** https://arxiv.org/abs/1908.10084\n- **MPNet (2020):** https://arxiv.org/abs/2004.09297\n- **MTEB Benchmark (2022):** https://arxiv.org/abs/2210.07316\n\n**RAPTOR:**\n- **RAPTOR Paper (2024):** https://arxiv.org/abs/2401.18059\n\n**Tools:**\n- **ChromaDB:** https://docs.trychroma.com/\n- **Ollama:** https://ollama.com/\n- **Open WebUI:** https://github.com/open-webui/open-webui\n\n---\n\n**Last Updated:** 2025-10-24\n\n**Status:** üîÑ Re-chunking with LLM-generated contextual summaries ‚Üí Then embedding generation"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}