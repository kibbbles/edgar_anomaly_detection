{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Development - Sample Data\n",
    "\n",
    "**Purpose:** Develop and test text processing pipeline on sample files from `eda/samples/`\n",
    "\n",
    "**Scope:**\n",
    "- Extract clean text from SRAF-XML-wrapper format\n",
    "- Implement contextual chunking strategies\n",
    "- **Test 12 chunk sizes** (200, 300, 400, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 8000 tokens)\n",
    "- Preserve document structure and metadata\n",
    "- Determine optimal chunk size empirically (expected: 500-1000 tokens per FinGPT research)\n",
    "\n",
    "**Data Source:** 1,375 sample files from `eda/samples/` (1993-2024 SEC filings)\n",
    "\n",
    "**Output:** Production-ready `text_processor.py` for `src/prod/data/`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sample directory: C:\\Users\\kabec\\Documents\\edgar_anomaly_detection\\eda\\samples\n",
      "[INFO] Sample files available: 1375\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Sample data location\n",
    "SAMPLES_DIR = project_root / 'eda' / 'samples'\n",
    "print(f\"[INFO] Sample directory: {SAMPLES_DIR}\")\n",
    "print(f\"[INFO] Sample files available: {len(list(SAMPLES_DIR.glob('*.txt')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Extraction from SRAF-XML-wrapper\n",
    "\n",
    "### 2.1 Load Sample Filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Testing with: 19931129_10-K_edgar_data_861439_0000912057-94-000263.txt\n",
      "[OK] Loaded 212,900 characters\n",
      "\n",
      "[Preview] First 1000 chars:\n",
      "<Header>\n",
      "<FileStats>\n",
      "    <FileName>19931129_10-K_edgar_data_861439_0000912057-94-000263.txt</FileName>\n",
      "    <GrossFileSize>278174</GrossFileSize>\n",
      "    <NetFileSize>211739</NetFileSize>\n",
      "    <NonText_DocumentType_Chars>0</NonText_DocumentType_Chars>\n",
      "    <HTML_Chars>1866</HTML_Chars>\n",
      "    <XBRL_Chars>0</XBRL_Chars>\n",
      "    <XML_Chars>0</XML_Chars>\n",
      "    <N_Exhibits>2</N_Exhibits>\n",
      "</FileStats>\n",
      "<SEC-Header>\n",
      "0000912057-94-000263.hdr.sgml : 19950608\n",
      "ACCESSION NUMBER:\t\t0000912057-94-000263\n",
      "CONFORMED SUBMISSION TYPE:\t10-K\n",
      "PUBLIC DOCUMENT COUNT:\t\t3\n",
      "CONFORMED PERIOD OF REPORT:\t19930831\n",
      "FILED AS OF DATE:\t\t19931129\n",
      "DATE AS OF CHANGE:\t\t19931129\n",
      "SROS:\t\t\tNONE\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tAMERICAN MEDICAL HOLDINGS INC\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0000861439\n",
      "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\t8060\n",
      "\t\tIRS NUMBER:\t\t\t\t133527632\n",
      "\t\tSTATE OF INCORPORATION:\t\t\tDE\n",
      "\t\tFISCAL YEAR END:\t\t\t0831\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t10-K\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t001-10511\n",
      "\t\tFILM NUMBER:\t\t94505453\n",
      "\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "# Load a sample 10-K for testing\n",
    "sample_files = list(SAMPLES_DIR.glob('*10-K*.txt'))\n",
    "if sample_files:\n",
    "    sample_path = sample_files[0]\n",
    "    print(f\"[INFO] Testing with: {sample_path.name}\")\n",
    "    \n",
    "    with open(sample_path, 'r', encoding='utf-8') as f:\n",
    "        raw_content = f.read()\n",
    "    \n",
    "    print(f\"[OK] Loaded {len(raw_content):,} characters\")\n",
    "    print(f\"\\n[Preview] First 1000 chars:\\n{raw_content[:1000]}\")\n",
    "else:\n",
    "    print(\"[FAIL] No 10-K samples found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parse SRAF Structure\n",
    "\n",
    "**What we're doing here:**\n",
    "SEC filings in SRAF format have a predictable structure with metadata and content separated into sections:\n",
    "\n",
    "```xml\n",
    "<Header>\n",
    "  <FileStats>...</FileStats>  <!-- File size, exhibit counts -->\n",
    "</Header>\n",
    "<SEC-Header>                   <!-- Company info, CIK, filing date -->\n",
    "  ACCESSION-NUMBER: ...\n",
    "  CONFORMED-NAME: ...\n",
    "  CIK: ...\n",
    "  FORM-TYPE: ...\n",
    "</SEC-Header>\n",
    "<TEXT>...</TEXT>                <!-- Actual filing content -->\n",
    "```\n",
    "\n",
    "**Why extract metadata separately?**\n",
    "- We need company name, CIK, and filing date to add context to each chunk\n",
    "- This metadata helps the LLM understand \"who\" and \"when\" for each piece of text\n",
    "- Enables filtering queries like \"Show me all Apple 10-Ks from 2020-2024\"\n",
    "\n",
    "**Why remove wrapper tags?**\n",
    "- The `<Header>` and `<SEC-Header>` sections contain technical filing metadata, not business content\n",
    "- We want clean text for chunking and embedding\n",
    "- HTML/XBRL tags are markup, not content - they interfere with semantic understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Extracted metadata:\n",
      "  COMPANY_NAME: AMERICAN MEDICAL HOLDINGS INC\n",
      "  CIK: 0000861439\n",
      "  FORM_TYPE: 10-K\n",
      "  FILING_DATE: 19931129\n",
      "  ACCESSION_NUMBER: 0000912057-94-000263\n",
      "  PERIOD_OF_REPORT: 19930831\n",
      "\n",
      "[OK] Clean text length: 209,491 characters\n",
      "\n",
      "[Preview] First 500 chars of clean text:\n",
      "Proc-Type: 2001,MIC-CLEAR Originator-Name: keymaster@town.hall.org Originator-Key-Asymmetric: MFkwCgYEVQgBAQICAgADSwAwSAJBALeWW4xDV4i7+b6+UyPn5RtObb1cJ7VkACDq pKb9/DClgTKIm08lCfoilvi9Wl4SODbR1+1waHhiGmeZO8OdgLUCAwEAAQ== MIC-Info: RSA-MD5,RSA, jSme4OE5puXgBpdHHyga1WdDJ0E3trqOOdfp13QPWNizEt4YLMTbUPjitjQi47a9 tBwulFatOU1F7uc/UNiQZQ== 0000912057-94-000263.txt : 19950608 10-K 1 10-K - - - - -------------------------------------------------------------------------------- - - - - ----------------------\n"
     ]
    }
   ],
   "source": [
    "def extract_sraf_metadata(content):\n",
    "    \"\"\"\n",
    "    Extract metadata from SRAF header\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata fields (CIK, company name, form type, date, etc.)\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # Extract SEC-Header section\n",
    "    sec_header_match = re.search(r'<SEC-Header>(.*?)</SEC-Header>', content, re.DOTALL | re.IGNORECASE)\n",
    "    if sec_header_match:\n",
    "        sec_header = sec_header_match.group(1)\n",
    "        \n",
    "        # Field mappings: (output_key, regex_patterns)\n",
    "        # Try multiple patterns for each field to handle format variations\n",
    "        field_mappings = {\n",
    "            'COMPANY_NAME': [\n",
    "                r'COMPANY CONFORMED NAME:\\s*(.+?)(?:\\n|$)',\n",
    "                r'CONFORMED-NAME:\\s*(.+?)(?:\\n|$)',\n",
    "                r'CONFORMED NAME:\\s*(.+?)(?:\\n|$)'\n",
    "            ],\n",
    "            'CIK': [\n",
    "                r'CENTRAL INDEX KEY:\\s*(.+?)(?:\\n|$)',\n",
    "                r'CIK:\\s*(.+?)(?:\\n|$)'\n",
    "            ],\n",
    "            'FORM_TYPE': [\n",
    "                r'FORM TYPE:\\s*(.+?)(?:\\n|$)',\n",
    "                r'FORM-TYPE:\\s*(.+?)(?:\\n|$)',\n",
    "                r'CONFORMED SUBMISSION TYPE:\\s*(.+?)(?:\\n|$)'\n",
    "            ],\n",
    "            'FILING_DATE': [\n",
    "                r'FILED AS OF DATE:\\s*(.+?)(?:\\n|$)',\n",
    "                r'FILED-AS-OF-DATE:\\s*(.+?)(?:\\n|$)',\n",
    "                r'DATE AS OF CHANGE:\\s*(.+?)(?:\\n|$)'\n",
    "            ],\n",
    "            'ACCESSION_NUMBER': [\n",
    "                r'ACCESSION NUMBER:\\s*(.+?)(?:\\n|$)',\n",
    "                r'ACCESSION-NUMBER:\\s*(.+?)(?:\\n|$)'\n",
    "            ],\n",
    "            'PERIOD_OF_REPORT': [\n",
    "                r'CONFORMED PERIOD OF REPORT:\\s*(.+?)(?:\\n|$)',\n",
    "                r'CONFORMED-PERIOD-OF-REPORT:\\s*(.+?)(?:\\n|$)'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Try each pattern until we find a match\n",
    "        for field, patterns in field_mappings.items():\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, sec_header, re.IGNORECASE)\n",
    "                if match:\n",
    "                    metadata[field] = match.group(1).strip()\n",
    "                    break  # Found match, move to next field\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def extract_clean_text(content):\n",
    "    \"\"\"\n",
    "    Extract clean text content from SRAF-XML-wrapper\n",
    "    \n",
    "    Returns:\n",
    "        str: Clean text with HTML/XML tags removed\n",
    "    \"\"\"\n",
    "    # Remove SRAF wrapper tags\n",
    "    text = re.sub(r'<Header>.*?</Header>', '', content, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'<SEC-Header>.*?</SEC-Header>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Remove XBRL tags\n",
    "    text = re.sub(r'<[^>]*xbrl[^>]*>.*?</[^>]*xbrl[^>]*>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Multiple newlines to double\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Test extraction\n",
    "metadata = extract_sraf_metadata(raw_content)\n",
    "clean_text = extract_clean_text(raw_content)\n",
    "\n",
    "print(\"[OK] Extracted metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n[OK] Clean text length: {len(clean_text):,} characters\")\n",
    "print(f\"\\n[Preview] First 500 chars of clean text:\\n{clean_text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunking Strategies\n",
    "\n",
    "**Why chunking matters:**\n",
    "Our filings average ~50K tokens (from EDA), but:\n",
    "- Embedding models have context limits (512-8192 tokens depending on model)\n",
    "- LLMs have context windows (we need to fit multiple chunks in one query)\n",
    "- Retrieval precision: smaller chunks = more targeted results\n",
    "\n",
    "**Our comprehensive test: 12 chunk sizes (200-8000 tokens)**\n",
    "- **200-500 tokens:** FinGPT optimal range for financial fact extraction\n",
    "- **750-2000 tokens:** Standard RAG range for balanced context\n",
    "- **3000-8000 tokens:** Large context testing (likely too broad)\n",
    "\n",
    "**Expected winner:** 500-1000 tokens based on FinGPT research on SEC filings\n",
    "\n",
    "### 3.1 Token Counting\n",
    "\n",
    "**What's a token?**\n",
    "- Not the same as words! \"running\" = 1 token, but \"anthropomorphic\" = 3 tokens\n",
    "- Average: ~0.75 tokens per word in English\n",
    "- LLMs and embedding models operate on tokens, not characters or words\n",
    "\n",
    "**Why use tiktoken?**\n",
    "- Official OpenAI tokenizer (same as GPT-3.5/4)\n",
    "- More accurate than \"characters/4\" estimation\n",
    "- Essential for staying within model limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Sample filing: 45,247 tokens\n",
      "\n",
      "[INFO] Estimated chunks for different sizes:\n",
      "  200 tokens: ~226 chunks\n",
      "  500 tokens: ~90 chunks (FinGPT optimal)\n",
      "  1000 tokens: ~45 chunks (FinGPT optimal)\n",
      "  2000 tokens: ~22 chunks\n",
      "  5000 tokens: ~9 chunks\n",
      "  8000 tokens: ~5 chunks\n"
     ]
    }
   ],
   "source": [
    "# Install tiktoken if needed\n",
    "try:\n",
    "    import tiktoken\n",
    "except ImportError:\n",
    "    print(\"[INFO] Installing tiktoken...\")\n",
    "    !pip install tiktoken\n",
    "    import tiktoken\n",
    "\n",
    "# Initialize tokenizer (cl100k_base used by GPT-3.5/4)\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Estimate tokens in sample\n",
    "token_count = count_tokens(clean_text)\n",
    "print(f\"[OK] Sample filing: {token_count:,} tokens\")\n",
    "print(f\"\\n[INFO] Estimated chunks for different sizes:\")\n",
    "print(f\"  200 tokens: ~{token_count // 200} chunks\")\n",
    "print(f\"  500 tokens: ~{token_count // 500} chunks (FinGPT optimal)\")\n",
    "print(f\"  1000 tokens: ~{token_count // 1000} chunks (FinGPT optimal)\")\n",
    "print(f\"  2000 tokens: ~{token_count // 2000} chunks\")\n",
    "print(f\"  5000 tokens: ~{token_count // 5000} chunks\")\n",
    "print(f\"  8000 tokens: ~{token_count // 8000} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fixed-Size Chunking with Overlap\n",
    "\n",
    "**How this works:**\n",
    "1. Convert entire text to tokens (list of integers)\n",
    "2. Slice tokens into chunks of `chunk_size` (e.g., 500 tokens)\n",
    "3. Move forward by `chunk_size - overlap` to create the next chunk\n",
    "4. Decode tokens back to text\n",
    "\n",
    "**Why overlap?**\n",
    "Without overlap, information at chunk boundaries gets split awkwardly:\n",
    "\n",
    "```\n",
    "Chunk 1: \"...revenue increased due to strong demand for our new prod-\"\n",
    "Chunk 2: \"-uct line, particularly in the Asia-Pacific region...\"\n",
    "```\n",
    "\n",
    "With overlap (~10% of chunk size):\n",
    "```\n",
    "Chunk 1: \"...revenue increased due to strong demand for our new product line, particularly in the Asia-Pacific region...\"\n",
    "Chunk 2: \"...our new product line, particularly in the Asia-Pacific region, which saw 45% growth...\"\n",
    "```\n",
    "\n",
    "**Overlap strategy:**\n",
    "- We use 10% overlap for all chunk sizes\n",
    "- 200 tokens â†’ 20 overlap, 500 tokens â†’ 50 overlap, 1000 tokens â†’ 100 overlap\n",
    "- Research shows 10-20% overlap optimal for preserving context without excessive redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Testing 12 different chunk sizes:\n",
      "\n",
      "[OK]   200 tokens: 252 chunks (overlap: 20 tokens)\n",
      "[OK]   300 tokens: 168 chunks (overlap: 30 tokens)\n",
      "[OK]   400 tokens: 126 chunks (overlap: 40 tokens)\n",
      "[OK]   500 tokens: 101 chunks (overlap: 50 tokens)\n",
      "[OK]   750 tokens:  68 chunks (overlap: 75 tokens)\n",
      "[OK]  1000 tokens:  51 chunks (overlap: 100 tokens)\n",
      "[OK]  1500 tokens:  34 chunks (overlap: 150 tokens)\n",
      "[OK]  2000 tokens:  26 chunks (overlap: 200 tokens)\n",
      "[OK]  3000 tokens:  17 chunks (overlap: 300 tokens)\n",
      "[OK]  4000 tokens:  13 chunks (overlap: 400 tokens)\n",
      "[OK]  5000 tokens:  11 chunks (overlap: 500 tokens)\n",
      "[OK]  8000 tokens:   7 chunks (overlap: 800 tokens)\n",
      "\n",
      "[Summary] Tested chunk sizes from 200 to 8000 tokens\n",
      "[Info] Sample filing has 45,247 tokens total\n",
      "\n",
      "[Preview] First chunk (200 tokens):\n",
      "Proc-Type: 2001,MIC-CLEAR Originator-Name: keymaster@town.hall.org Originator-Key-Asymmetric: MFkwCgYEVQgBAQICAgADSwAwSAJBALeWW4xDV4i7+b6+UyPn5RtObb1cJ7VkACDq pKb9/DClgTKIm08lCfoilvi9Wl4SODbR1+1waHhiGmeZO8OdgLUCAwEAAQ== MIC-Info: RSA-MD5,RSA, jSme4OE5puXgBpdHHyga1WdDJ0E3trqOOdfp13QPWNizEt4YLMTbUPjit...\n",
      "\n",
      "[Preview] First chunk (8000 tokens):\n",
      "Proc-Type: 2001,MIC-CLEAR Originator-Name: keymaster@town.hall.org Originator-Key-Asymmetric: MFkwCgYEVQgBAQICAgADSwAwSAJBALeWW4xDV4i7+b6+UyPn5RtObb1cJ7VkACDq pKb9/DClgTKIm08lCfoilvi9Wl4SODbR1+1waHhiGmeZO8OdgLUCAwEAAQ== MIC-Info: RSA-MD5,RSA, jSme4OE5puXgBpdHHyga1WdDJ0E3trqOOdfp13QPWNizEt4YLMTbUPjit...\n"
     ]
    }
   ],
   "source": [
    "def chunk_by_tokens(text, chunk_size=2000, overlap=200):\n",
    "    \"\"\"\n",
    "    Chunk text by token count with overlap\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        chunk_size: Target tokens per chunk\n",
    "        overlap: Token overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        # Move start forward by (chunk_size - overlap)\n",
    "        start += (chunk_size - overlap)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Test ALL 12 chunk sizes with 10% overlap\n",
    "CHUNK_SIZES = [200, 300, 400, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 8000]\n",
    "\n",
    "print(\"[INFO] Testing 12 different chunk sizes:\\n\")\n",
    "\n",
    "chunking_results = {}\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    overlap = int(chunk_size * 0.1)  # 10% overlap\n",
    "    chunks = chunk_by_tokens(clean_text, chunk_size=chunk_size, overlap=overlap)\n",
    "    \n",
    "    chunking_results[chunk_size] = {\n",
    "        'num_chunks': len(chunks),\n",
    "        'overlap': overlap,\n",
    "        'chunks': chunks\n",
    "    }\n",
    "    \n",
    "    print(f\"[OK] {chunk_size:>5} tokens: {len(chunks):>3} chunks (overlap: {overlap} tokens)\")\n",
    "\n",
    "print(f\"\\n[Summary] Tested chunk sizes from {min(CHUNK_SIZES)} to {max(CHUNK_SIZES)} tokens\")\n",
    "print(f\"[Info] Sample filing has {token_count:,} tokens total\")\n",
    "\n",
    "# Preview first chunk from smallest and largest sizes\n",
    "print(f\"\\n[Preview] First chunk (200 tokens):\\n{chunking_results[200]['chunks'][0][:300]}...\\n\")\n",
    "print(f\"[Preview] First chunk (8000 tokens):\\n{chunking_results[8000]['chunks'][0][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Contextual Chunking\n",
    "\n",
    "**The problem with raw chunks:**\n",
    "When you retrieve a chunk from the vector database, it's just text with no context:\n",
    "```\n",
    "\"Revenue increased 15% year-over-year to $2.3B...\"\n",
    "```\n",
    "\n",
    "Questions the LLM can't answer:\n",
    "- Which company is this?\n",
    "- What year?\n",
    "- Is this a 10-K or 10-Q?\n",
    "\n",
    "**Contextual chunking solution:**\n",
    "Add a header to EVERY chunk with document metadata:\n",
    "```\n",
    "Document: Apple Inc. (10-K) filed 2024-01-15 [CIK: 0000320193]\n",
    "\n",
    "Revenue increased 15% year-over-year to $2.3B...\n",
    "```\n",
    "\n",
    "**Why this matters for RAG:**\n",
    "1. **Better retrieval:** Embedding models encode the metadata too, improving matching\n",
    "2. **LLM context:** The LLM knows exactly what document it's reading\n",
    "3. **Multi-document queries:** \"Compare Apple vs Microsoft revenue\" - LLM can distinguish chunks\n",
    "4. **Citation:** System can cite exactly which filing a fact came from\n",
    "\n",
    "**Trade-off:**\n",
    "- Adds ~50-100 tokens per chunk (context header)\n",
    "- But dramatically improves retrieval quality and answer accuracy\n",
    "- Worth it for complex queries across multiple filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating contextual chunks for all chunk sizes:\n",
      "\n",
      "[OK]   200 tokens: 252 contextual chunks created\n",
      "[OK]   300 tokens: 168 contextual chunks created\n",
      "[OK]   400 tokens: 126 contextual chunks created\n",
      "[OK]   500 tokens: 101 contextual chunks created\n",
      "[OK]   750 tokens: 68 contextual chunks created\n",
      "[OK]  1000 tokens: 51 contextual chunks created\n",
      "[OK]  1500 tokens: 34 contextual chunks created\n",
      "[OK]  2000 tokens: 26 contextual chunks created\n",
      "[OK]  3000 tokens: 17 contextual chunks created\n",
      "[OK]  4000 tokens: 13 contextual chunks created\n",
      "[OK]  5000 tokens: 11 contextual chunks created\n",
      "[OK]  8000 tokens: 7 contextual chunks created\n",
      "\n",
      "[Preview] Contextual chunk example (500 tokens):\n",
      "Document: AMERICAN MEDICAL HOLDINGS INC (10-K) filed 19931129 [CIK: 0000861439]\n",
      "\n",
      "Proc-Type: 2001,MIC-CLEAR Originator-Name: keymaster@town.hall.org Originator-Key-Asymmetric: MFkwCgYEVQgBAQICAgADSwAwSAJBALeWW4xDV4i7+b6+UyPn5RtObb1cJ7VkACDq pKb9/DClgTKIm08lCfoilvi9Wl4SODbR1+1waHhiGmeZO8OdgLUCAwEAAQ== MIC-Info: RSA-MD5,RSA, jSme4OE5puXgBpdHHyga1WdDJ0E3trqOOdfp13QPWNizEt4YLMTbUPjitjQi47a9 tBwulFatOU1F7uc/UNiQZQ== 0000912057-94-000263.txt : 19950608 10-K 1 10-K - - - - -------------------------------------------------------------------------------- - - - - -----------------------------------------...\n",
      "\n",
      "[Metadata]:\n",
      "{\n",
      "  \"company\": \"AMERICAN MEDICAL HOLDINGS INC\",\n",
      "  \"form_type\": \"10-K\",\n",
      "  \"filing_date\": \"19931129\",\n",
      "  \"cik\": \"0000861439\",\n",
      "  \"chunk_index\": 0,\n",
      "  \"total_chunks\": 101\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def create_contextual_chunks(chunks, metadata):\n",
    "    \"\"\"\n",
    "    Add document context to each chunk\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of text chunks\n",
    "        metadata: Document metadata dict\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dicts with chunk text and metadata\n",
    "    \"\"\"\n",
    "    contextual_chunks = []\n",
    "    \n",
    "    # Create context header (use new field names)\n",
    "    company = metadata.get('COMPANY_NAME', 'Unknown Company')\n",
    "    form_type = metadata.get('FORM_TYPE', 'Unknown Form')\n",
    "    filing_date = metadata.get('FILING_DATE', 'Unknown Date')\n",
    "    cik = metadata.get('CIK', 'Unknown CIK')\n",
    "    \n",
    "    context_header = f\"Document: {company} ({form_type}) filed {filing_date} [CIK: {cik}]\\n\\n\"\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        contextual_chunks.append({\n",
    "            'chunk_id': i,\n",
    "            'text': context_header + chunk,\n",
    "            'metadata': {\n",
    "                'company': company,\n",
    "                'form_type': form_type,\n",
    "                'filing_date': filing_date,\n",
    "                'cik': cik,\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(chunks)\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return contextual_chunks\n",
    "\n",
    "\n",
    "# Create contextual chunks for all 12 chunk sizes\n",
    "print(\"[INFO] Creating contextual chunks for all chunk sizes:\\n\")\n",
    "\n",
    "contextual_results = {}\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    chunks = chunking_results[chunk_size]['chunks']\n",
    "    contextual_chunks = create_contextual_chunks(chunks, metadata)\n",
    "    contextual_results[chunk_size] = contextual_chunks\n",
    "    print(f\"[OK] {chunk_size:>5} tokens: {len(contextual_chunks)} contextual chunks created\")\n",
    "\n",
    "# Preview contextual chunk from 500 tokens (likely optimal per FinGPT)\n",
    "print(f\"\\n[Preview] Contextual chunk example (500 tokens):\")\n",
    "print(f\"{contextual_results[500][0]['text'][:600]}...\")\n",
    "print(f\"\\n[Metadata]:\\n{json.dumps(contextual_results[500][0]['metadata'], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Processing All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing all sample files with 12 different chunk sizes...\n",
      "[INFO] Total files: 1375\n",
      "\n",
      "Processing   200 token chunks... Done! (1375/1375 files)\n",
      "Processing   300 token chunks... Done! (1375/1375 files)\n",
      "Processing   400 token chunks... Done! (1375/1375 files)\n",
      "Processing   500 token chunks... Done! (1375/1375 files)\n",
      "Processing   750 token chunks... Done! (1375/1375 files)\n",
      "Processing  1000 token chunks... Done! (1375/1375 files)\n",
      "Processing  1500 token chunks... Done! (1375/1375 files)\n",
      "Processing  2000 token chunks... Done! (1375/1375 files)\n",
      "Processing  3000 token chunks... Done! (1375/1375 files)\n",
      "Processing  4000 token chunks... Done! (1375/1375 files)\n",
      "Processing  5000 token chunks... Done! (1375/1375 files)\n",
      "Processing  8000 token chunks... Done! (1375/1375 files)\n",
      "\n",
      "================================================================================\n",
      "[COMPLETE] All 12 chunk sizes processed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def process_filing(file_path, chunk_size=2000, overlap=None):\n",
    "    \"\"\"\n",
    "    Complete processing pipeline for a single filing\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to filing\n",
    "        chunk_size: Target tokens per chunk\n",
    "        overlap: Token overlap (defaults to 10% of chunk_size)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Processed filing with chunks and metadata\n",
    "    \"\"\"\n",
    "    if overlap is None:\n",
    "        overlap = int(chunk_size * 0.1)  # Default 10% overlap\n",
    "    \n",
    "    # Load file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_content = f.read()\n",
    "    \n",
    "    # Extract metadata and clean text\n",
    "    metadata = extract_sraf_metadata(raw_content)\n",
    "    clean_text = extract_clean_text(raw_content)\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = chunk_by_tokens(clean_text, chunk_size, overlap)\n",
    "    \n",
    "    # Add context\n",
    "    contextual_chunks = create_contextual_chunks(chunks, metadata)\n",
    "    \n",
    "    return {\n",
    "        'file_name': file_path.name,\n",
    "        'metadata': metadata,\n",
    "        'total_tokens': count_tokens(clean_text),\n",
    "        'chunk_size': chunk_size,\n",
    "        'overlap': overlap,\n",
    "        'num_chunks': len(chunks),\n",
    "        'chunks': contextual_chunks\n",
    "    }\n",
    "\n",
    "\n",
    "# Process all samples with ALL 12 chunk sizes\n",
    "print(\"[INFO] Processing all sample files with 12 different chunk sizes...\")\n",
    "print(f\"[INFO] Total files: {len(list(SAMPLES_DIR.glob('*.txt')))}\\n\")\n",
    "\n",
    "all_results = {chunk_size: [] for chunk_size in CHUNK_SIZES}\n",
    "\n",
    "sample_count = len(list(SAMPLES_DIR.glob('*.txt')))\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    print(f\"Processing {chunk_size:>5} token chunks... \", end='', flush=True)\n",
    "    \n",
    "    for sample_path in SAMPLES_DIR.glob('*.txt'):\n",
    "        try:\n",
    "            result = process_filing(sample_path, chunk_size=chunk_size)\n",
    "            all_results[chunk_size].append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[FAIL] {sample_path.name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Done! ({len(all_results[chunk_size])}/{sample_count} files)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[COMPLETE] All 12 chunk sizes processed successfully!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating comprehensive statistics for all chunk sizes...\n",
      "\n",
      "================================================================================\n",
      "CHUNK SIZE COMPARISON - ALL 12 SIZES\n",
      "================================================================================\n",
      " chunk_size  overlap  total_chunks  avg_chunks_per_filing  avg_tokens_per_filing  estimated_storage_mb\n",
      "        200       20        381933                  277.8                  49910               2237.89\n",
      "        300       30        254864                  185.4                  49910               1493.34\n",
      "        400       40        191315                  139.1                  49910               1120.99\n",
      "        500       50        153207                  111.4                  49910                897.70\n",
      "        750       75        102338                   74.4                  49910                599.64\n",
      "       1000      100         76953                   56.0                  49910                450.90\n",
      "       1500      150         51515                   37.5                  49910                301.85\n",
      "       2000      200         38820                   28.2                  49910                227.46\n",
      "       3000      300         26091                   19.0                  49910                152.88\n",
      "       4000      400         19747                   14.4                  49910                115.71\n",
      "       5000      500         15927                   11.6                  49910                 93.32\n",
      "       8000      800         10214                    7.4                  49910                 59.85\n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS:\n",
      "================================================================================\n",
      "Total files processed: 1375\n",
      "Chunk sizes tested: 12\n",
      "\n",
      "Smallest chunks (200 tokens): 381,933.0 total chunks\n",
      "Largest chunks (8000 tokens): 10,214.0 total chunks\n",
      "\n",
      "Storage range: 59.9 MB - 2237.9 MB\n",
      "\n",
      "Expected optimal (FinGPT research): 500-1000 token chunks\n",
      "\n",
      "================================================================================\n",
      "FINGPT RECOMMENDED RANGE:\n",
      "================================================================================\n",
      "\n",
      "500 tokens:\n",
      "  Total chunks: 153,207.0\n",
      "  Avg chunks/filing: 111.4\n",
      "  Storage: 897.70 MB\n",
      "\n",
      "1000 tokens:\n",
      "  Total chunks: 76,953.0\n",
      "  Avg chunks/filing: 56.0\n",
      "  Storage: 450.90 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comprehensive comparison across all 12 chunk sizes\n",
    "print(\"[INFO] Generating comprehensive statistics for all chunk sizes...\\n\")\n",
    "\n",
    "comparison_stats = []\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    filings = all_results[chunk_size]\n",
    "    \n",
    "    total_chunks = sum(f['num_chunks'] for f in filings)\n",
    "    avg_chunks = total_chunks / len(filings) if filings else 0\n",
    "    avg_tokens = sum(f['total_tokens'] for f in filings) / len(filings) if filings else 0\n",
    "    \n",
    "    # Estimate storage (assuming 1536-dimensional embeddings at 4 bytes/float)\n",
    "    embedding_size_mb = (total_chunks * 1536 * 4) / (1024 * 1024)\n",
    "    \n",
    "    comparison_stats.append({\n",
    "        'chunk_size': chunk_size,\n",
    "        'overlap': int(chunk_size * 0.1),\n",
    "        'total_chunks': total_chunks,\n",
    "        'avg_chunks_per_filing': round(avg_chunks, 1),\n",
    "        'avg_tokens_per_filing': int(avg_tokens),\n",
    "        'estimated_storage_mb': round(embedding_size_mb, 2)\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_stats)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHUNK SIZE COMPARISON - ALL 12 SIZES\")\n",
    "print(\"=\"*80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total files processed: {len(all_results[CHUNK_SIZES[0]])}\")\n",
    "print(f\"Chunk sizes tested: {len(CHUNK_SIZES)}\")\n",
    "print(f\"\\nSmallest chunks (200 tokens): {df_comparison.iloc[0]['total_chunks']:,} total chunks\")\n",
    "print(f\"Largest chunks (8000 tokens): {df_comparison.iloc[-1]['total_chunks']:,} total chunks\")\n",
    "print(f\"\\nStorage range: {df_comparison['estimated_storage_mb'].min():.1f} MB - {df_comparison['estimated_storage_mb'].max():.1f} MB\")\n",
    "print(f\"\\nExpected optimal (FinGPT research): 500-1000 token chunks\")\n",
    "\n",
    "# Highlight 500 and 1000 token stats\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINGPT RECOMMENDED RANGE:\")\n",
    "print(f\"{'='*80}\")\n",
    "for size in [500, 1000]:\n",
    "    row = df_comparison[df_comparison['chunk_size'] == size].iloc[0]\n",
    "    print(f\"\\n{size} tokens:\")\n",
    "    print(f\"  Total chunks: {row['total_chunks']:,}\")\n",
    "    print(f\"  Avg chunks/filing: {row['avg_chunks_per_filing']}\")\n",
    "    print(f\"  Storage: {row['estimated_storage_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results\n",
    "\n",
    "Save processed chunks for embedding generation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving processed chunks for all 12 chunk sizes...\n",
      "\n",
      "[OK]   200 tokens: processed_samples_200tok.json (482.48 MB)\n",
      "[OK]   300 tokens: processed_samples_300tok.json (435.41 MB)\n",
      "[OK]   400 tokens: processed_samples_400tok.json (411.89 MB)\n",
      "[OK]   500 tokens: processed_samples_500tok.json (397.77 MB)\n",
      "[OK]   750 tokens: processed_samples_750tok.json (378.93 MB)\n",
      "[OK]  1000 tokens: processed_samples_1000tok.json (369.44 MB)\n",
      "[OK]  1500 tokens: processed_samples_1500tok.json (359.97 MB)\n",
      "[OK]  2000 tokens: processed_samples_2000tok.json (355.11 MB)\n",
      "[OK]  3000 tokens: processed_samples_3000tok.json (350.13 MB)\n",
      "[OK]  4000 tokens: processed_samples_4000tok.json (347.46 MB)\n",
      "[OK]  5000 tokens: processed_samples_5000tok.json (345.65 MB)\n",
      "[OK]  8000 tokens: processed_samples_8000tok.json (342.44 MB)\n",
      "\n",
      "[OK] Saved comparison summary: chunk_size_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "EXPORT COMPLETE\n",
      "================================================================================\n",
      "Total files saved: 12 JSON files + 1 CSV summary\n",
      "Output directory: C:\\Users\\kabec\\Documents\\edgar_anomaly_detection\\notebooks\\prototyping\\output\n",
      "\n",
      "Files created:\n",
      "  - 12 JSON files: processed_samples_200tok.json through processed_samples_8000tok.json\n",
      "  - 1 CSV summary: chunk_size_comparison.csv\n",
      "\n",
      "Next steps:\n",
      "1. Review chunk coherence for different sizes\n",
      "2. Create test queries for retrieval evaluation\n",
      "3. Test embedding generation (next notebook: 02_embedding_tests.ipynb)\n",
      "4. Determine optimal chunk size empirically\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = project_root / 'notebooks' / 'prototyping' / 'output'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"[INFO] Saving processed chunks for all 12 chunk sizes...\\n\")\n",
    "\n",
    "# Save each chunk size separately for easy comparison\n",
    "saved_files = []\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    output_path = output_dir / f'processed_samples_{chunk_size}tok.json'\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results[chunk_size], f, indent=2)\n",
    "    \n",
    "    file_size_mb = output_path.stat().st_size / (1024*1024)\n",
    "    saved_files.append({\n",
    "        'chunk_size': chunk_size,\n",
    "        'filename': output_path.name,\n",
    "        'size_mb': round(file_size_mb, 2)\n",
    "    })\n",
    "    \n",
    "    print(f\"[OK] {chunk_size:>5} tokens: {output_path.name} ({file_size_mb:.2f} MB)\")\n",
    "\n",
    "# Save comparison summary\n",
    "summary_path = output_dir / 'chunk_size_comparison.csv'\n",
    "df_comparison.to_csv(summary_path, index=False)\n",
    "print(f\"\\n[OK] Saved comparison summary: {summary_path.name}\")\n",
    "\n",
    "# Clean up old files\n",
    "old_file = output_dir / 'processed_samples_2k.json'\n",
    "if old_file.exists():\n",
    "    old_file.unlink()\n",
    "    print(f\"[INFO] Removed old file: processed_samples_2k.json\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total files saved: {len(saved_files)} JSON files + 1 CSV summary\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - 12 JSON files: processed_samples_200tok.json through processed_samples_8000tok.json\")\n",
    "print(f\"  - 1 CSV summary: chunk_size_comparison.csv\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Review chunk coherence for different sizes\")\n",
    "print(f\"2. Create test queries for retrieval evaluation\")\n",
    "print(f\"3. Test embedding generation (next notebook: 02_embedding_tests.ipynb)\")\n",
    "print(f\"4. Determine optimal chunk size empirically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "### Completed âœ…\n",
    "- Extracted clean text from SRAF-XML format\n",
    "- Tested **12 different chunk sizes** (200, 300, 400, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 8000 tokens)\n",
    "- Created contextual chunks with document metadata\n",
    "- Processed all 1,375 sample files with all chunk sizes\n",
    "- Generated comprehensive comparison statistics\n",
    "- Exported 12 JSON files (one per chunk size) + CSV summary\n",
    "\n",
    "### Next Steps ðŸ”„\n",
    "\n",
    "**1. Chunk Coherence Review (Manual - ~30 minutes):**\n",
    "   - Inspect sample chunks from each of the 12 sizes\n",
    "   - Check: Are 200-token chunks too fragmented?\n",
    "   - Check: Are 8000-token chunks too broad?\n",
    "   - Identify which sizes maintain semantic coherence\n",
    "   - Focus on FinGPT range: 500-1000 tokens\n",
    "\n",
    "**2. Retrieval Quality Evaluation (`02_chunking_strategies.ipynb`):**\n",
    "   - Define 20 test questions with known answers\n",
    "   - Examples:\n",
    "     - \"What was revenue in Q3?\" (fact extraction â†’ favors small chunks)\n",
    "     - \"Explain cyber risk factors\" (narrative â†’ favors larger chunks)\n",
    "   - Run retrieval tests on **all 12 chunk sizes**\n",
    "   - Measure: precision (did we retrieve answer?), accuracy (did LLM answer correctly?), rank (position of best chunk)\n",
    "   - Compare performance across all sizes\n",
    "   - Expected winner: 500-1000 tokens (FinGPT research)\n",
    "\n",
    "**3. Embedding Generation Tests (`03_embedding_tests.ipynb`):**\n",
    "   - Test embedding models: nomic-embed-text-v1.5 (8192 limit), BGE-large (512 limit)\n",
    "   - Generate embeddings for chunks from top 3-5 chunk sizes (after narrowing down)\n",
    "   - Compare embedding quality and retrieval performance\n",
    "\n",
    "**4. Determine Optimal Chunk Size (Empirical):**\n",
    "   - Based on FinGPT research: expect 500-1000 tokens to win for fact-based queries\n",
    "   - Validate with our test queries and data\n",
    "   - May find different sizes optimal for different query types:\n",
    "     - Fact extraction: Likely 200-500 tokens\n",
    "     - Narrative understanding: Likely 1000-2000 tokens\n",
    "     - Complex reasoning: Possibly 2000-4000 tokens\n",
    "\n",
    "**5. Migrate to Production (`src/prod/data/text_processor.py`):**\n",
    "   - Copy winning chunking strategy (or top 2-3 if query-type dependent)\n",
    "   - Add error handling and logging\n",
    "   - Optimize for batch processing\n",
    "   - Create unit tests\n",
    "\n",
    "**6. Scale to Full Dataset:**\n",
    "   - Process all 1,375 files with optimal chunk size\n",
    "   - Generate embeddings\n",
    "   - Store in vector database (Qdrant/Weaviate)\n",
    "\n",
    "---\n",
    "\n",
    "### Reference Files:\n",
    "\n",
    "**Processed chunks (12 JSON files):**\n",
    "- `output/processed_samples_200tok.json` through `output/processed_samples_8000tok.json`\n",
    "- Each file contains all 1,375 filings chunked at that size\n",
    "\n",
    "**Comparison summary:**\n",
    "- `output/chunk_size_comparison.csv` - Statistics for all 12 chunk sizes\n",
    "  - Total chunks per size\n",
    "  - Average chunks per filing\n",
    "  - Storage requirements\n",
    "  - Overlap strategy\n",
    "\n",
    "**Expected optimal range (FinGPT):** 500-1000 tokens  \n",
    "**Validation method:** Retrieval quality metrics in next notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
