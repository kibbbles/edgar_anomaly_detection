{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing - 2024 SEC Filings\n",
    "\n",
    "**Purpose:** Process all 2024 10-K/10-Q filings for RAPTOR RAG prototype\n",
    "\n",
    "**Key Changes from Multi-Year Version:**\n",
    "- Source: `data/external/10-X_C_2024.zip` (26,018 filings)\n",
    "- Strategy: More data within single timeframe (vs. samples across 31 years)\n",
    "- Chunk size: **500 tokens only** (validated optimal from FinGPT research)\n",
    "- Benefits: Temporal consistency, better clustering, statistically robust\n",
    "\n",
    "**Data Scope:**\n",
    "- Time period: Full year 2024 (Q1-Q4)\n",
    "- Total filings: 26,018\n",
    "- Compressed size: 1.6 GB\n",
    "- Form types: 10-K, 10-Q (and variants)\n",
    "\n",
    "**Output:** `output/processed_2024_500tok.json` for embedding generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Data locations\n",
    "DATA_ZIP = project_root / 'data' / 'external' / '10-X_C_2024.zip'\n",
    "OUTPUT_DIR = project_root / 'notebooks' / 'prototyping' / 'output'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"[INFO] Data source: {DATA_ZIP}\")\n",
    "print(f\"[INFO] Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"[INFO] Zip file exists: {DATA_ZIP.exists()}\")\n",
    "\n",
    "if DATA_ZIP.exists():\n",
    "    zip_size_mb = DATA_ZIP.stat().st_size / (1024*1024)\n",
    "    print(f\"[OK] Zip file size: {zip_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect 2024 Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek inside the zip to understand structure\n",
    "with zipfile.ZipFile(DATA_ZIP, 'r') as z:\n",
    "    file_list = z.namelist()\n",
    "    \n",
    "print(f\"[OK] Total files in zip: {len(file_list):,}\")\n",
    "print(f\"\\n[INFO] First 10 files:\")\n",
    "for f in file_list[:10]:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Count by quarter\n",
    "quarters = defaultdict(int)\n",
    "for f in file_list:\n",
    "    if 'QTR' in f:\n",
    "        qtr = f.split('/')[1] if '/' in f else 'unknown'\n",
    "        quarters[qtr] += 1\n",
    "\n",
    "print(f\"\\n[INFO] Files by quarter:\")\n",
    "for qtr in sorted(quarters.keys()):\n",
    "    print(f\"  {qtr}: {quarters[qtr]:,} files\")\n",
    "\n",
    "# Filter to .txt files only (exclude directories)\n",
    "txt_files = [f for f in file_list if f.endswith('.txt')]\n",
    "print(f\"\\n[OK] Text files to process: {len(txt_files):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Extraction Functions\n",
    "\n",
    "**Reusing proven logic from multi-year prototype:**\n",
    "- Extract metadata from SRAF-XML wrapper\n",
    "- Clean text (remove HTML/XML tags)\n",
    "- Handle format variations (1993-2024 formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sraf_metadata(content):\n",
    "    \"\"\"\n",
    "    Extract metadata from SRAF header\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata fields (CIK, company name, form type, date, etc.)\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # Extract SEC-Header section\n",
    "    sec_header_match = re.search(r'<SEC-Header>(.*?)</SEC-Header>', content, re.DOTALL | re.IGNORECASE)\n",
    "    if sec_header_match:\n",
    "        sec_header = sec_header_match.group(1)\n",
    "        \n",
    "        # Field mappings: try multiple patterns for each field\n",
    "        field_mappings = {\n",
    "            'COMPANY_NAME': [\n",
    "                r'COMPANY CONFORMED NAME:\\s*(.+?)(?:\\n|$)',\n",
    "                r'CONFORMED-NAME:\\s*(.+?)(?:\\n|$)',\n",
    "                r'CONFORMED NAME:\\s*(.+?)(?:\\n|$)'\n",
    "            ],\n",
    "            'CIK': [\n",
    "                r'CENTRAL INDEX KEY:\\s*(.+?)(?:\\n|$)',\n",
    "                r'CIK:\\s*(.+?)(?:\\n|$)'\n",
    "            ],\n",
    "            'FORM_TYPE': [\n",
    "                r'FORM TYPE:\\s*(.+?)(?:\\n|$)',\n",
    "                r'FORM-TYPE:\\s*(.+?)(?:\\n|$)',\n",
    "                r'CONFORMED SUBMISSION TYPE:\\s*(.+?)(?:\\n|$)'\n",
    "            ],\n",
    "            'FILING_DATE': [\n",
    "                r'FILED AS OF DATE:\\s*(.+?)(?:\\n|$)',\n",
    "                r'FILED-AS-OF-DATE:\\s*(.+?)(?:\\n|$)',\n",
    "                r'DATE AS OF CHANGE:\\s*(.+?)(?:\\n|$)'\n",
    "            ],\n",
    "            'ACCESSION_NUMBER': [\n",
    "                r'ACCESSION NUMBER:\\s*(.+?)(?:\\n|$)',\n",
    "                r'ACCESSION-NUMBER:\\s*(.+?)(?:\\n|$)'\n",
    "            ],\n",
    "            'PERIOD_OF_REPORT': [\n",
    "                r'CONFORMED PERIOD OF REPORT:\\s*(.+?)(?:\\n|$)',\n",
    "                r'CONFORMED-PERIOD-OF-REPORT:\\s*(.+?)(?:\\n|$)'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Try each pattern until we find a match\n",
    "        for field, patterns in field_mappings.items():\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, sec_header, re.IGNORECASE)\n",
    "                if match:\n",
    "                    metadata[field] = match.group(1).strip()\n",
    "                    break\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def extract_clean_text(content):\n",
    "    \"\"\"\n",
    "    Extract clean text content from SRAF-XML-wrapper\n",
    "    \n",
    "    Returns:\n",
    "        str: Clean text with HTML/XML tags removed\n",
    "    \"\"\"\n",
    "    # Remove SRAF wrapper tags\n",
    "    text = re.sub(r'<Header>.*?</Header>', '', content, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'<SEC-Header>.*?</SEC-Header>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Remove XBRL tags\n",
    "    text = re.sub(r'<[^>]*xbrl[^>]*>.*?</[^>]*xbrl[^>]*>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "print(\"[OK] Text extraction functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunking Functions\n",
    "\n",
    "**Strategy:**\n",
    "- **500 tokens per chunk** (validated optimal from FinGPT research)\n",
    "- **50 token overlap** (10% of chunk size)\n",
    "- **Contextual headers** on every chunk (company, form, date, CIK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tiktoken if needed\n",
    "try:\n",
    "    import tiktoken\n",
    "except ImportError:\n",
    "    print(\"[INFO] Installing tiktoken...\")\n",
    "    !pip install tiktoken\n",
    "    import tiktoken\n",
    "\n",
    "# Initialize tokenizer (cl100k_base used by GPT-3.5/4)\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "def chunk_by_tokens(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Chunk text by token count with overlap\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        chunk_size: Target tokens per chunk (default: 500)\n",
    "        overlap: Token overlap between chunks (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        # Move start forward by (chunk_size - overlap)\n",
    "        start += (chunk_size - overlap)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_contextual_chunks(chunks, metadata):\n",
    "    \"\"\"\n",
    "    Add document context to each chunk\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of text chunks\n",
    "        metadata: Document metadata dict\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dicts with chunk text and metadata\n",
    "    \"\"\"\n",
    "    contextual_chunks = []\n",
    "    \n",
    "    # Create context header\n",
    "    company = metadata.get('COMPANY_NAME', 'Unknown Company')\n",
    "    form_type = metadata.get('FORM_TYPE', 'Unknown Form')\n",
    "    filing_date = metadata.get('FILING_DATE', 'Unknown Date')\n",
    "    cik = metadata.get('CIK', 'Unknown CIK')\n",
    "    \n",
    "    context_header = f\"Document: {company} ({form_type}) filed {filing_date} [CIK: {cik}]\\n\\n\"\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        contextual_chunks.append({\n",
    "            'chunk_id': i,\n",
    "            'text': context_header + chunk,\n",
    "            'metadata': {\n",
    "                'company': company,\n",
    "                'form_type': form_type,\n",
    "                'filing_date': filing_date,\n",
    "                'cik': cik,\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(chunks)\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return contextual_chunks\n",
    "\n",
    "\n",
    "print(\"[OK] Chunking functions loaded\")\n",
    "print(f\"[INFO] Chunk size: 500 tokens\")\n",
    "print(f\"[INFO] Overlap: 50 tokens (10%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test on Sample Filing\n",
    "\n",
    "**Before processing all 26K files, validate on one sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test first .txt file from zip\n",
    "with zipfile.ZipFile(DATA_ZIP, 'r') as z:\n",
    "    # Get first .txt file\n",
    "    txt_files = [f for f in z.namelist() if f.endswith('.txt')]\n",
    "    sample_file = txt_files[0]\n",
    "    \n",
    "    print(f\"[INFO] Testing with: {sample_file}\")\n",
    "    \n",
    "    # Read file from zip\n",
    "    with z.open(sample_file) as f:\n",
    "        raw_content = f.read().decode('utf-8', errors='ignore')\n",
    "\n",
    "print(f\"[OK] Loaded {len(raw_content):,} characters\")\n",
    "\n",
    "# Extract metadata and clean text\n",
    "metadata = extract_sraf_metadata(raw_content)\n",
    "clean_text = extract_clean_text(raw_content)\n",
    "\n",
    "print(f\"\\n[OK] Extracted metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n[OK] Clean text length: {len(clean_text):,} characters\")\n",
    "\n",
    "# Create chunks\n",
    "token_count = count_tokens(clean_text)\n",
    "chunks = chunk_by_tokens(clean_text, chunk_size=500, overlap=50)\n",
    "contextual_chunks = create_contextual_chunks(chunks, metadata)\n",
    "\n",
    "print(f\"\\n[OK] Chunking results:\")\n",
    "print(f\"  Total tokens: {token_count:,}\")\n",
    "print(f\"  Total chunks: {len(chunks)}\")\n",
    "print(f\"  Avg tokens/chunk: {token_count // len(chunks) if chunks else 0}\")\n",
    "\n",
    "# Preview first chunk\n",
    "print(f\"\\n[Preview] First contextual chunk:\")\n",
    "print(contextual_chunks[0]['text'][:600])\n",
    "print(f\"\\n[Metadata]:\")\n",
    "print(json.dumps(contextual_chunks[0]['metadata'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Process All 2024 Filings\n",
    "\n",
    "**Processing strategy:**\n",
    "- Read directly from zip (no need to extract to disk)\n",
    "- Process in batches with progress updates\n",
    "- Skip files that fail (log errors)\n",
    "- Estimated time: 30-60 minutes for 26K filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filing(file_content, file_name, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Complete processing pipeline for a single filing\n",
    "    \n",
    "    Args:\n",
    "        file_content: Raw file content (string)\n",
    "        file_name: Name of the file\n",
    "        chunk_size: Target tokens per chunk\n",
    "        overlap: Token overlap\n",
    "    \n",
    "    Returns:\n",
    "        dict: Processed filing with chunks and metadata\n",
    "    \"\"\"\n",
    "    # Extract metadata and clean text\n",
    "    metadata = extract_sraf_metadata(file_content)\n",
    "    clean_text = extract_clean_text(file_content)\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = chunk_by_tokens(clean_text, chunk_size, overlap)\n",
    "    \n",
    "    # Add context\n",
    "    contextual_chunks = create_contextual_chunks(chunks, metadata)\n",
    "    \n",
    "    return {\n",
    "        'file_name': file_name,\n",
    "        'metadata': metadata,\n",
    "        'total_tokens': count_tokens(clean_text),\n",
    "        'chunk_size': chunk_size,\n",
    "        'overlap': overlap,\n",
    "        'num_chunks': len(chunks),\n",
    "        'chunks': contextual_chunks\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"[OK] Processing function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files from zip\n",
    "print(f\"[INFO] Processing all 2024 filings...\")\n",
    "print(f\"[INFO] Chunk size: 500 tokens with 50 token overlap\\n\")\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "with zipfile.ZipFile(DATA_ZIP, 'r') as z:\n",
    "    txt_files = [f for f in z.namelist() if f.endswith('.txt')]\n",
    "    total_files = len(txt_files)\n",
    "    \n",
    "    print(f\"[INFO] Total files to process: {total_files:,}\\n\")\n",
    "    \n",
    "    for i, file_path in enumerate(txt_files, 1):\n",
    "        try:\n",
    "            # Read file from zip\n",
    "            with z.open(file_path) as f:\n",
    "                content = f.read().decode('utf-8', errors='ignore')\n",
    "            \n",
    "            # Process filing\n",
    "            result = process_filing(content, file_path, chunk_size=500, overlap=50)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Progress update every 1000 files\n",
    "            if i % 1000 == 0:\n",
    "                pct = (i / total_files) * 100\n",
    "                print(f\"[Progress] {i:,}/{total_files:,} ({pct:.1f}%) - Latest: {file_path.split('/')[-1]}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            errors.append({'file': file_path, 'error': str(e)})\n",
    "            if len(errors) <= 10:  # Only print first 10 errors\n",
    "                print(f\"[FAIL] {file_path}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[COMPLETE] Processing finished!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Successfully processed: {len(results):,} filings\")\n",
    "print(f\"Errors encountered: {len(errors)}\")\n",
    "if errors:\n",
    "    print(f\"\\nFirst few errors:\")\n",
    "    for err in errors[:5]:\n",
    "        print(f\"  {err['file']}: {err['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate statistics\n",
    "total_chunks = sum(f['num_chunks'] for f in results)\n",
    "total_tokens = sum(f['total_tokens'] for f in results)\n",
    "avg_chunks = total_chunks / len(results) if results else 0\n",
    "avg_tokens = total_tokens / len(results) if results else 0\n",
    "\n",
    "# Estimate storage (1536-dimensional embeddings at 4 bytes/float)\n",
    "embedding_size_mb = (total_chunks * 1536 * 4) / (1024 * 1024)\n",
    "\n",
    "# Distribution stats\n",
    "chunks_per_filing = [f['num_chunks'] for f in results]\n",
    "tokens_per_filing = [f['total_tokens'] for f in results]\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"2024 FILING PROCESSING SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total filings: {len(results):,}\")\n",
    "print(f\"  Time period: 2024 (full year)\")\n",
    "print(f\"  Form types: 10-K, 10-Q (and variants)\")\n",
    "\n",
    "print(f\"\\nChunking:\")\n",
    "print(f\"  Chunk size: 500 tokens\")\n",
    "print(f\"  Overlap: 50 tokens (10%)\")\n",
    "print(f\"  Total chunks: {total_chunks:,}\")\n",
    "print(f\"  Avg chunks/filing: {avg_chunks:.1f}\")\n",
    "\n",
    "print(f\"\\nToken Statistics:\")\n",
    "print(f\"  Total tokens: {total_tokens:,}\")\n",
    "print(f\"  Avg tokens/filing: {avg_tokens:,.0f}\")\n",
    "print(f\"  Min tokens/filing: {min(tokens_per_filing):,}\")\n",
    "print(f\"  Max tokens/filing: {max(tokens_per_filing):,}\")\n",
    "print(f\"  Median tokens/filing: {np.median(tokens_per_filing):,.0f}\")\n",
    "\n",
    "print(f\"\\nStorage:\")\n",
    "print(f\"  Estimated embedding size: {embedding_size_mb:,.2f} MB\")\n",
    "print(f\"  Per-chunk embedding: 6.14 KB (1536 dims * 4 bytes)\")\n",
    "\n",
    "print(f\"\\nDistribution:\")\n",
    "print(f\"  P25 chunks/filing: {np.percentile(chunks_per_filing, 25):.0f}\")\n",
    "print(f\"  P50 chunks/filing: {np.percentile(chunks_per_filing, 50):.0f}\")\n",
    "print(f\"  P75 chunks/filing: {np.percentile(chunks_per_filing, 75):.0f}\")\n",
    "print(f\"  P95 chunks/filing: {np.percentile(chunks_per_filing, 95):.0f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed chunks\n",
    "output_file = OUTPUT_DIR / 'processed_2024_500tok.json'\n",
    "\n",
    "print(f\"[INFO] Saving to {output_file}...\")\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "file_size_mb = output_file.stat().st_size / (1024*1024)\n",
    "\n",
    "print(f\"[OK] Saved: {output_file.name}\")\n",
    "print(f\"[OK] File size: {file_size_mb:,.2f} MB\")\n",
    "\n",
    "# Save error log if any errors occurred\n",
    "if errors:\n",
    "    error_file = OUTPUT_DIR / 'processing_errors_2024.json'\n",
    "    with open(error_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(errors, f, indent=2)\n",
    "    print(f\"[INFO] Error log saved: {error_file.name}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPORT COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - {output_file.name} ({file_size_mb:,.2f} MB)\")\n",
    "if errors:\n",
    "    print(f\"  - processing_errors_2024.json ({len(errors)} errors)\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Generate embeddings (next notebook: 03_embedding_generation.ipynb)\")\n",
    "print(f\"2. Test with both Ollama models: gpt-oss and llama3-sec\")\n",
    "print(f\"3. Implement RAPTOR clustering (04_raptor_clustering.ipynb)\")\n",
    "print(f\"4. Build RAG query interface (05_rag_query.ipynb)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
