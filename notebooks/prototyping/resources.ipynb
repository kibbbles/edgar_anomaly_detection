{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Resources & Citations\n",
    "\n",
    "**Purpose:** Centralized list of all research papers, tools, frameworks, and techniques used in this project\n",
    "\n",
    "**Last Updated:** 2025-10-16\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Research Papers & Academic Sources\n\n### RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval)\n- **Paper:** \"RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\"\n- **Venue:** ICLR 2024\n- **Link:** https://arxiv.org/abs/2401.18059\n- **Used for:** Hierarchical clustering and recursive summarization approach\n- **Key takeaway:** Multi-level clustering using UMAP + GMM, recursive summarization for tree-organized retrieval\n\n---\n\n### Sentence-BERT (SBERT)\n- **Paper:** \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\"\n- **Authors:** Reimers & Gurevych (2019)\n- **Link:** https://arxiv.org/abs/1908.10084\n- **Used for:** Embedding model foundation\n- **Key takeaway:** 10,000x faster than BERT for similarity search, optimized for semantic similarity\n\n---\n\n### MTEB (Massive Text Embedding Benchmark)\n- **Paper:** \"MTEB: Massive Text Embedding Benchmark\"\n- **Year:** 2022\n- **Link:** https://arxiv.org/abs/2210.07316\n- **Leaderboard:** https://huggingface.co/spaces/mteb/leaderboard\n- **Used for:** Validating embedding model quality\n- **Key takeaway:** 58 embedding tasks for comprehensive evaluation, multi-qa-mpnet-base-dot-v1 in top 10%\n\n---\n\n### Anthropic Contextual Retrieval\n- **Article:** \"Introducing Contextual Retrieval\"\n- **Date:** September 2024\n- **Link:** https://www.anthropic.com/news/contextual-retrieval\n- **Used for:** LLM-generated contextual summaries for each chunk\n- **Implementation:**\n  - Generate 50-100 token context using Claude/LLM for each chunk\n  - Context explains what the chunk discusses in relation to the full document\n  - Prepend context before embedding (contextualized embedding)\n  - Store both: original chunk (for retrieval) + contextualized chunk (for search)\n- **Key takeaway:** 35% reduction in retrieval failures with contextual embeddings, 49% with BM25, 67% with reranking\n- **Example:** For chunk \"The company's revenue grew by 3% over the previous quarter\" → Generate context \"This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million.\"\n\n---\n\n### RAGAS (Retrieval-Augmented Generation Assessment)\n- **Paper:** \"Ragas: Automated Evaluation of Retrieval Augmented Generation\"\n- **Venue:** EACL 2024 (Demo Track)\n- **Link:** https://arxiv.org/abs/2309.15217\n- **ACL Anthology:** https://aclanthology.org/2024.eacl-demo.16/\n- **Docs:** https://docs.ragas.io/en/stable/\n- **Used for:** Evaluating RAG system performance (faithfulness, relevancy, precision, recall)\n- **Key takeaway:** Reference-free evaluation using LLMs, no need for human-annotated ground truth\n\n---\n\n### Financial Report Chunking for RAG\n- **Paper:** \"Financial Report Chunking for Effective Retrieval Augmented Generation\"\n- **Authors:** Jimeno Yepes et al.\n- **Year:** 2024\n- **Link:** https://arxiv.org/html/2402.05131v3\n- **Chunk sizes tested:** 128, 256, 512 tokens + element-based chunking\n- **Key findings:** 512 tokens produces similar results to element-based chunking for financial documents\n- **Used for:** Validating 500-token chunk size choice for SEC filings\n\n---\n\n### LlamaIndex Chunk Size Evaluation\n- **Article:** \"Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex\"\n- **Author:** Ravi Theja\n- **Date:** October 5, 2023\n- **Link:** https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n- **Chunk sizes tested:** 128, 256, 512, 1024, 2048 tokens\n- **Key findings:** 1024 tokens optimal for faithfulness and relevancy; 512 second best\n- **Used for:** Understanding chunk size vs performance tradeoffs\n\n---\n\n### NVIDIA Chunking Strategy Research\n- **Article:** \"Finding the Best Chunking Strategy for Accurate AI Responses\"\n- **Author:** Steve Han (NVIDIA)\n- **Date:** June 18, 2024\n- **Link:** https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses/\n- **Chunk sizes tested:** 128, 256, 512, 1024, 2048 tokens with 10%, 15%, 20% overlap\n- **Key findings:** \n  - Page-level chunking best overall (0.648 accuracy)\n  - 512-1024 tokens optimal for financial documents\n  - 15% overlap best for FinanceBench\n  - No mention of contextual embeddings\n- **Used for:** Validating chunk size for financial data\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Embedding Models\n\n### multi-qa-mpnet-base-dot-v1 (Sentence Transformers) - SELECTED\n- **Model:** sentence-transformers/multi-qa-mpnet-base-dot-v1\n- **Link:** https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1\n- **Docs:** https://www.sbert.net/\n- **Specifications:**\n  - Dimensions: 768\n  - Parameters: 420M\n  - Base: MPNet architecture\n  - Training: Question-answering pairs (MS MARCO, Stack Exchange, Yahoo Answers)\n  - Similarity: Dot-product (faster than cosine)\n  - Context window: 512 tokens\n- **Used for:** Generating 768-dim embeddings for SEC filing chunks\n- **Why chosen:** \n  - High-dimensional for precise retrieval of exact wording\n  - Trained specifically for Q&A tasks (\"find X in document\" queries)\n  - Preserves financial/legal terminology distinctions\n  - Top 10% on MTEB benchmark\n  - Dot-product similarity enables faster search\n\n---\n\n### all-MiniLM-L6-v2 (Alternative - NOT used)\n- **Model:** sentence-transformers/all-MiniLM-L6-v2\n- **Link:** https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n- **Specifications:**\n  - Dimensions: 384\n  - Parameters: 22.7M\n  - Speed: ~1000 sentences/second on CPU\n- **Why NOT chosen:** Too low-dimensional (384 vs 768), loses nuance needed for exact wording retrieval in legal/financial text\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LLM Models & Serving\n\n### Ollama\n- **Website:** https://ollama.com/\n- **Alternate URL:** https://ollama.ai/\n- **GitHub:** https://github.com/ollama/ollama\n- **Python SDK:** https://github.com/ollama/ollama-python\n- **Models Library:** https://ollama.com/library\n- **Used for:** Local LLM serving and inference\n- **Why chosen:** Easy local deployment, GGUF support, Docker-friendly\n\n---\n\n### gpt-oss (13 GB model)\n- **Pulled via:** `ollama pull gpt-oss`\n- **Size:** 13 GB\n- **Used for:** LLM inference and summarization testing\n- **Format:** GGUF (quantized)\n\n---\n\n### Arcee AI llama3-sec\n- **Model Page:** https://ollama.com/arcee-ai/llama3-sec\n- **Pulled via:** `ollama pull arcee-ai/llama3-sec`\n- **Used for:** SEC filing-specific LLM (fine-tuned for financial documents)\n- **Note:** Fine-tuned for SEC/financial domain\n\n---\n\n### GGUF (GPT-Generated Unified Format)\n- **Description:** Quantized model format for efficient inference\n- **Used by:** Ollama, llama.cpp\n- **Quantization levels:**\n  - q2_K: 2-bit (smallest, fastest, lower quality)\n  - q4_K_M: 4-bit (balanced) ← Recommended\n  - q8_0: 8-bit (best quality, largest)\n- **Benefit:** 8x smaller than full precision (32-bit → 4-bit)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database\n",
    "\n",
    "### ChromaDB\n",
    "- **Website:** https://www.trychroma.com/\n",
    "- **Docs:** https://docs.trychroma.com/\n",
    "- **GitHub:** https://github.com/chroma-core/chroma\n",
    "- **Used for:** Storing and retrieving 2.7M chunk embeddings\n",
    "- **Why chosen:** Python-native, easy local setup, supports metadata filtering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries & Frameworks\n",
    "\n",
    "### LangChain\n",
    "- **Docs:** https://python.langchain.com/\n",
    "- **GitHub:** https://github.com/langchain-ai/langchain\n",
    "- **Used for:** LLM orchestration and RAG pipeline building\n",
    "\n",
    "---\n",
    "\n",
    "### Sentence Transformers\n",
    "- **Docs:** https://www.sbert.net/\n",
    "- **GitHub:** https://github.com/UKPLab/sentence-transformers\n",
    "- **PyPI:** https://pypi.org/project/sentence-transformers/\n",
    "- **Used for:** Loading and running embedding models (all-MiniLM-L6-v2)\n",
    "\n",
    "---\n",
    "\n",
    "### tiktoken\n",
    "- **GitHub:** https://github.com/openai/tiktoken\n",
    "- **PyPI:** https://pypi.org/project/tiktoken/\n",
    "- **Used for:** Token counting (cl100k_base encoding)\n",
    "- **Why chosen:** Accurate token counts for GPT-style tokenization\n",
    "\n",
    "---\n",
    "\n",
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "- **Paper:** https://arxiv.org/abs/1802.03426\n",
    "- **Docs:** https://umap-learn.readthedocs.io/\n",
    "- **PyPI:** https://pypi.org/project/umap-learn/\n",
    "- **Used for:** Dimensionality reduction before clustering (384 dims → 10 dims)\n",
    "- **Why chosen:** RAPTOR paper uses UMAP for clustering\n",
    "\n",
    "---\n",
    "\n",
    "### scikit-learn (GMM - Gaussian Mixture Models)\n",
    "- **Docs:** https://scikit-learn.org/\n",
    "- **GMM Docs:** https://scikit-learn.org/stable/modules/mixture.html\n",
    "- **Used for:** Clustering embeddings after UMAP reduction\n",
    "- **Why chosen:** RAPTOR uses GMM for hierarchical clustering\n",
    "\n",
    "---\n",
    "\n",
    "### NumPy\n",
    "- **Docs:** https://numpy.org/doc/\n",
    "- **Used for:** Array operations and embedding storage (.npy files)\n",
    "\n",
    "---\n",
    "\n",
    "### pandas\n",
    "- **Docs:** https://pandas.pydata.org/docs/\n",
    "- **Used for:** Data manipulation and statistics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Sources\n\n### SEC EDGAR 10-X Files (Notre Dame SRAF)\n- **Source:** Notre Dame Software Repository for Accounting and Finance (SRAF)\n- **Link:** https://sraf.nd.edu/sec-edgar-data/cleaned-10x-files/\n- **Data Cleaning Documentation:** https://sraf.nd.edu/sec-edgar-data/cleaned-10x-files/10x-stage-one-parsing-documentation/\n- **Description:** Pre-cleaned SEC 10-K and 10-Q filings (1993-2024)\n- **Format:** SRAF-XML wrapper around original HTML/XML/SGML filings\n- **Coverage:** 31 years of data (~51 GB total)\n- **Our dataset:** `10-X_C_2024.zip` (26,014 filings, 1.6 GB compressed)\n\n---\n\n### SEC EDGAR API\n- **Documentation:** https://www.sec.gov/edgar/sec-api-documentation\n- **Used for:** Understanding SEC filing structure and metadata\n- **Note:** Raw API access (we use pre-cleaned SRAF data instead)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Deployment & Infrastructure\n\n### Docker\n- **Website:** https://www.docker.com/\n- **Docs:** https://docs.docker.com/\n- **Used for:** Containerizing LLM services, ChromaDB, and Open WebUI\n\n---\n\n### Docker Compose\n- **Docs:** https://docs.docker.com/compose/\n- **Used for:** Multi-container orchestration (Ollama + ChromaDB + Open WebUI)\n\n---\n\n### AWS EC2\n- **Docs:** https://docs.aws.amazon.com/ec2/\n- **Instance Types:** https://aws.amazon.com/ec2/instance-types/\n- **Planned instance:** r6i.4xlarge (128 GB RAM, 16 vCPUs)\n- **Used for:** Production deployment of RAPTOR RAG system\n\n---\n\n### AWS S3\n- **Docs:** https://docs.aws.amazon.com/s3/\n- **Used for:** Storing GGUF models for EC2 deployment (free intra-region transfer)\n\n---\n\n### Open WebUI\n- **GitHub:** https://github.com/open-webui/open-webui\n- **Docs:** https://docs.openwebui.com/\n- **Used for:** Interactive web interface for querying RAG system\n\n---\n\n### Open WebUI Custom Themes\n- **Tutorial:** \"How to Build Custom Open WebUI Themes\"\n- **Author:** Jonas Scholz (code42cate)\n- **Link:** https://dev.to/code42cate/how-to-build-custom-open-webui-themes-55hh\n- **Used for:** Customizing Open WebUI appearance and styling\n- **Key features:** CSS customization, color schemes, layout modifications\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Implementation References\n\n### FinGPT RAPTOR Implementation\n- **GitHub:** https://github.com/AI4Finance-Foundation/FinGPT\n- **RAPTOR Code:** https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n- **Used for:** Reference implementation of RAPTOR for financial documents\n- **Key insight:** Adapted for SEC 10-K/10-Q filings\n\n---\n\n### RAPTOR RAG Documentation (FinGPT)\n- **Documentation:** https://deepwiki.com/AI4Finance-Foundation/FinGPT/5.1-raptor-rag-system\n- **Used for:** Understanding FinGPT's RAPTOR implementation details\n- **Key sections:** Hierarchical clustering, recursive summarization, query interface\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Techniques & Methodologies\n\n### Contextual Chunking (Anthropic's Method)\n- **Source:** Anthropic Contextual Retrieval (Sept 2024)\n- **Link:** https://www.anthropic.com/news/contextual-retrieval\n- **Description:** Generate LLM-based contextual summaries for each chunk before embedding\n- **Our implementation:**\n  - Core chunk: 500 tokens (stored for retrieval)\n  - LLM-generated context: 50-100 tokens explaining chunk's role in document\n  - Contextualized chunk: context + core chunk (embedded for search)\n  - LLM prompt: \"Provide concise context explaining what this chunk discusses in relation to the full {form_type} filing from {company} dated {date}\"\n- **Example Output:**\n  - Original chunk: \"The company's revenue grew by 3% over the previous quarter.\"\n  - Generated context: \"This chunk is from ACME Corp's Q2 2023 10-Q filing discussing quarterly revenue performance; Q1 2023 revenue was $314 million.\"\n  - Contextualized chunk (for embedding): [context] + [original chunk]\n- **Expected improvement:** 35-49% better retrieval accuracy vs non-contextual chunks\n- **Implementation:** Uses Ollama (qwen2.5 or llama3) to generate context summaries\n\n---\n\n### RAPTOR Hierarchical Clustering\n- **Source:** RAPTOR Paper (ICLR 2024)\n- **Link:** https://arxiv.org/abs/2401.18059\n- **Description:** Multi-level clustering and recursive summarization\n- **Steps:**\n  1. Embed all chunks (using multi-qa-mpnet-base-dot-v1, 768-dim)\n  2. Reduce dimensions (UMAP: 768 → 10 dims)\n  3. Cluster (GMM with BIC for optimal K)\n  4. Summarize at 3 levels (chunk → cluster → document)\n  5. Store summaries for hierarchical retrieval\n\n---\n\n### RAG (Retrieval-Augmented Generation)\n- **Description:** Retrieve relevant context, augment prompt, generate answer\n- **Pipeline:**\n  1. User query → embed query (768-dim)\n  2. Similarity search in ChromaDB → retrieve top-K chunks\n  3. Augment prompt with retrieved context\n  4. LLM generates answer based on context\n- **Variants tested:**\n  - **Baseline:** No RAG (LLM alone)\n  - **Simple RAG:** Basic retrieval (no RAPTOR)\n  - **RAPTOR RAG:** Hierarchical retrieval with cluster-aware context\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Frameworks\n",
    "\n",
    "### RAGAS Metrics\n",
    "- **Framework:** https://docs.ragas.io/en/stable/\n",
    "- **Metrics used:**\n",
    "  - **Faithfulness:** Does the answer align with retrieved context?\n",
    "  - **Answer Relevancy:** Does the answer address the question?\n",
    "  - **Context Precision:** Are relevant chunks ranked higher?\n",
    "  - **Context Recall:** Was all needed info retrieved?\n",
    "- **Used for:** Comparing Baseline vs Simple RAG vs RAPTOR RAG\n",
    "\n",
    "---\n",
    "\n",
    "### Manual Evaluation\n",
    "- **Method:** Human review of 5 test questions across 3 systems (15 answers total)\n",
    "- **Criteria:**\n",
    "  - Factual accuracy (verifiable from filings)\n",
    "  - Citation correctness (references right documents)\n",
    "  - Completeness (addresses all parts of question)\n",
    "  - No hallucinations (all claims grounded in context)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Project-Specific Decisions\n\n### Chunk Size: 500 Tokens\n- **Source:** Multi-year prototype testing + 2024 research validation\n- **Test range:** 200-8000 tokens (12 variants tested)\n- **Empirical findings:** 500-1000 tokens optimal for SEC filings\n- **Chosen:** 500 tokens (balance between granularity and context)\n\n**Research validation:**\n1. **Financial Report Chunking (2024):** 512 tokens produces similar results to element-based chunking\n2. **NVIDIA FinanceBench (2024):** 512-1024 tokens best for financial documents\n3. **LlamaIndex study (2023):** 512 tokens second best overall (1024 best, but slower)\n\n**Why 500 over 512 or 1000?**\n- **Granularity:** More chunks per filing → better for RAPTOR clustering\n- **Performance:** Close to research-validated 512 tokens\n- **Storage:** Reasonable size\n- **Context boost:** With LLM-generated context (~100 tokens), effective chunk = ~600 tokens\n- **RAPTOR advantage:** More chunks = richer hierarchical tree structure\n\n**Citations:**\n- Jimeno Yepes et al. (2024): https://arxiv.org/html/2402.05131v3\n- NVIDIA (2024): https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses/\n- LlamaIndex (2023): https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n\n---\n\n### LLM-Generated Context: 50-100 Tokens\n- **Source:** Anthropic Contextual Retrieval (Sept 2024)\n- **Recommendation:** 50-100 tokens\n- **Our choice:** Flexible (50-100 tokens), aiming for concise context\n- **Implementation:** Ollama with qwen2.5:1.5b or llama3\n- **Prompt:** \"Provide concise context (50-100 tokens) explaining what this chunk discusses in relation to the full {form_type} filing from {company} dated {date}.\"\n\n**Citations:**\n- Anthropic Contextual Retrieval (Sept 2024): https://www.anthropic.com/news/contextual-retrieval\n\n---\n\n### Embedding Model: multi-qa-mpnet-base-dot-v1 (768-dim)\n- **Decision:** Use 768-dimensional embeddings instead of 384-dim\n- **Rationale:**\n  - Exact wording retrieval requires high precision\n  - Financial/legal jargon needs fine-grained distinctions\n  - \"Material adverse effect\" vs \"material impact\" → legally distinct\n  - 2x storage (8.6GB vs 4.3GB) acceptable for quality improvement\n- **Source:** MTEB benchmark + use case requirements\n\n---\n\n### 2024 Data Only (26,014 filings)\n- **Decision:** Process 2024 only instead of multi-year samples\n- **Rationale:**\n  - 19x more data than multi-year samples\n  - Temporal consistency\n  - Better for clustering\n  - Cleaner baseline for prototyping\n- **Source:** Coworker suggestion, validated by research\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### SEC Filing Guides\n",
    "- **How to Read 10-K/10-Q:** https://www.sec.gov/resources-for-investors/investor-alerts-bulletins/how-read-10-k10-q\n",
    "- **SEC.gov:** https://www.sec.gov/\n",
    "\n",
    "---\n",
    "\n",
    "### Related Research\n",
    "- **LLM Analysis of 10-K/10-Q:** https://www.researchgate.net/publication/377746616_LLM_Analysis_of_10-K_and_10-Q_Filings_RAG_Results\n",
    "- **RAG Evaluation Study (Oct 2024):** https://www.mdpi.com/2076-3417/14/20/9318\n",
    "- **Financial Chatbot with RAG:** https://medium.com/@RobuRishabh/financial-analysis-chatbot-for-10-q-10-k-reports-using-retrieval-augmented-generation-rag-ef3938892086\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics (Our Project)\n",
    "\n",
    "### Data Processing\n",
    "- **Filings processed:** 26,014 (100% success rate)\n",
    "- **Total chunks:** 2,725,171\n",
    "- **Processing time:** 42.1 minutes\n",
    "- **Output size:** 14,957 MB (~15 GB)\n",
    "\n",
    "### Token Statistics\n",
    "- **Total tokens:** 1,356,067,955 (1.36 billion)\n",
    "- **Core tokens (stored):** 1,356,067,955\n",
    "- **Extended tokens (embedded):** 1,625,920,116\n",
    "- **Context overhead:** 19.9%\n",
    "- **Avg tokens/filing:** 52,128\n",
    "\n",
    "### Expected Outputs\n",
    "- **Embeddings:** ~4.2 GB (2.7M × 384 dims × 4 bytes)\n",
    "- **ChromaDB:** ~10-15 GB (with metadata)\n",
    "- **Total storage:** ~30 GB for complete system\n",
    "\n",
    "---\n",
    "\n",
    "**Last Updated:** 2025-10-16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}