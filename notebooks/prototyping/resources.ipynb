{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Resources & Citations\n",
    "\n",
    "**Purpose:** Centralized list of all research papers, tools, frameworks, and techniques used in this project\n",
    "\n",
    "**Last Updated:** 2025-10-16\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Research Papers & Academic Sources\n\n### RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval)\n- **Paper:** \"RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\"\n- **Venue:** ICLR 2024\n- **Link:** https://arxiv.org/abs/2401.18059\n- **Used for:** Hierarchical clustering and recursive summarization approach\n- **Key takeaway:** Uses SBERT embeddings, 100-token base chunks, hierarchical retrieval improves performance\n\n---\n\n### Sentence-BERT (SBERT)\n- **Paper:** \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\"\n- **Authors:** Reimers & Gurevych (2019)\n- **Link:** https://arxiv.org/abs/1908.10084\n- **Used for:** Embedding model selection (all-MiniLM-L6-v2 is from SBERT family)\n- **Key takeaway:** 10,000x faster than BERT for similarity search, optimized for semantic similarity\n\n---\n\n### MTEB (Massive Text Embedding Benchmark)\n- **Paper:** \"MTEB: Massive Text Embedding Benchmark\"\n- **Year:** 2022\n- **Link:** https://arxiv.org/abs/2210.07316\n- **Leaderboard:** https://huggingface.co/spaces/mteb/leaderboard\n- **Used for:** Validating embedding model quality (all-MiniLM-L6-v2 scores 56.26/100, top 20%)\n- **Key takeaway:** 58 embedding tasks for comprehensive evaluation\n\n---\n\n### Anthropic Contextual Retrieval\n- **Article:** \"Introducing Contextual Retrieval\"\n- **Date:** September 2024\n- **Link:** https://www.anthropic.com/news/contextual-retrieval\n- **Used for:** Contextual chunking strategy (50-100 token context window)\n- **Key takeaway:** 35% reduction in retrieval failures with contextual embeddings, 49% with BM25, 67% with reranking\n\n---\n\n### RAGAS (Retrieval-Augmented Generation Assessment)\n- **Paper:** \"Ragas: Automated Evaluation of Retrieval Augmented Generation\"\n- **Venue:** EACL 2024 (Demo Track)\n- **Link:** https://arxiv.org/abs/2309.15217\n- **ACL Anthology:** https://aclanthology.org/2024.eacl-demo.16/\n- **Docs:** https://docs.ragas.io/en/stable/\n- **Used for:** Evaluating RAG system performance (faithfulness, relevancy, precision, recall)\n- **Key takeaway:** Reference-free evaluation using LLMs, no need for human-annotated ground truth\n\n---\n\n### Financial Report Chunking for RAG\n- **Paper:** \"Financial Report Chunking for Effective Retrieval Augmented Generation\"\n- **Authors:** Jimeno Yepes et al.\n- **Year:** 2024\n- **Link:** https://arxiv.org/html/2402.05131v3\n- **Chunk sizes tested:** 128, 256, 512 tokens + element-based chunking\n- **Key findings:** 512 tokens produces similar results to element-based chunking for financial documents\n- **Used for:** Validating 500-token chunk size choice for SEC filings\n\n---\n\n### LlamaIndex Chunk Size Evaluation\n- **Article:** \"Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex\"\n- **Author:** Ravi Theja\n- **Date:** October 5, 2023\n- **Link:** https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n- **Chunk sizes tested:** 128, 256, 512, 1024, 2048 tokens\n- **Key findings:** 1024 tokens optimal for faithfulness and relevancy; 512 second best\n- **Used for:** Understanding chunk size vs performance tradeoffs\n\n---\n\n### NVIDIA Chunking Strategy Research\n- **Article:** \"Finding the Best Chunking Strategy for Accurate AI Responses\"\n- **Author:** Steve Han (NVIDIA)\n- **Date:** June 18, 2024\n- **Link:** https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses/\n- **Chunk sizes tested:** 128, 256, 512, 1024, 2048 tokens with 10%, 15%, 20% overlap\n- **Key findings:** 15% overlap best for FinanceBench; 256-512 tokens best for factoid queries; 1024 for complex queries\n- **Used for:** Validating overlap strategy and chunk size for financial data\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "\n",
    "### all-MiniLM-L6-v2 (Sentence Transformers)\n",
    "- **Model:** sentence-transformers/all-MiniLM-L6-v2\n",
    "- **Link:** https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "- **Docs:** https://www.sbert.net/\n",
    "- **Specifications:**\n",
    "  - Dimensions: 384\n",
    "  - Parameters: 22.7M\n",
    "  - Speed: ~1000 sentences/second on CPU\n",
    "  - Context window: 512 tokens\n",
    "  - Training: 1B+ sentence pairs\n",
    "- **Used for:** Generating embeddings for 2.7M SEC filing chunks\n",
    "- **Why chosen:** Best performance-to-size ratio, RAPTOR paper uses SBERT family, 200M+ downloads\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LLM Models & Serving\n\n### Ollama\n- **Website:** https://ollama.com/\n- **Alternate URL:** https://ollama.ai/\n- **GitHub:** https://github.com/ollama/ollama\n- **Python SDK:** https://github.com/ollama/ollama-python\n- **Models Library:** https://ollama.com/library\n- **Used for:** Local LLM serving and inference\n- **Why chosen:** Easy local deployment, GGUF support, Docker-friendly\n\n---\n\n### gpt-oss (13 GB model)\n- **Pulled via:** `ollama pull gpt-oss`\n- **Size:** 13 GB\n- **Used for:** LLM inference and summarization testing\n- **Format:** GGUF (quantized)\n\n---\n\n### Arcee AI llama3-sec\n- **Model Page:** https://ollama.com/arcee-ai/llama3-sec\n- **Pulled via:** `ollama pull arcee-ai/llama3-sec`\n- **Used for:** SEC filing-specific LLM (fine-tuned for financial documents)\n- **Note:** Fine-tuned for SEC/financial domain\n\n---\n\n### GGUF (GPT-Generated Unified Format)\n- **Description:** Quantized model format for efficient inference\n- **Used by:** Ollama, llama.cpp\n- **Quantization levels:**\n  - q2_K: 2-bit (smallest, fastest, lower quality)\n  - q4_K_M: 4-bit (balanced) ← Recommended\n  - q8_0: 8-bit (best quality, largest)\n- **Benefit:** 8x smaller than full precision (32-bit → 4-bit)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database\n",
    "\n",
    "### ChromaDB\n",
    "- **Website:** https://www.trychroma.com/\n",
    "- **Docs:** https://docs.trychroma.com/\n",
    "- **GitHub:** https://github.com/chroma-core/chroma\n",
    "- **Used for:** Storing and retrieving 2.7M chunk embeddings\n",
    "- **Why chosen:** Python-native, easy local setup, supports metadata filtering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries & Frameworks\n",
    "\n",
    "### LangChain\n",
    "- **Docs:** https://python.langchain.com/\n",
    "- **GitHub:** https://github.com/langchain-ai/langchain\n",
    "- **Used for:** LLM orchestration and RAG pipeline building\n",
    "\n",
    "---\n",
    "\n",
    "### Sentence Transformers\n",
    "- **Docs:** https://www.sbert.net/\n",
    "- **GitHub:** https://github.com/UKPLab/sentence-transformers\n",
    "- **PyPI:** https://pypi.org/project/sentence-transformers/\n",
    "- **Used for:** Loading and running embedding models (all-MiniLM-L6-v2)\n",
    "\n",
    "---\n",
    "\n",
    "### tiktoken\n",
    "- **GitHub:** https://github.com/openai/tiktoken\n",
    "- **PyPI:** https://pypi.org/project/tiktoken/\n",
    "- **Used for:** Token counting (cl100k_base encoding)\n",
    "- **Why chosen:** Accurate token counts for GPT-style tokenization\n",
    "\n",
    "---\n",
    "\n",
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "- **Paper:** https://arxiv.org/abs/1802.03426\n",
    "- **Docs:** https://umap-learn.readthedocs.io/\n",
    "- **PyPI:** https://pypi.org/project/umap-learn/\n",
    "- **Used for:** Dimensionality reduction before clustering (384 dims → 10 dims)\n",
    "- **Why chosen:** RAPTOR paper uses UMAP for clustering\n",
    "\n",
    "---\n",
    "\n",
    "### scikit-learn (GMM - Gaussian Mixture Models)\n",
    "- **Docs:** https://scikit-learn.org/\n",
    "- **GMM Docs:** https://scikit-learn.org/stable/modules/mixture.html\n",
    "- **Used for:** Clustering embeddings after UMAP reduction\n",
    "- **Why chosen:** RAPTOR uses GMM for hierarchical clustering\n",
    "\n",
    "---\n",
    "\n",
    "### NumPy\n",
    "- **Docs:** https://numpy.org/doc/\n",
    "- **Used for:** Array operations and embedding storage (.npy files)\n",
    "\n",
    "---\n",
    "\n",
    "### pandas\n",
    "- **Docs:** https://pandas.pydata.org/docs/\n",
    "- **Used for:** Data manipulation and statistics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Sources\n\n### SEC EDGAR 10-X Files (Notre Dame SRAF)\n- **Source:** Notre Dame Software Repository for Accounting and Finance (SRAF)\n- **Link:** https://sraf.nd.edu/sec-edgar-data/cleaned-10x-files/\n- **Data Cleaning Documentation:** https://sraf.nd.edu/sec-edgar-data/cleaned-10x-files/10x-stage-one-parsing-documentation/\n- **Description:** Pre-cleaned SEC 10-K and 10-Q filings (1993-2024)\n- **Format:** SRAF-XML wrapper around original HTML/XML/SGML filings\n- **Coverage:** 31 years of data (~51 GB total)\n- **Our dataset:** `10-X_C_2024.zip` (26,014 filings, 1.6 GB compressed)\n\n---\n\n### SEC EDGAR API\n- **Documentation:** https://www.sec.gov/edgar/sec-api-documentation\n- **Used for:** Understanding SEC filing structure and metadata\n- **Note:** Raw API access (we use pre-cleaned SRAF data instead)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Deployment & Infrastructure\n\n### Docker\n- **Website:** https://www.docker.com/\n- **Docs:** https://docs.docker.com/\n- **Used for:** Containerizing LLM services, ChromaDB, and Open WebUI\n\n---\n\n### Docker Compose\n- **Docs:** https://docs.docker.com/compose/\n- **Used for:** Multi-container orchestration (Ollama + ChromaDB + Open WebUI)\n\n---\n\n### AWS EC2\n- **Docs:** https://docs.aws.amazon.com/ec2/\n- **Instance Types:** https://aws.amazon.com/ec2/instance-types/\n- **Planned instance:** r6i.4xlarge (128 GB RAM, 16 vCPUs)\n- **Used for:** Production deployment of RAPTOR RAG system\n\n---\n\n### AWS S3\n- **Docs:** https://docs.aws.amazon.com/s3/\n- **Used for:** Storing GGUF models for EC2 deployment (free intra-region transfer)\n\n---\n\n### Open WebUI\n- **GitHub:** https://github.com/open-webui/open-webui\n- **Docs:** https://docs.openwebui.com/\n- **Used for:** Interactive web interface for querying RAG system\n\n---\n\n### Open WebUI Custom Themes\n- **Tutorial:** \"How to Build Custom Open WebUI Themes\"\n- **Author:** Jonas Scholz (code42cate)\n- **Link:** https://dev.to/code42cate/how-to-build-custom-open-webui-themes-55hh\n- **Used for:** Customizing Open WebUI appearance and styling\n- **Key features:** CSS customization, color schemes, layout modifications\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Implementation References\n\n### FinGPT RAPTOR Implementation\n- **GitHub:** https://github.com/AI4Finance-Foundation/FinGPT\n- **RAPTOR Code:** https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n- **Used for:** Reference implementation of RAPTOR for financial documents\n- **Key insight:** Adapted for SEC 10-K/10-Q filings\n\n---\n\n### RAPTOR RAG Documentation (FinGPT)\n- **Documentation:** https://deepwiki.com/AI4Finance-Foundation/FinGPT/5.1-raptor-rag-system\n- **Used for:** Understanding FinGPT's RAPTOR implementation details\n- **Key sections:** Hierarchical clustering, recursive summarization, query interface\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques & Methodologies\n",
    "\n",
    "### Contextual Chunking\n",
    "- **Source:** Anthropic Contextual Retrieval (Sept 2024)\n",
    "- **Link:** https://www.anthropic.com/news/contextual-retrieval\n",
    "- **Description:** Embed extended chunks (with context) but store only core chunks\n",
    "- **Our implementation:**\n",
    "  - Core chunk: 500 tokens (stored)\n",
    "  - Context window: 100 tokens (50 before + 50 after)\n",
    "  - Extended chunk: ~700 tokens (embedded)\n",
    "  - Header: Company, form type, filing date, CIK\n",
    "- **Expected improvement:** 35-49% better retrieval accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### RAPTOR Hierarchical Clustering\n",
    "- **Source:** RAPTOR Paper (ICLR 2024)\n",
    "- **Link:** https://arxiv.org/abs/2401.18059\n",
    "- **Description:** Multi-level clustering and recursive summarization\n",
    "- **Steps:**\n",
    "  1. Embed all chunks (SBERT)\n",
    "  2. Reduce dimensions (UMAP: 384 → 10 dims)\n",
    "  3. Cluster (GMM with BIC for optimal K)\n",
    "  4. Summarize at 3 levels (chunk → cluster → document)\n",
    "  5. Store summaries for hierarchical retrieval\n",
    "\n",
    "---\n",
    "\n",
    "### RAG (Retrieval-Augmented Generation)\n",
    "- **Description:** Retrieve relevant context, augment prompt, generate answer\n",
    "- **Pipeline:**\n",
    "  1. User query → embed query\n",
    "  2. Similarity search in ChromaDB → retrieve top-K chunks\n",
    "  3. Augment prompt with retrieved context\n",
    "  4. LLM generates answer based on context\n",
    "- **Variants tested:**\n",
    "  - **Baseline:** No RAG (LLM alone)\n",
    "  - **Simple RAG:** Basic retrieval (no RAPTOR)\n",
    "  - **RAPTOR RAG:** Hierarchical retrieval with cluster-aware context\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Frameworks\n",
    "\n",
    "### RAGAS Metrics\n",
    "- **Framework:** https://docs.ragas.io/en/stable/\n",
    "- **Metrics used:**\n",
    "  - **Faithfulness:** Does the answer align with retrieved context?\n",
    "  - **Answer Relevancy:** Does the answer address the question?\n",
    "  - **Context Precision:** Are relevant chunks ranked higher?\n",
    "  - **Context Recall:** Was all needed info retrieved?\n",
    "- **Used for:** Comparing Baseline vs Simple RAG vs RAPTOR RAG\n",
    "\n",
    "---\n",
    "\n",
    "### Manual Evaluation\n",
    "- **Method:** Human review of 5 test questions across 3 systems (15 answers total)\n",
    "- **Criteria:**\n",
    "  - Factual accuracy (verifiable from filings)\n",
    "  - Citation correctness (references right documents)\n",
    "  - Completeness (addresses all parts of question)\n",
    "  - No hallucinations (all claims grounded in context)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Project-Specific Decisions\n\n### Chunk Size: 500 Tokens\n- **Source:** Multi-year prototype testing (archive_v1_multi_year/) + 2024 research validation\n- **Test range:** 200-8000 tokens (12 variants tested on 1,375 filings)\n- **Empirical findings:** 500-1000 tokens optimal for SEC filings\n- **Chosen:** 500 tokens (balance between granularity and context)\n\n**Research validation:**\n1. **Financial Report Chunking (2024):** 512 tokens produces similar results to element-based chunking\n2. **NVIDIA FinanceBench (2024):** 256-512 tokens best for factoid queries; 1024 for complex queries\n3. **LlamaIndex study (2023):** 512 tokens second best overall (1024 best, but slower)\n\n**Why 500 over 512 or 1000?**\n- **Granularity:** 104.8 chunks/filing vs 56 for 1000 tokens → better for RAPTOR clustering\n- **Performance:** Close to research-validated 512 tokens\n- **Storage:** Reasonable at 15 GB for 26K filings\n- **Context boost:** With 100-token context window, effective chunk = ~700 tokens\n- **RAPTOR advantage:** More chunks = richer hierarchical tree structure\n\n**Chunk size comparison (1,375 multi-year sample filings):**\n- 200 tokens: 381,933 chunks (too granular, 2.2 GB storage)\n- 500 tokens: 153,207 chunks (optimal balance, 897 MB)\n- 1000 tokens: 76,953 chunks (less granular, 450 MB)\n- 8000 tokens: 10,214 chunks (too coarse, 59 MB)\n\n**Citations:**\n- Jimeno Yepes et al. (2024): https://arxiv.org/html/2402.05131v3\n- NVIDIA (2024): https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses/\n- LlamaIndex (2023): https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n\n---\n\n### Context Window: 100 Tokens\n- **Source:** Anthropic research + RAPTOR paper\n- **Recommendation:** 50-100 tokens\n- **Our choice:** 100 tokens (50 before + 50 after)\n- **Rationale:** Aligns with both Anthropic and RAPTOR recommendations\n\n**Citations:**\n- Anthropic Contextual Retrieval (Sept 2024): https://www.anthropic.com/news/contextual-retrieval\n- RAPTOR Paper (ICLR 2024): https://arxiv.org/abs/2401.18059\n\n---\n\n### 2024 Data Only (vs Multi-Year)\n- **Decision:** 26K filings from 2024 instead of 1,375 samples across 1993-2024\n- **Rationale:**\n  - 19x more data\n  - Temporal consistency\n  - Better for clustering\n  - Cleaner baseline for prototyping\n- **Source:** Coworker suggestion, validated by research\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### SEC Filing Guides\n",
    "- **How to Read 10-K/10-Q:** https://www.sec.gov/resources-for-investors/investor-alerts-bulletins/how-read-10-k10-q\n",
    "- **SEC.gov:** https://www.sec.gov/\n",
    "\n",
    "---\n",
    "\n",
    "### Related Research\n",
    "- **LLM Analysis of 10-K/10-Q:** https://www.researchgate.net/publication/377746616_LLM_Analysis_of_10-K_and_10-Q_Filings_RAG_Results\n",
    "- **RAG Evaluation Study (Oct 2024):** https://www.mdpi.com/2076-3417/14/20/9318\n",
    "- **Financial Chatbot with RAG:** https://medium.com/@RobuRishabh/financial-analysis-chatbot-for-10-q-10-k-reports-using-retrieval-augmented-generation-rag-ef3938892086\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics (Our Project)\n",
    "\n",
    "### Data Processing\n",
    "- **Filings processed:** 26,014 (100% success rate)\n",
    "- **Total chunks:** 2,725,171\n",
    "- **Processing time:** 42.1 minutes\n",
    "- **Output size:** 14,957 MB (~15 GB)\n",
    "\n",
    "### Token Statistics\n",
    "- **Total tokens:** 1,356,067,955 (1.36 billion)\n",
    "- **Core tokens (stored):** 1,356,067,955\n",
    "- **Extended tokens (embedded):** 1,625,920,116\n",
    "- **Context overhead:** 19.9%\n",
    "- **Avg tokens/filing:** 52,128\n",
    "\n",
    "### Expected Outputs\n",
    "- **Embeddings:** ~4.2 GB (2.7M × 384 dims × 4 bytes)\n",
    "- **ChromaDB:** ~10-15 GB (with metadata)\n",
    "- **Total storage:** ~30 GB for complete system\n",
    "\n",
    "---\n",
    "\n",
    "**Last Updated:** 2025-10-16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}