{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Resources & Citations\n",
    "\n",
    "**Purpose:** Centralized list of all research papers, tools, frameworks, and techniques used in this project\n",
    "\n",
    "**Last Updated:** 2025-10-16\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Papers & Academic Sources\n",
    "\n",
    "### RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval)\n",
    "- **Paper:** \"RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\"\n",
    "- **Venue:** ICLR 2024\n",
    "- **Link:** https://arxiv.org/abs/2401.18059\n",
    "- **Used for:** Hierarchical clustering and recursive summarization approach\n",
    "- **Key takeaway:** Uses SBERT embeddings, 100-token base chunks, hierarchical retrieval improves performance\n",
    "\n",
    "---\n",
    "\n",
    "### Sentence-BERT (SBERT)\n",
    "- **Paper:** \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\"\n",
    "- **Authors:** Reimers & Gurevych (2019)\n",
    "- **Link:** https://arxiv.org/abs/1908.10084\n",
    "- **Used for:** Embedding model selection (all-MiniLM-L6-v2 is from SBERT family)\n",
    "- **Key takeaway:** 10,000x faster than BERT for similarity search, optimized for semantic similarity\n",
    "\n",
    "---\n",
    "\n",
    "### MTEB (Massive Text Embedding Benchmark)\n",
    "- **Paper:** \"MTEB: Massive Text Embedding Benchmark\"\n",
    "- **Year:** 2022\n",
    "- **Link:** https://arxiv.org/abs/2210.07316\n",
    "- **Leaderboard:** https://huggingface.co/spaces/mteb/leaderboard\n",
    "- **Used for:** Validating embedding model quality (all-MiniLM-L6-v2 scores 56.26/100, top 20%)\n",
    "- **Key takeaway:** 58 embedding tasks for comprehensive evaluation\n",
    "\n",
    "---\n",
    "\n",
    "### Anthropic Contextual Retrieval\n",
    "- **Article:** \"Introducing Contextual Retrieval\"\n",
    "- **Date:** September 2024\n",
    "- **Link:** https://www.anthropic.com/news/contextual-retrieval\n",
    "- **Used for:** Contextual chunking strategy (50-100 token context window)\n",
    "- **Key takeaway:** 35% reduction in retrieval failures with contextual embeddings, 49% with BM25, 67% with reranking\n",
    "\n",
    "---\n",
    "\n",
    "### RAGAS (Retrieval-Augmented Generation Assessment)\n",
    "- **Paper:** \"Ragas: Automated Evaluation of Retrieval Augmented Generation\"\n",
    "- **Venue:** EACL 2024 (Demo Track)\n",
    "- **Link:** https://arxiv.org/abs/2309.15217\n",
    "- **ACL Anthology:** https://aclanthology.org/2024.eacl-demo.16/\n",
    "- **Docs:** https://docs.ragas.io/en/stable/\n",
    "- **Used for:** Evaluating RAG system performance (faithfulness, relevancy, precision, recall)\n",
    "- **Key takeaway:** Reference-free evaluation using LLMs, no need for human-annotated ground truth\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "\n",
    "### all-MiniLM-L6-v2 (Sentence Transformers)\n",
    "- **Model:** sentence-transformers/all-MiniLM-L6-v2\n",
    "- **Link:** https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "- **Docs:** https://www.sbert.net/\n",
    "- **Specifications:**\n",
    "  - Dimensions: 384\n",
    "  - Parameters: 22.7M\n",
    "  - Speed: ~1000 sentences/second on CPU\n",
    "  - Context window: 512 tokens\n",
    "  - Training: 1B+ sentence pairs\n",
    "- **Used for:** Generating embeddings for 2.7M SEC filing chunks\n",
    "- **Why chosen:** Best performance-to-size ratio, RAPTOR paper uses SBERT family, 200M+ downloads\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Models & Serving\n",
    "\n",
    "### Ollama\n",
    "- **Website:** https://ollama.com/\n",
    "- **GitHub:** https://github.com/ollama/ollama\n",
    "- **Python SDK:** https://github.com/ollama/ollama-python\n",
    "- **Used for:** Local LLM serving and inference\n",
    "- **Why chosen:** Easy local deployment, GGUF support, Docker-friendly\n",
    "\n",
    "---\n",
    "\n",
    "### gpt-oss (13 GB model)\n",
    "- **Pulled via:** `ollama pull gpt-oss`\n",
    "- **Size:** 13 GB\n",
    "- **Used for:** LLM inference and summarization testing\n",
    "- **Format:** GGUF (quantized)\n",
    "\n",
    "---\n",
    "\n",
    "### GGUF (GPT-Generated Unified Format)\n",
    "- **Description:** Quantized model format for efficient inference\n",
    "- **Used by:** Ollama, llama.cpp\n",
    "- **Quantization levels:**\n",
    "  - q2_K: 2-bit (smallest, fastest, lower quality)\n",
    "  - q4_K_M: 4-bit (balanced) ← Recommended\n",
    "  - q8_0: 8-bit (best quality, largest)\n",
    "- **Benefit:** 8x smaller than full precision (32-bit → 4-bit)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database\n",
    "\n",
    "### ChromaDB\n",
    "- **Website:** https://www.trychroma.com/\n",
    "- **Docs:** https://docs.trychroma.com/\n",
    "- **GitHub:** https://github.com/chroma-core/chroma\n",
    "- **Used for:** Storing and retrieving 2.7M chunk embeddings\n",
    "- **Why chosen:** Python-native, easy local setup, supports metadata filtering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries & Frameworks\n",
    "\n",
    "### LangChain\n",
    "- **Docs:** https://python.langchain.com/\n",
    "- **GitHub:** https://github.com/langchain-ai/langchain\n",
    "- **Used for:** LLM orchestration and RAG pipeline building\n",
    "\n",
    "---\n",
    "\n",
    "### Sentence Transformers\n",
    "- **Docs:** https://www.sbert.net/\n",
    "- **GitHub:** https://github.com/UKPLab/sentence-transformers\n",
    "- **PyPI:** https://pypi.org/project/sentence-transformers/\n",
    "- **Used for:** Loading and running embedding models (all-MiniLM-L6-v2)\n",
    "\n",
    "---\n",
    "\n",
    "### tiktoken\n",
    "- **GitHub:** https://github.com/openai/tiktoken\n",
    "- **PyPI:** https://pypi.org/project/tiktoken/\n",
    "- **Used for:** Token counting (cl100k_base encoding)\n",
    "- **Why chosen:** Accurate token counts for GPT-style tokenization\n",
    "\n",
    "---\n",
    "\n",
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "- **Paper:** https://arxiv.org/abs/1802.03426\n",
    "- **Docs:** https://umap-learn.readthedocs.io/\n",
    "- **PyPI:** https://pypi.org/project/umap-learn/\n",
    "- **Used for:** Dimensionality reduction before clustering (384 dims → 10 dims)\n",
    "- **Why chosen:** RAPTOR paper uses UMAP for clustering\n",
    "\n",
    "---\n",
    "\n",
    "### scikit-learn (GMM - Gaussian Mixture Models)\n",
    "- **Docs:** https://scikit-learn.org/\n",
    "- **GMM Docs:** https://scikit-learn.org/stable/modules/mixture.html\n",
    "- **Used for:** Clustering embeddings after UMAP reduction\n",
    "- **Why chosen:** RAPTOR uses GMM for hierarchical clustering\n",
    "\n",
    "---\n",
    "\n",
    "### NumPy\n",
    "- **Docs:** https://numpy.org/doc/\n",
    "- **Used for:** Array operations and embedding storage (.npy files)\n",
    "\n",
    "---\n",
    "\n",
    "### pandas\n",
    "- **Docs:** https://pandas.pydata.org/docs/\n",
    "- **Used for:** Data manipulation and statistics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "### SEC EDGAR 10-X Files (Notre Dame SRAF)\n",
    "- **Source:** Notre Dame Software Repository for Accounting and Finance (SRAF)\n",
    "- **Link:** https://sraf.nd.edu/sec-edgar-data/cleaned-10x-files/\n",
    "- **Description:** Pre-cleaned SEC 10-K and 10-Q filings (1993-2024)\n",
    "- **Format:** SRAF-XML wrapper around original HTML/XML/SGML filings\n",
    "- **Coverage:** 31 years of data (~51 GB total)\n",
    "- **Our dataset:** `10-X_C_2024.zip` (26,014 filings, 1.6 GB compressed)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment & Infrastructure\n",
    "\n",
    "### Docker\n",
    "- **Website:** https://www.docker.com/\n",
    "- **Docs:** https://docs.docker.com/\n",
    "- **Used for:** Containerizing LLM services, ChromaDB, and Open WebUI\n",
    "\n",
    "---\n",
    "\n",
    "### Docker Compose\n",
    "- **Docs:** https://docs.docker.com/compose/\n",
    "- **Used for:** Multi-container orchestration (Ollama + ChromaDB + Open WebUI)\n",
    "\n",
    "---\n",
    "\n",
    "### AWS EC2\n",
    "- **Docs:** https://docs.aws.amazon.com/ec2/\n",
    "- **Planned instance:** r6i.4xlarge (128 GB RAM, 16 vCPUs)\n",
    "- **Used for:** Production deployment of RAPTOR RAG system\n",
    "\n",
    "---\n",
    "\n",
    "### AWS S3\n",
    "- **Docs:** https://docs.aws.amazon.com/s3/\n",
    "- **Used for:** Storing GGUF models for EC2 deployment (free intra-region transfer)\n",
    "\n",
    "---\n",
    "\n",
    "### Open WebUI\n",
    "- **GitHub:** https://github.com/open-webui/open-webui\n",
    "- **Docs:** https://docs.openwebui.com/\n",
    "- **Used for:** Interactive web interface for querying RAG system\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation References\n",
    "\n",
    "### FinGPT RAPTOR Implementation\n",
    "- **GitHub:** https://github.com/AI4Finance-Foundation/FinGPT\n",
    "- **RAPTOR Code:** https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n",
    "- **Used for:** Reference implementation of RAPTOR for financial documents\n",
    "- **Key insight:** Adapted for SEC 10-K/10-Q filings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques & Methodologies\n",
    "\n",
    "### Contextual Chunking\n",
    "- **Source:** Anthropic Contextual Retrieval (Sept 2024)\n",
    "- **Link:** https://www.anthropic.com/news/contextual-retrieval\n",
    "- **Description:** Embed extended chunks (with context) but store only core chunks\n",
    "- **Our implementation:**\n",
    "  - Core chunk: 500 tokens (stored)\n",
    "  - Context window: 100 tokens (50 before + 50 after)\n",
    "  - Extended chunk: ~700 tokens (embedded)\n",
    "  - Header: Company, form type, filing date, CIK\n",
    "- **Expected improvement:** 35-49% better retrieval accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### RAPTOR Hierarchical Clustering\n",
    "- **Source:** RAPTOR Paper (ICLR 2024)\n",
    "- **Link:** https://arxiv.org/abs/2401.18059\n",
    "- **Description:** Multi-level clustering and recursive summarization\n",
    "- **Steps:**\n",
    "  1. Embed all chunks (SBERT)\n",
    "  2. Reduce dimensions (UMAP: 384 → 10 dims)\n",
    "  3. Cluster (GMM with BIC for optimal K)\n",
    "  4. Summarize at 3 levels (chunk → cluster → document)\n",
    "  5. Store summaries for hierarchical retrieval\n",
    "\n",
    "---\n",
    "\n",
    "### RAG (Retrieval-Augmented Generation)\n",
    "- **Description:** Retrieve relevant context, augment prompt, generate answer\n",
    "- **Pipeline:**\n",
    "  1. User query → embed query\n",
    "  2. Similarity search in ChromaDB → retrieve top-K chunks\n",
    "  3. Augment prompt with retrieved context\n",
    "  4. LLM generates answer based on context\n",
    "- **Variants tested:**\n",
    "  - **Baseline:** No RAG (LLM alone)\n",
    "  - **Simple RAG:** Basic retrieval (no RAPTOR)\n",
    "  - **RAPTOR RAG:** Hierarchical retrieval with cluster-aware context\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Frameworks\n",
    "\n",
    "### RAGAS Metrics\n",
    "- **Framework:** https://docs.ragas.io/en/stable/\n",
    "- **Metrics used:**\n",
    "  - **Faithfulness:** Does the answer align with retrieved context?\n",
    "  - **Answer Relevancy:** Does the answer address the question?\n",
    "  - **Context Precision:** Are relevant chunks ranked higher?\n",
    "  - **Context Recall:** Was all needed info retrieved?\n",
    "- **Used for:** Comparing Baseline vs Simple RAG vs RAPTOR RAG\n",
    "\n",
    "---\n",
    "\n",
    "### Manual Evaluation\n",
    "- **Method:** Human review of 5 test questions across 3 systems (15 answers total)\n",
    "- **Criteria:**\n",
    "  - Factual accuracy (verifiable from filings)\n",
    "  - Citation correctness (references right documents)\n",
    "  - Completeness (addresses all parts of question)\n",
    "  - No hallucinations (all claims grounded in context)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project-Specific Decisions\n",
    "\n",
    "### Chunk Size: 500 Tokens\n",
    "- **Source:** Multi-year prototype testing (archive_v1_multi_year/)\n",
    "- **Test range:** 200-8000 tokens (12 variants)\n",
    "- **Findings:** 500-1000 tokens optimal for SEC filings\n",
    "- **Chosen:** 500 tokens (balance between context and specificity)\n",
    "\n",
    "---\n",
    "\n",
    "### Context Window: 100 Tokens\n",
    "- **Source:** Anthropic research + RAPTOR paper\n",
    "- **Recommendation:** 50-100 tokens\n",
    "- **Our choice:** 100 tokens (50 before + 50 after)\n",
    "- **Rationale:** Aligns with both Anthropic and RAPTOR recommendations\n",
    "\n",
    "---\n",
    "\n",
    "### 2024 Data Only (vs Multi-Year)\n",
    "- **Decision:** 26K filings from 2024 instead of 1,375 samples across 1993-2024\n",
    "- **Rationale:**\n",
    "  - 19x more data\n",
    "  - Temporal consistency\n",
    "  - Better for clustering\n",
    "  - Cleaner baseline for prototyping\n",
    "- **Source:** Coworker suggestion, validated by research\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### SEC Filing Guides\n",
    "- **How to Read 10-K/10-Q:** https://www.sec.gov/resources-for-investors/investor-alerts-bulletins/how-read-10-k10-q\n",
    "- **SEC.gov:** https://www.sec.gov/\n",
    "\n",
    "---\n",
    "\n",
    "### Related Research\n",
    "- **LLM Analysis of 10-K/10-Q:** https://www.researchgate.net/publication/377746616_LLM_Analysis_of_10-K_and_10-Q_Filings_RAG_Results\n",
    "- **RAG Evaluation Study (Oct 2024):** https://www.mdpi.com/2076-3417/14/20/9318\n",
    "- **Financial Chatbot with RAG:** https://medium.com/@RobuRishabh/financial-analysis-chatbot-for-10-q-10-k-reports-using-retrieval-augmented-generation-rag-ef3938892086\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics (Our Project)\n",
    "\n",
    "### Data Processing\n",
    "- **Filings processed:** 26,014 (100% success rate)\n",
    "- **Total chunks:** 2,725,171\n",
    "- **Processing time:** 42.1 minutes\n",
    "- **Output size:** 14,957 MB (~15 GB)\n",
    "\n",
    "### Token Statistics\n",
    "- **Total tokens:** 1,356,067,955 (1.36 billion)\n",
    "- **Core tokens (stored):** 1,356,067,955\n",
    "- **Extended tokens (embedded):** 1,625,920,116\n",
    "- **Context overhead:** 19.9%\n",
    "- **Avg tokens/filing:** 52,128\n",
    "\n",
    "### Expected Outputs\n",
    "- **Embeddings:** ~4.2 GB (2.7M × 384 dims × 4 bytes)\n",
    "- **ChromaDB:** ~10-15 GB (with metadata)\n",
    "- **Total storage:** ~30 GB for complete system\n",
    "\n",
    "---\n",
    "\n",
    "**Last Updated:** 2025-10-16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
