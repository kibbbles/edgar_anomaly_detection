{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC EDGAR Data Collection - Technical Summary\n",
    "\n",
    "**Date:** October 8, 2025  \n",
    "**Updated:** Scope expanded to full SRAF dataset (2001-2024)  \n",
    "**Objective:** Document data collection approach and decisions for fraud detection project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "\n",
    "### Goal\n",
    "Develop an AI-powered fraud detection system using RAG (Retrieval-Augmented Generation) and optional classification models to analyze SEC EDGAR filings:\n",
    "- **10-K/10-Q Analysis:** Year-over-year changes in risk disclosures, financial statements, MD&A\n",
    "- **Focus:** All sections (not just Item 1A) to detect comprehensive fraud signals\n",
    "- **Optional 8-K Analysis:** Event-driven anomalies for supplementary validation\n",
    "\n",
    "### Known Fraud Cases (Validation Set)\n",
    "1. **Under Armour (UAA)** - Accounting fraud 2015-2017 (revenue manipulation)\n",
    "2. **Luckin Coffee (LK)** - Accounting fraud 2019-2020 (fabricated sales, delisted)\n",
    "3. **Nikola Corporation (NKLA)** - Securities fraud 2019-2020 (false product claims)\n",
    "\n",
    "### Final Dataset Scope (Updated October 8, 2025)\n",
    "- **Companies:** ALL companies in SRAF dataset (thousands)\n",
    "- **Time Period:** 2001-2024 (24 years)\n",
    "- **Primary Source:** Notre Dame SRAF 10-X Cleaned Files\n",
    "- **Supplementary Source:** Self-scraped 8-K filings (3,964 filings, 2015-2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Initial Data Collection Approach (Scrapers)\n",
    "\n",
    "### 2.1 Technology Stack\n",
    "```python\n",
    "# Core libraries\n",
    "import sec_edgar_downloader  # SEC API wrapper with rate limiting\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "```\n",
    "\n",
    "### 2.2 Custom Scrapers Built (Proof of Effort)\n",
    "\n",
    "**Note:** These scrapers were built to understand the data structure and validate that we could access SEC filings programmatically. After evaluation of external datasets, we switched to SRAF as the primary source (see Section 5).\n",
    "\n",
    "#### **10-K Scraper** (`src/data/edgar_10k_scraper.py`)\n",
    "- **Purpose:** Download annual 10-K reports for 23 selected companies\n",
    "- **Date Range:** 2015-2024 (captures Under Armour fraud period 2015-2017)\n",
    "- **Rate Limiting:** Built-in via `sec-edgar-downloader` (10 req/sec)\n",
    "- **Output:** `data/raw/sec-edgar-filings/{CIK}/10-K/{ACCESSION}/`\n",
    "  - `full-submission.txt` - Complete filing\n",
    "  - `primary-document.html` - Main 10-K document\n",
    "\n",
    "**Results:**\n",
    "- **230 filings** across 23 companies\n",
    "- **Fraud case coverage:**\n",
    "  - Under Armour (UAA): 11 filings\n",
    "  - Nikola (NKLA): 6 filings\n",
    "  - Luckin Coffee (LK): 0 filings (delisted)\n",
    "\n",
    "#### **8-K Scraper** (`src/data/edgar_8k_scraper.py`)\n",
    "- **Purpose:** Download event-driven 8-K filings for fraud indicators\n",
    "- **Target Items:**\n",
    "  - **Item 4.02:** Non-reliance on financials (restatements)\n",
    "  - **Item 5.02:** Departure of directors/officers\n",
    "  - **Item 8.01:** Other events (investigations, lawsuits)\n",
    "  - **Item 2.01:** Acquisition/disposition\n",
    "- **Date Range:** 2015-2024\n",
    "\n",
    "**Results:**\n",
    "- **3,964 filings** across 23 companies\n",
    "- **Fraud case coverage:**\n",
    "  - Under Armour (UAA): 141 filings\n",
    "  - Nikola (NKLA): 118 filings\n",
    "- **Kept as supplementary data source**\n",
    "\n",
    "### 2.3 Why We Built These Scrapers\n",
    "\n",
    "1. **Validate SEC API access** - Confirmed we could programmatically download filings\n",
    "2. **Understand data structure** - Learned about HTML/XML formatting variations\n",
    "3. **Fraud case validation** - Confirmed UAA and NKLA data availability\n",
    "4. **Backup data source** - 8-K filings provide supplementary event signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 File Structure\n",
    "\n",
    "```\n",
    "data/raw/sec-edgar-filings/\n",
    "‚îú‚îÄ‚îÄ {CIK}/                          # 10-digit company identifier\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 10-K/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ {ACCESSION_NUMBER}/     # e.g., 0000012927-24-000010\n",
    "‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ full-submission.txt # Complete HTML/XML filing\n",
    "‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ primary-document.html\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 8-K/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ {ACCESSION_NUMBER}/\n",
    "‚îÇ           ‚îú‚îÄ‚îÄ full-submission.txt\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ primary-document.html\n",
    "```\n",
    "\n",
    "**Storage:** ~2-3 GB total (uncompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Company Configuration\n",
    "\n",
    "Companies selected for sector diversity and fraud case representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total companies: 23\n",
      "\n",
      "Fraud Cases:\n",
      "  - LK: Luckin Coffee Inc. (Restaurants/Beverages)\n",
      "  - NKLA: Nikola Corporation (Automotive/Electric Vehicles)\n",
      "  - UAA: Under Armour Inc. (Consumer Goods/Apparel)\n",
      "\n",
      "Sector Distribution:\n",
      "  Technology: 3\n",
      "  Financial Services: 3\n",
      "  Healthcare/Pharma: 2\n",
      "  Retail: 2\n",
      "  Energy: 2\n",
      "  Consumer Goods: 2\n",
      "  E-commerce/Tech: 1\n",
      "  Automotive/Tech: 1\n",
      "  Healthcare: 1\n",
      "  Aerospace/Defense: 1\n",
      "  Entertainment/Media: 1\n",
      "  Beverages: 1\n",
      "  Restaurants/Beverages: 1\n",
      "  Automotive/Electric Vehicles: 1\n",
      "  Consumer Goods/Apparel: 1\n"
     ]
    }
   ],
   "source": [
    "# Load company list\n",
    "import json\n",
    "\n",
    "with open('../config/companies.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"Total companies: {len(config['companies'])}\\n\")\n",
    "\n",
    "# Show fraud cases\n",
    "fraud_cases = [c for c in config['companies'] if c.get('fraud_case', False)]\n",
    "print(\"Fraud Cases:\")\n",
    "for company in fraud_cases:\n",
    "    print(f\"  - {company['ticker']}: {company['name']} ({company['sector']})\")\n",
    "\n",
    "# Sector breakdown\n",
    "from collections import Counter\n",
    "sectors = Counter(c['sector'] for c in config['companies'])\n",
    "print(\"\\nSector Distribution:\")\n",
    "for sector, count in sectors.most_common():\n",
    "    print(f\"  {sector}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Download Statistics\n",
    "\n",
    "### 3.1 Load Download Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 10-K Download Summary ===\n",
      "Companies: 23\n",
      "Target Years: [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "Total Filings: 230\n",
      "\n",
      "Top 5 by filing count:\n",
      "  Ticker  Count\n",
      "1   MSFT     11\n",
      "4   TSLA     11\n",
      "3   AMZN     11\n",
      "5    JPM     11\n",
      "6    BAC     11\n"
     ]
    }
   ],
   "source": [
    "# Load 10-K summary\n",
    "with open('../data/raw/download_summary.json', 'r') as f:\n",
    "    summary_10k = json.load(f)\n",
    "\n",
    "print(\"=== 10-K Download Summary ===\")\n",
    "print(f\"Companies: {summary_10k['companies']}\")\n",
    "print(f\"Target Years: {summary_10k['target_years']}\")\n",
    "print(f\"Total Filings: {summary_10k['total_filings']}\")\n",
    "print(f\"\\nTop 5 by filing count:\")\n",
    "import pandas as pd\n",
    "df_10k = pd.DataFrame(list(summary_10k['by_company'].items()), columns=['Ticker', 'Count'])\n",
    "print(df_10k.sort_values('Count', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 8-K Download Summary ===\n",
      "Total Filings: 3967\n",
      "Critical Items: ['4.02', '5.02', '8.01', '2.01']\n",
      "\n",
      "Top 5 by filing count:\n",
      "   Ticker  Count\n",
      "7     WFC    870\n",
      "5     JPM    344\n",
      "19     PG    229\n",
      "4    TSLA    184\n",
      "10    UNH    171\n"
     ]
    }
   ],
   "source": [
    "# Load 8-K summary\n",
    "with open('../data/raw/download_summary_8k.json', 'r') as f:\n",
    "    summary_8k = json.load(f)\n",
    "\n",
    "print(\"=== 8-K Download Summary ===\")\n",
    "print(f\"Total Filings: {summary_8k['total_filings']}\")\n",
    "print(f\"Critical Items: {summary_8k['critical_items']}\")\n",
    "print(f\"\\nTop 5 by filing count:\")\n",
    "df_8k = pd.DataFrame(list(summary_8k['by_company'].items()), columns=['Ticker', 'Count'])\n",
    "print(df_8k.sort_values('Count', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fraud Case Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Case Filing Coverage:\n",
      "\n",
      "UAA - 2015-2017 (accounting fraud - revenue manipulation)\n",
      "  10-K: 11 filings\n",
      "  8-K: 141 filings\n",
      "\n",
      "NKLA - 2019-2020 (securities fraud - false product claims)\n",
      "  10-K: 6 filings\n",
      "  8-K: 118 filings\n",
      "\n",
      "LK - 2019-2020 (accounting fraud - fabricated sales, delisted)\n",
      "  10-K: 0 filings\n",
      "  8-K: 0 filings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_tickers = ['UAA', 'NKLA', 'LK']\n",
    "fraud_periods = {\n",
    "    'UAA': '2015-2017 (accounting fraud - revenue manipulation)',\n",
    "    'NKLA': '2019-2020 (securities fraud - false product claims)',\n",
    "    'LK': '2019-2020 (accounting fraud - fabricated sales, delisted)'\n",
    "}\n",
    "\n",
    "print(\"Fraud Case Filing Coverage:\\n\")\n",
    "for ticker in fraud_tickers:\n",
    "    count_10k = summary_10k['by_company'].get(ticker, 0)\n",
    "    count_8k = summary_8k['by_company'].get(ticker, 0)\n",
    "    print(f\"{ticker} - {fraud_periods[ticker]}\")\n",
    "    print(f\"  10-K: {count_10k} filings\")\n",
    "    print(f\"  8-K: {count_8k} filings\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Decision to Switch to SRAF Dataset\n",
    "\n",
    "### 4.1 Problem with Scraper Approach\n",
    "\n",
    "**Limitations:**\n",
    "- **Limited scope:** Only 23 companies (not enough for robust model)\n",
    "- **Time period:** Only 2015-2024 (missing historical context)\n",
    "- **Manual selection:** Potential selection bias\n",
    "- **Rate limiting:** Slow downloads (10 req/sec)\n",
    "- **Processing burden:** Need to parse HTML/XML ourselves\n",
    "\n",
    "### 4.2 External Dataset Evaluation\n",
    "\n",
    "Evaluated 4 datasets (see `external_datasets_evaluation.xlsx`):\n",
    "\n",
    "1. **‚ùå Kaggle - SEC EDGAR Company Facts (Lang)** - 13.86 GB of XBRL numbers only\n",
    "2. **‚ùå Kaggle - Parsed 10-Q Filings (Aenlle)** - Quarterly metrics only\n",
    "3. **‚ùå Kaggle - Company Facts v2 (Vanak)** - 75.4M rows of numerical facts\n",
    "4. **‚úÖ Notre Dame SRAF 10-X Files** - **FULL TEXT** of 10-K/10-Q (1993-2024)\n",
    "\n",
    "**Key Finding:** All Kaggle datasets contain only structured XBRL numbers (no narrative text). SRAF is the only source with full filing text needed for NLP fraud detection.\n",
    "\n",
    "### 4.3 SRAF Dataset Verification\n",
    "\n",
    "**Test:** Extracted sample 10-K from 2016-2020 zip file\n",
    "\n",
    "```bash\n",
    "unzip -p \"10-X_C_2016-2020.zip\" \"2016/QTR1/20160104_10-K_*.txt\" | head -100\n",
    "```\n",
    "\n",
    "**Result:** Found actual Item 1A Risk Factors text, MD&A sections, full narrative content ‚úÖ\n",
    "\n",
    "**Decision:** Use SRAF as primary source, keep scraped 8-K data as supplementary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. SRAF Dataset - Final Data Source\n",
    "\n",
    "### 5.1 Dataset Overview\n",
    "\n",
    "**Source:** Notre Dame Software Repository for Accounting and Finance (SRAF)  \n",
    "**URL:** https://sraf.nd.edu/sec-edgar-data/  \n",
    "**Content:** Full text of 10-K and 10-Q filings (cleaned format)  \n",
    "**Coverage:** 1993-2024 (31 years)\n",
    "\n",
    "### 5.2 Downloaded Files\n",
    "\n",
    "All files stored in `data/external/`:\n",
    "\n",
    "| File | Period | Status |\n",
    "|------|--------|--------|\n",
    "| 10-X_C_2001-2005.zip | 2001-2005 | ‚úÖ Downloaded |\n",
    "| 10-X_C_2006-2010.zip | 2006-2010 | ‚úÖ Downloaded |\n",
    "| 10-X_C_2011-2015.zip | 2011-2015 | ‚úÖ Downloaded |\n",
    "| 10-X_C_2016-2020.zip | 2016-2020 | ‚úÖ Downloaded |\n",
    "| 10-X_C_2021.zip | 2021 | ‚úÖ Downloaded |\n",
    "| 10-X_C_2022.zip | 2022 | ‚úÖ Downloaded |\n",
    "| 10-X_C_2023.zip | 2023 | ‚úÖ Downloaded |\n",
    "| 10-X_C_2024.zip | 2024 | ‚úÖ Downloaded |\n",
    "\n",
    "**Total:** 8 files, ~42 GB compressed\n",
    "\n",
    "### 5.3 File Structure\n",
    "\n",
    "```\n",
    "data/external/10-X_C_2016-2020.zip\n",
    "    ‚îî‚îÄ‚îÄ 2016/\n",
    "        ‚îî‚îÄ‚îÄ QTR1/\n",
    "            ‚îî‚îÄ‚îÄ 20160104_10-K_edgar_data_12345_0001234567-16-000123.txt\n",
    "```\n",
    "\n",
    "**Naming convention:** `YYYYMMDD_FORM_edgar_data_CIK_ACCESSION.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Current Status\n",
    "\n",
    "### ‚úÖ Completed\n",
    "- [x] Built 10-K scraper (230 filings, 23 companies)\n",
    "- [x] Built 8-K scraper (3,964 filings, 23 companies)\n",
    "- [x] Evaluated external datasets (Kaggle + SRAF)\n",
    "- [x] Downloaded SRAF 10-X files (2001-2024, ~42 GB compressed)\n",
    "- [x] Verified SRAF contains full text (not just numbers)\n",
    "\n",
    "### üìã Next Steps (See `01_project_plan.ipynb` for details)\n",
    "1. **Text Extraction:** Extract full text from SRAF zip files\n",
    "2. **Embeddings:** Create vector representations of filings\n",
    "3. **Vector Database:** Store embeddings (ChromaDB or FAISS)\n",
    "4. **RAG Pipeline:** Build retrieval system with FinGPT/Ollama\n",
    "5. **Interface:** Set up Open WebUI for team interaction\n",
    "6. **Deployment:** Deploy on AWS EC2 with GPU\n",
    "7. **Optional:** Train fraud classifier for automated detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Technical Notes\n",
    "\n",
    "### SEC API Compliance (Scrapers)\n",
    "- **User-Agent Required:** Set in `.env` as `SEC_USER_AGENT`\n",
    "- **Rate Limit:** 10 requests/second (enforced by `sec-edgar-downloader`)\n",
    "- **Retry Logic:** Built-in for 503 errors\n",
    "- **Errors Encountered:** 3 temporary 503 errors during 8-K download\n",
    "\n",
    "### SRAF Data Quality\n",
    "- **Format:** Pre-cleaned .txt files (HTML/XML already parsed)\n",
    "- **Consistency:** Standardized structure across years\n",
    "- **Coverage:** Comprehensive (all public companies filing 10-K/10-Q)\n",
    "- **Advantages over raw SEC data:**\n",
    "  - No HTML parsing needed\n",
    "  - Consistent formatting\n",
    "  - Bulk download (vs. API rate limits)\n",
    "  - Historical data readily available\n",
    "\n",
    "### Storage Considerations\n",
    "- **Git exclusions:** `data/raw/`, `data/processed/`, `data/external/` in `.gitignore`\n",
    "- **Current storage:** ~42 GB compressed, ~100+ GB uncompressed (estimated)\n",
    "- **Processing approach:** Stream from zip files (avoid uncompressing all at once)\n",
    "- **Embeddings storage:** ~1-2 GB for vector database (manageable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. References\n",
    "\n",
    "### Fraud Cases\n",
    "- **Under Armour:** SEC charges (2021) - https://www.sec.gov/newsroom/press-releases/2021-78\n",
    "- **Luckin Coffee:** SEC settlement (2020) - https://www.sec.gov/newsroom/press-releases/2020-319\n",
    "- **Nikola:** Trevor Milton indictment (2021) - https://www.sec.gov/newsroom/press-releases/2021-141\n",
    "\n",
    "### Technical Resources\n",
    "- SEC EDGAR API: https://www.sec.gov/edgar/sec-api-documentation\n",
    "- `sec-edgar-downloader`: https://github.com/jadchaar/sec-edgar-downloader\n",
    "- SRAF Datasets: https://sraf.nd.edu/sec-edgar-data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
