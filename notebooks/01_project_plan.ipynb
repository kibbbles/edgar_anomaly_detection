{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8241fd",
   "metadata": {},
   "source": "# SEC 10-K/10-Q Analysis - RAPTOR RAG Project Plan\n\n## Project Overview\nAI-powered system for analyzing complete SEC 10-K and 10-Q filings (1993-2024) using RAPTOR RAG (Recursive Adaptive Processing and Topical Organizational Retrieval). The system will create an enhanced knowledge base from financial filings that users can query interactively to identify year-over-year changes, patterns, and potential anomalies.\n\n**Data Coverage:** 1993-2024 (31 years of SEC EDGAR filings, ~51 GB)\n\n---\n\n## Core Architecture\n\n### Infrastructure\n- **Development Environment**: Local Windows laptop with Ollama + Docker Desktop\n- **Production Deployment**: AWS EC2 Instance: secAI (8 vCPUs, 64 GB RAM, Ubuntu 24.04)\n- **Containerization**: Docker Compose for service orchestration\n- **Model Hosting**: Ollama (installed on EC2, not containerized)\n- **User Interface**: Open WebUI (containerized)\n- **Vector Database**: ChromaDB (containerized, port 8000)\n- **Data Storage**: EC2 EBS volume for processed embeddings and knowledge base\n\n### Deployment Architecture\n\n**Development (Phase 2):**\n```\nLocal Laptop (32 GB RAM)\n├── Docker Compose\n│   ├── Ollama Container (gpt-oss 13 GB)\n│   ├── RAPTOR API Container (Python app)\n│   └── Open WebUI Container\n└── Local Volumes\n    ├── ./data/external (sample data)\n    ├── ./data/embeddings (ChromaDB)\n    └── ./models (Ollama models)\n```\n\n**Production (Phase 4):**\n```\nAWS EC2 Instance: secAI (8 vCPUs, 64 GB RAM, Ubuntu 24.04)\n├── Ollama (installed directly, not containerized)\n├── Docker Containers\n│   ├── ChromaDB Container (port 8000)\n│   ├── Open WebUI Container\n│   └── Data Processing Containers (chunking, embedding)\n└── /app/data/ (EBS Volume)\n    ├── edgar/extracted/ (51 GB SEC filings - 1993-2024, organized by year/quarter)\n    ├── processed/ (chunked filings with metadata)\n    ├── embeddings/ (ChromaDB vector DB)\n    └── models/ (Ollama models)\n```\n\n### Architecture Diagram\n\n![System Architecture](diagrams/architecture.png)\n\n### RAPTOR RAG System\nUnlike traditional RAG systems that use simple similarity search, RAPTOR implements:\n- **Hierarchical Clustering**: Multi-level organization (global + local) using UMAP and Gaussian Mixture Models\n- **Recursive Summarization**: 3-level hierarchical summaries capturing both granular details and high-level concepts\n- **Enhanced Context Retrieval**: Cluster-aware retrieval providing richer context for LLM queries\n\n---\n\n## Technical Stack\n\n### NLP & ML\n\n**Development Model (Phase 2):**\n- **gpt-oss** (13 GB) - General purpose with reasoning capabilities\n- Suitable for: Local testing, rapid iteration, pipeline validation\n- RAM requirement: ~16-20 GB\n\n**Production Model (Phase 4):**\n- **llama3-sec** (Arcee AI, Llama-3-70B based, fine-tuned for SEC filings)\n  - Source: `arcee-ai/llama3-sec` on Ollama\n  - Trained on 72B tokens of SEC filing data (20B checkpoint available)\n  - Deployment: Via Ollama in Docker container\n  - Specialized for: SEC data analysis, investment analysis, risk assessment\n\n**Fallback Models:**\n- qwen2.5:1.5b (986 MB) - Lightweight for quick testing\n\n**RAPTOR Implementation:**\n- Adapted from FinGPT's `FinancialReportAnalysis/utils/rag.py`\n- Source: https://github.com/AI4Finance-Foundation/FinGPT\n- Custom implementation in `src/models/raptor.py`\n\n**Embeddings & Clustering:**\n- Sentence Transformers (`all-MiniLM-L6-v2`) for local, cost-free embedding generation (GPU-accelerated)\n- UMAP (dimensionality reduction) + scikit-learn GMM for clustering\n- LLM Interface: Ollama Python client\n\n### Infrastructure & Deployment\n\n**Containerization:**\n- Docker + Docker Compose for all services\n- Reproducible environments (dev → production)\n- Service isolation and orchestration\n\n**AWS EC2 Configuration (Production):**\n- **Instance Name**: secAI\n- **Instance Type**: t3.xlarge\n  - 8 vCPUs\n  - 64 GB RAM (upgraded from 32 GB)\n- **Storage**: EBS volume at /app/data\n  - 51 GB raw data (1993-2024, ~1.2M files)\n  - Organized: /app/data/edgar/extracted/YEAR/QTRN/\n  - Processed output: /app/data/processed/\n- **OS**: Ubuntu 24.04 LTS\n- **Security**: VPC with restricted security groups, SSH key access\n\n**Data Processing:**\n- **Chunking Strategy**: 500-token chunks with 100-token contextual window (Anthropic's approach)\n  - Base chunk: 500 tokens (semantic unit)\n  - Contextual summary: 100-token LLM-generated summary prepended to each chunk\n  - Total overhead: ~19.9% additional tokens for context\n  - Library: `tiktoken` for accurate token counting\n- **Vector Storage**: ChromaDB (containerized, port 8000)\n- **Data Format**: JSON for structured storage (metadata + chunks)\n\n### Libraries\n- `langchain`, `langchain_community` - LLM orchestration\n- `sentence-transformers` - Local embeddings (GPU-accelerated)\n- `umap-learn` - Dimensionality reduction\n- `scikit-learn` - Clustering algorithms (GMM)\n- `pandas`, `numpy` - Data manipulation\n- `requests` - SEC EDGAR API access\n- `ollama` - Python client for Ollama\n- `docker`, `docker-compose` - Containerization\n- `tiktoken` - Token counting for chunking\n- `tqdm` - Progress bars\n\n---\n\n## Data Scope\n\n### Current Data Holdings\n- **Time Period:** 1993-2024 (31 years)\n- **Data Size:** ~51 GB\n- **Data Location:** `/app/data/edgar/extracted/` on EC2\n- **File Count:** ~1.2 million files\n- **Organization:** YEAR/QTRN structure (e.g., 2024/QTR1/)\n- **Filing Types:** Complete 10-K (annual reports) and 10-Q (quarterly filings)\n- **Processing Scope:** Full filing text (all sections)\n- **Analysis Focus:** Year-over-year changes, topic trends, boilerplate vs. substantive disclosure\n\n**Why Process Complete Filings:**\n- RAG enables users to ask questions about ANY section (not just risk factors)\n- RAPTOR clustering naturally organizes content by topic regardless of section\n- Maximizes system flexibility and future-proofs the knowledge base\n- Supports queries like: \"How did revenue recognition policies change?\" or \"Compare executive compensation across years\"\n- Historical coverage (1993-2024) enables long-term trend analysis\n\n---\n\n## RAPTOR Pipeline Flowchart\n\n![RAPTOR Pipeline](diagrams/raptor_pipeline.png)\n\n---\n\n## Data Processing Workflow\n\n![Data Processing Workflow](diagrams/data_processing_workflow.png)\n\n---\n\n## Implementation Phases\n\n### Phase 1: Model Research & Setup (Week 1) ✅ COMPLETE\n**Objectives:**\n- [x] Clarify FinGPT components: RAPTOR (Python implementation)\n- [x] Set up Ollama and test local LLM deployment\n- [x] Evaluate and download appropriate models for SEC filing analysis\n- [x] Set up project structure (`src/`, `data/`, `notebooks/`, `dashboard/`)\n- [ ] Copy and adapt RAPTOR class from FinGPT's `rag.py` (deferred to Phase 3)\n- [ ] Create base `Raptor` class skeleton in `src/models/raptor.py` (deferred to Phase 3)\n\n**Completed Actions:**\n- ✅ Installed Ollama v0.12.5 on Windows\n- ✅ Downloaded qwen2.5:1.5b (986 MB) - lightweight model for testing\n- ✅ Downloaded gpt-oss (13 GB) - reasoning-capable model for development\n- ✅ Verified Python ollama package integration\n- ✅ Tested model inference via command line and Python\n\n**Model Selection Strategy:**\n- **Development (Phase 2):** gpt-oss (13 GB) on local laptop\n- **Production (Phase 4):** llama3-sec on AWS EC2 g6.2xlarge\n- **Rationale:** Start with smaller model for faster iteration, scale to specialized model in production\n\n**Key Clarification:**\n- ✅ **RAPTOR** = Python implementation for hierarchical clustering/summarization (we copy the code)\n- ✅ **llama3-sec** = Domain-specific SEC filing model (production)\n- ✅ **gpt-oss** = General purpose reasoning model (development)\n\n---\n\n### Phase 2: Data Processing Pipeline (Week 2) - IN PROGRESS\n**Objectives:**\n- [x] Extract filings from archives (complete - 1.2M files on EC2)\n- [x] Parse complete 10-K/10-Q text from HTML/XML/SGML formats (complete)\n- [x] Implement document chunking with contextual embedding (500-token chunks + 100-token context)\n- [ ] Generate embeddings using local Sentence Transformers\n- [ ] Store structured data (chunks + metadata) in JSON\n- [x] Set up Docker for data processing containers\n\n**Key Files:**\n- `src/data/filing_extractor.py` - Unzip archives, extract full filing text\n- `src/data/text_processor.py` - Clean text, chunk into 500-token segments with metadata extraction\n- `src/models/embedding_generator.py` - Embedding creation\n- `src/Dockerfile` - Data processing container definition\n- `docker-compose.chunking.yml` - Chunking service orchestration\n- `deploy_and_run_chunking.py` - Automated deployment to EC2\n\n**Docker-Based Processing:**\n- Built image: `edgar-chunking` (8.4 GB, Python 3.12 + dependencies)\n- Volume mounts:\n  - Input: `/app/data/edgar/` (read-only)\n  - Output: `/app/data/processed/`\n- Resource limits: 4 CPUs, 8GB memory\n- Deployment: Automated via `deploy_and_run_chunking.py`\n\n**Current Status:**\n- Complete dataset extracted on EC2 (1.2M files, 1993-2024)\n- Docker image built successfully on EC2\n- Ready to process 2024 Q1 as initial test (~6,337 files)\n- Chunking implementation: 500 tokens + 100-token context (Anthropic method)\n\n**Validation:**\n- Test on 2024 Q1 first (single quarter)\n- Verify text extraction accuracy and metadata capture\n- Confirm chunking preserves semantic coherence\n- Validate JSON output structure\n\n---\n\n### Phase 3: RAPTOR System Implementation (Week 3)\n**Objectives:**\n- [ ] Copy RAPTOR class from FinGPT to `src/models/raptor.py`\n- [ ] Implement hierarchical clustering (adapted from FinGPT's RAPTOR):\n  - Global clustering (UMAP → GMM with BIC for optimal cluster count)\n  - Local clustering (secondary refinement within global clusters)\n- [ ] Build recursive summarization engine (3 levels deep) using llama3-sec\n- [ ] Create enhanced knowledge base combining:\n  - Original document chunks\n  - Level 1 summaries (cluster summaries)\n  - Level 2 summaries (summary of summaries)\n  - Level 3 summaries (highest abstraction)\n- [ ] Implement cluster-aware retrieval mechanism\n- [ ] Test on sample data with gpt-oss\n\n**Key Methods in `Raptor` class (adapted from FinGPT):**\n```python\ndef global_cluster_embeddings(embeddings, dim, n_neighbors, metric=\"cosine\")\ndef local_cluster_embeddings(embeddings, dim, num_neighbors=10)\ndef get_optimal_clusters(embeddings, max_clusters=50)\ndef GMM_cluster(embeddings, threshold, random_state=0)\ndef perform_clustering(embeddings, dim, threshold)\ndef recursive_embed_cluster_summarize(texts, level=1, n_levels=3)\n```\n\n**Source Reference:**\n- Original implementation: https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n\n**Testing:**\n- Validate clustering quality on sample documents\n- Review generated summaries for coherence\n- Ensure topics are properly grouped (e.g., all revenue-related content clusters together)\n- Benchmark with gpt-oss before scaling to production\n\n---\n\n### Phase 4: Production Deployment on AWS EC2 (Week 4-5)\n**Objectives:**\n- [x] Provision AWS EC2 instance (secAI - 8 vCPUs, 64GB RAM)\n- [x] Set up Docker + Docker Compose on EC2\n- [x] Deploy Ollama (installed directly, not containerized)\n- [x] Deploy ChromaDB container (port 8000)\n- [x] Deploy Open WebUI container\n- [ ] Deploy RAPTOR API container with full pipeline\n- [ ] Process complete 51 GB dataset into knowledge base\n- [ ] Implement query handling for diverse topics across filings\n- [ ] Create sample query templates for common use cases\n- [ ] Set up monitoring and logging\n\n**EC2 Setup Steps:**\n1. ✅ Launch EC2 instance (secAI - t3.xlarge, 64GB RAM)\n2. ✅ Attach EBS volume at /app/data\n3. ✅ Install Docker + Docker Compose\n4. ✅ Install Ollama directly on EC2\n5. ✅ Clone repository to EC2\n6. ✅ Extract all SEC filings (1993-2024, 1.2M files)\n7. ✅ Deploy ChromaDB container (port 8000)\n8. ✅ Deploy Open WebUI container\n9. [ ] Process full 51 GB dataset with chunking pipeline\n10. [ ] Generate embeddings and store in ChromaDB\n\n**Integration Workflow:**\n1. User submits query via Open WebUI (web interface)\n2. RAPTOR retrieves relevant chunks + hierarchical summaries from ChromaDB\n3. Context passed to Ollama LLM (llama3-sec) via API\n4. LLM generates response with supporting evidence\n5. Results displayed in WebUI with citations\n\n**Example Queries:**\n- \"What cyber risks did Apple disclose in 2023?\"\n- \"How have revenue recognition policies evolved from 2010 to 2024?\"\n- \"Compare executive compensation disclosures between tech companies\"\n- \"Show boilerplate vs. substantive language in risk disclosures\"\n\n**Deliverables:**\n- Functional production system on EC2\n- Dockerized, reproducible deployment\n- Full 51 GB dataset processed into knowledge base\n- Open WebUI interface accessible\n- Documentation for deployment and maintenance\n\n---\n\n## Understanding Docker: A Beginner's Guide\n\n### What is Docker?\n\nDocker is a tool that packages software and all its dependencies into standardized units called **containers**. Think of it like shipping containers for software - everything your code needs to run is packaged together in one box.\n\n### Docker Images vs. Docker Containers\n\n**Docker Image:**\n- A **blueprint** or **recipe** containing your code + dependencies + configuration\n- Stored on disk, doesn't run by itself\n- Like an MP3 file sitting on your hard drive\n- Example: `edgar-chunking` image (8.4 GB) contains Python 3.12 + tiktoken + tqdm + our chunking script\n\n**Docker Container:**\n- A **running instance** of an image\n- Actively executing your code\n- Like playing an MP3 file (the music you hear)\n- Example: When we run `docker run edgar-chunking`, it creates a container that executes our chunking script\n\n**Analogy:**\n- **Image** = Recipe for chocolate chip cookies (stored in a cookbook)\n- **Container** = Actual cookies baking in the oven (active process)\n\n### Why Use Docker for This Project?\n\n**1. Consistency Across Environments**\n- Your laptop, EC2 instance, teammate's machine - all run the same code the exact same way\n- \"It works on my machine\" problem solved\n\n**2. Dependency Management**\n- No need to manually install Python, tiktoken, tqdm, etc. on EC2\n- Everything bundled in the image\n- Avoids version conflicts\n\n**3. Isolation**\n- Each container runs independently\n- ChromaDB container won't interfere with Ollama container\n- Different services can use different Python versions if needed\n\n**4. Reproducibility**\n- Dockerfile is code - checked into git\n- Anyone can rebuild the exact same environment\n- Deployment = `docker-compose up`\n\n**5. Easy Deployment**\n- Build image once, run anywhere\n- Update code → rebuild image → deploy new container\n- No manual server configuration\n\n### Our Docker Setup\n\n**Current Docker Images on EC2:**\n- `edgar-chunking` (8.4 GB) - Contains Python 3.12 + all dependencies + chunking script\n- Used to process SEC filing text into 500-token chunks\n\n**How We Use Docker:**\n1. Write code locally (e.g., `text_processor.py`)\n2. Create `Dockerfile` (recipe for building the image)\n3. Upload files to EC2 via SCP\n4. Build image on EC2: `docker build -t edgar-chunking .`\n5. Run container: `docker run edgar-chunking` or `docker compose run chunking`\n6. Container executes our script, outputs processed data to `/app/data/processed/`\n\n**Docker Compose:**\n- Tool for managing multiple containers at once\n- We use `docker-compose.chunking.yml` to configure:\n  - Which image to use (`edgar-chunking`)\n  - Volume mounts (share data between host and container)\n  - Resource limits (CPU, memory)\n  - Command to run\n\n**Volume Mounts Explained:**\n- Volumes let containers access files on the host machine\n- Example: `-v /app/data/edgar:/app/data/edgar:ro`\n  - `/app/data/edgar` on EC2 host → `/app/data/edgar` inside container\n  - `:ro` means read-only (container can't modify input data)\n- Output: `-v /app/data/processed:/app/data/processed` (read-write)\n\n---\n\n## Docker Strategy\n\n### Development vs. Production\n\n**Development (Local Laptop):**\n- Purpose: Build and test pipeline with sample data\n- Model: gpt-oss (13 GB) - fits in 32 GB RAM\n- Data: Sample filings (~1-5 GB)\n- Services: Ollama + RAPTOR API + Open WebUI\n- Command: `docker-compose -f docker-compose.dev.yml up`\n\n**Production (AWS EC2):**\n- Purpose: Full-scale deployment with complete dataset\n- Model: llama3-sec - optimized for SEC filings\n- Data: Complete 51 GB SEC filings (1993-2024)\n- Services: Ollama (installed) + ChromaDB (containerized) + Open WebUI (containerized) + Data processing (containerized)\n- Command: `docker compose -f docker-compose.chunking.yml run --rm chunking`\n\n### Benefits of Docker Approach:\n\n1. **Reproducibility**: Same environment dev → production\n2. **Isolation**: Services don't conflict (different Python versions, dependencies)\n3. **Portability**: Works on laptop, EC2, teammate's machine\n4. **Scalability**: Easy to add services (monitoring, caching, etc.)\n5. **Version Control**: Infrastructure as code (`Dockerfile`, `docker-compose.yml`)\n6. **Easy Deployment**: `git pull && docker-compose up` deploys updates\n7. **Resource Management**: Set CPU/memory limits per container\n\n### Repository Structure with Docker\n```\nedgar_anomaly_detection/\n├── data/\n│   ├── external/         # Downloaded filings (gitignored)\n│   ├── processed/        # Chunked filings (gitignored)\n│   └── embeddings/       # ChromaDB files (gitignored)\n├── src/\n│   ├── Dockerfile        # Data processing container definition\n│   ├── requirements.txt  # Python dependencies\n│   ├── data/\n│   │   ├── filing_extractor.py\n│   │   └── text_processor.py\n│   ├── models/\n│   │   ├── raptor.py\n│   │   ├── embedding_generator.py\n│   │   └── clustering.py\n│   └── api/\n│       └── main.py       # FastAPI server for RAPTOR\n├── notebooks/\n│   └── 01_project_plan.ipynb\n├── docker-compose.chunking.yml   # Chunking service\n├── docker-compose.dev.yml        # Development setup\n├── docker-compose.prod.yml       # Production setup (EC2)\n├── deploy_and_run_chunking.py    # Automated deployment script\n├── .dockerignore\n├── .gitignore\n├── requirements.txt\n└── README.md\n```\n\n---\n\n## RAPTOR vs. Traditional RAG Comparison\n\n| Feature | Traditional RAG | RAPTOR RAG |\n|---------|----------------|------------|\n| Text Processing | Simple chunking | Recursive, hierarchical |\n| Clustering | None or basic | Multi-level (global + local) |\n| Summarization | None or single-level | Recursive, 3-level |\n| Context Selection | Similarity-based only | Cluster-aware + similarity |\n| Document Understanding | Flat representation | Hierarchical representation |\n| Knowledge Integration | Direct chunks only | Chunks + multi-level summaries |\n\n**Why RAPTOR for Financial Filings?**\n- Financial documents have hierarchical structure (sections, subsections, themes)\n- YoY analysis requires understanding both granular changes and high-level shifts\n- Boilerplate detection benefits from cluster analysis (repetitive language clusters together)\n- Complex queries need multi-level context (e.g., \"How did regulatory disclosures evolve?\")\n- Historical coverage (1993-2024) enables long-term trend analysis\n\n---\n\n## Success Metrics\n- [ ] Successfully process 90%+ of downloaded filings (1993-2024) into knowledge base\n- [ ] Clustering produces coherent, interpretable topic groups\n- [ ] Generated summaries accurately capture content at each hierarchical level\n- [ ] LLM queries return relevant, accurate responses with supporting evidence\n- [ ] System responds to queries in <10 seconds (including retrieval + generation)\n- [ ] Manual validation: Test 10 diverse queries across different topics and decades, verify accuracy\n- [ ] Docker deployment: Services start successfully on both dev and production\n- [ ] EC2 deployment: System runs stably for 7+ days without intervention\n\n---\n\n## Key Advantages of AI-First Approach\n1. **No Manual Feature Engineering**: LLM infers patterns from enhanced context (vs. building YoY diff algorithms)\n2. **Flexible Queries**: Users can ask arbitrary questions about ANY topic or section\n3. **Semantic Understanding**: Detects substantive changes even when wording differs\n4. **Scalable**: Adding new filings just requires re-running RAPTOR pipeline\n5. **Explainable**: LLM can cite specific sections supporting its conclusions\n6. **Historical Depth**: 31 years of data enables long-term trend analysis\n7. **Reproducible**: Docker ensures consistent environment across deployments\n\n---\n\n## Technical Challenges & Mitigations\n\n### Challenge 1: Model Size vs. Available RAM\n- **Issue**: llama3-sec requires significant RAM, local laptop has 32 GB\n- **Solution**: \n  - Development: Use gpt-oss (13 GB) on laptop\n  - Production: Deploy llama3-sec on AWS EC2 (64 GB RAM)\n  - Benefits: Faster iteration locally, best quality in production\n\n### Challenge 2: Embedding Generation at Scale\n- **Issue**: Processing 31 years of complete filings (~51 GB, 1.2M files) requires significant compute power\n- **Solution**: Process in batches (quarterly), parallelize where possible\n\n### Challenge 3: Infrastructure Complexity\n- **Issue**: Managing multiple services (Ollama, RAPTOR API, WebUI, ChromaDB)\n- **Solution**: \n  - Docker Compose orchestrates all services\n  - Single command deployment: `docker-compose up`\n  - Services isolated and independently scalable\n\n### Challenge 4: Clustering Quality\n- **Issue**: Poorly defined clusters reduce summary quality\n- **Solution**: Use BIC for optimal cluster count, validate clusters manually on samples\n\n### Challenge 5: Context Window Limits\n- **Issue**: LLMs have token limits, can't ingest entire knowledge base\n- **Solution**: RAPTOR's hierarchical retrieval provides most relevant chunks + summaries\n\n### Challenge 6: Data Format Evolution\n- **Issue**: SEC filing formats changed significantly between 1993 and 2024 (SGML → HTML → XML)\n- **Solution**: Built robust parsing logic that handles multiple formats, tested across time periods\n\n### Challenge 7: Processing Time\n- **Issue**: Processing 1.2M files takes significant time\n- **Solution**: Start with single quarter (2024 Q1), then scale to full dataset, use Docker for resource management\n\n---\n\n## Next Steps\n1. ✅ Download and test gpt-oss model locally\n2. ✅ Extract complete SEC filing dataset to EC2 (1.2M files)\n3. ✅ Build Docker image for data processing (`edgar-chunking`)\n4. [ ] Run chunking pipeline on 2024 Q1 (test run)\n5. [ ] Validate chunked output and metadata extraction\n6. [ ] Copy RAPTOR class from FinGPT GitHub to `src/models/raptor.py`\n7. [ ] Scale chunking to full dataset (1993-2024)\n8. [ ] Generate embeddings and store in ChromaDB\n\n---\n\n## References\n- **FinGPT GitHub:** https://github.com/AI4Finance-Foundation/FinGPT\n- **FinGPT RAPTOR Implementation:** https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n- **FinGPT Models on Hugging Face:** https://huggingface.co/AI4Finance-Foundation\n- **RAPTOR RAG Documentation:** https://deepwiki.com/AI4Finance-Foundation/FinGPT/5.1-raptor-rag-system\n- **SEC EDGAR API:** https://www.sec.gov/edgar/sec-api-documentation\n- **Ollama:** https://ollama.ai/\n- **Ollama Docker:** https://hub.docker.com/r/ollama/ollama\n- **Ollama Models Library:** https://ollama.com/library\n- **Arcee AI llama3-sec:** https://ollama.com/arcee-ai/llama3-sec\n- **Open WebUI:** https://github.com/open-webui/open-webui\n- **Docker Documentation:** https://docs.docker.com/\n- **AWS EC2 Instance Types:** https://aws.amazon.com/ec2/instance-types/\n- **Anthropic Contextual Embeddings:** https://www.anthropic.com/news/contextual-retrieval"
  },
  {
   "cell_type": "markdown",
   "id": "vozfu971969",
   "source": "## EC2 Production File Structure & Processing Status (Updated 2025-10-24)\n\n### Actual EC2 Data Organization\n\n**Location:** AWS EC2 Instance `secAI` at `/app/data/`\n\n```\n/app/data/\n├── edgar/                          # Raw SEC EDGAR filings\n│   └── extracted/                  # Unzipped filing text files\n│       └── 2024/                   # Year-based organization\n│           ├── QTR1/               # Q1 2024 (6,337 .txt files)   ✅ CHUNKED\n│           ├── QTR2/               # Q2 2024 (7,247 .txt files)   ✅ CHUNKED\n│           ├── QTR3/               # Q3 2024 (6,248 .txt files)   ✅ CHUNKED\n│           └── QTR4/               # Q4 2024 (6,182 .txt files)   ✅ CHUNKED\n│\n├── processed/                      # Chunked JSON output (500-token chunks)\n│   └── 2024/                       # Mirrors input structure\n│       ├── QTR1/                   # 6,337 JSON files (one per filing)\n│       │   └── YYYYMMDD_FORM_edgar_data_CIK_ACCESSION.json\n│       ├── QTR2/                   # 7,247 JSON files\n│       ├── QTR3/                   # 6,248 JSON files\n│       └── QTR4/                   # 6,182 JSON files\n│\n└── embeddings/                     # Vector embeddings (NEXT PHASE)\n    ├── test/                       # Test subset (3 files for validation)\n    │   ├── embeddings.parquet      # Embedding vectors (768-dim)\n    │   ├── metadata.parquet        # Chunk metadata (CIK, date, form, etc.)\n    │   └── index.faiss             # Optional: FAISS index for fast search\n    │\n    └── 2024/                       # Full 2024 embeddings (future)\n        ├── QTR1/\n        │   ├── embeddings.parquet\n        │   └── metadata.parquet\n        ├── QTR2/\n        ├── QTR3/\n        └── QTR4/\n```\n\n### 2024 Processing Results (✅ COMPLETE - All Quarters)\n\n**Chunking completed on 2025-10-24 using Docker container `edgar-chunking`**\n\n| Quarter | Files | Chunks | Tokens | Status |\n|---------|-------|--------|--------|--------|\n| **Q1** | 6,337 | 1,235,886 | 616,372,446 | ✅ Complete |\n| **Q2** | 7,247 | 584,914 | 290,670,736 | ✅ Complete |\n| **Q3** | 6,248 | 522,716 | 259,802,386 | ✅ Complete |\n| **Q4** | 6,182 | 498,682 | 247,809,543 | ✅ Complete |\n| **TOTAL** | **26,014** | **2,842,198** | **1,414,655,111** | ✅ Complete |\n\n**Processing Details:**\n- **Method**: Docker Compose orchestration via `docker-compose.chunking.yml`\n- **Container**: `edgar-chunking` (8.4 GB image with Python 3.12 + tiktoken + tqdm)\n- **Total Processing Time**: ~4 hours (sequential quarterly processing)\n- **Chunk Size**: 500 tokens (tiktoken tokenizer)\n- **Metadata Extracted**: CIK, company name, form type, filing date per chunk\n- **Output Format**: JSON (one file per filing, array of chunks with metadata)\n\n### Chunking Strategy: 500-Token Base Chunks (No Contextual Window)\n\n**Implementation:**\n- **Core Approach**: Direct 500-token chunks using `tiktoken` tokenizer\n- **No Contextual Window**: Initial implementation does NOT use Anthropic's contextual retrieval method\n- **Rationale**: Simpler baseline for testing; can add contextual embeddings later if needed\n\n**Why We Chose This Approach:**\n1. **Faster Initial Processing**: No LLM calls required for context generation\n2. **Baseline for Comparison**: Establishes simple RAG baseline before adding complexity\n3. **Sufficient for High-Dimensional Embeddings**: 768-dim embeddings capture nuance without extra context\n4. **Can Add Later**: If retrieval quality is insufficient, can reprocess with contextual windows\n\n**Alternative Considered (Anthropic Contextual Retrieval):**\n- 500-token core + 100-token LLM-generated contextual summary\n- ~19.9% token overhead\n- Deferred for future iteration if needed\n\n---\n\n## Embedding Strategy: High-Dimensional for Precise Retrieval\n\n### Model Selection: `multi-qa-mpnet-base-dot-v1` (768 dimensions)\n\n**Why High-Dimensional Embeddings for This Project?**\n\n**Use Case Requirements:**\n- **Exact wording retrieval** from SEC filings (not general summarization)\n- **Fine-grained distinctions** between similar financial/legal terms\n- **No overfitting concerns** (using pre-trained models, not training)\n- **Jargon preservation** (e.g., \"material adverse effect\" vs \"material impact\")\n\n**Model Comparison:**\n\n| Model | Dimensions | Use Case | Decision |\n|-------|-----------|----------|----------|\n| **all-MiniLM-L6-v2** | 384 | General semantic similarity | ❌ Too low-dimensional, loses nuance |\n| **all-mpnet-base-v2** | 768 | Best overall quality | ✅ Good option |\n| **multi-qa-mpnet-base-dot-v1** | 768 | Question-answering retrieval | ✅✅ **SELECTED** |\n\n**Why `multi-qa-mpnet-base-dot-v1`?**\n1. **768 dimensions** → Captures fine-grained semantic distinctions\n2. **Trained for Q&A tasks** → Perfect for \"find exact wording about X\" queries\n3. **Dot-product similarity** → Faster search than cosine similarity\n4. **High quality** → 420M parameters, state-of-the-art MPNet architecture\n5. **Exact retrieval** → Preserves legal/financial terminology precision\n\n**Storage Impact:**\n- **2.8M chunks × 768 dims × 4 bytes = ~8.6 GB** (embeddings only)\n- Manageable for EC2 storage\n- Quality improvement worth the 2x storage vs 384-dim\n\n### Test Plan: 3-File Validation\n\n**Test Files (Q4 2024):**\n1. `20241024_10-Q_edgar_data_1318605_0001628280-24-043486.txt`\n2. `20241030_10-Q_edgar_data_789019_0000950170-24-118967.txt`\n3. `20241101_10-K_edgar_data_320193_0000320193-24-000123.txt`\n\n**Test Objectives:**\n1. Validate embedding generation pipeline\n2. Test retrieval quality with high-dimensional embeddings\n3. Verify metadata preservation\n4. Benchmark performance before scaling to full 2024 dataset\n\n**Expected Outputs:**\n- `embeddings.parquet` → 768-dim vectors for all chunks in 3 files\n- `metadata.parquet` → CIK, company, form, date, chunk_id for each chunk\n- Test queries to evaluate retrieval precision\n\n---\n\n## Why High Dimensions Are Better For This Project\n\n**1. Precision Over Generalization**\n- **Low-dim (384)**: Good for general topics, loses subtle distinctions\n- **High-dim (768)**: Distinguishes \"revenue decreased\" vs \"revenue declined slightly\" vs \"revenue fell sharply\"\n\n**2. No Overfitting Risk**\n- Overfitting only matters when **training** models\n- We use **pre-trained** embeddings for inference only\n- Higher dimensions = more information capacity = better retrieval\n\n**3. Financial/Legal Jargon Preservation**\n- SEC filings use highly specific terminology\n- \"Subsequent event\" vs \"subsequent development\" → legally distinct\n- High-dim embeddings preserve these critical distinctions\n\n**4. Storage Trade-off Is Acceptable**\n- 384-dim: ~4.3 GB for 2.8M chunks\n- 768-dim: ~8.6 GB for 2.8M chunks\n- **2x storage for significantly better retrieval quality = worth it**\n\n**5. Supports Complex Queries**\n- \"Find all instances where companies disclosed cybersecurity incidents in Q1 vs Q4\"\n- Requires distinguishing between similar but distinct concepts\n- High-dimensional space enables precise matching\n\n---\n\n## Research-Backed Embedding Selection\n\n**Sentence-BERT Foundation (2019):**\n- Reimers & Gurevych: SBERT is 10,000x faster than BERT for similarity search\n- Source: https://arxiv.org/abs/1908.10084\n- MPNet builds on SBERT architecture\n\n**MPNet: Masked and Permuted Pre-training (2020):**\n- Microsoft Research: MPNet outperforms BERT, RoBERTa, XLNet on GLUE/SQuAD\n- Source: https://arxiv.org/abs/2004.09297\n- Best semantic representation for retrieval tasks\n\n**MTEB Benchmark (2022):**\n- `all-mpnet-base-v2` ranks in **top 10%** across 58 embedding tasks\n- Source: https://arxiv.org/abs/2210.07316\n- Leaderboard: https://huggingface.co/spaces/mteb/leaderboard\n\n**Multi-QA Training:**\n- Fine-tuned on question-answer pairs from Stack Exchange, Yahoo Answers, etc.\n- Optimized for: \"given a question, find the best passage\"\n- Perfect match for RAG retrieval: \"given a query, find exact wording in filings\"\n\n---\n\n## Next Steps (Embedding Phase)\n\n**Immediate (Test on 3 files):**\n1. Create `src/models/embedding_generator.py` script\n2. Load 3 test files from `/app/data/processed/2024/QTR4/`\n3. Generate embeddings using `multi-qa-mpnet-base-dot-v1`\n4. Save to `/app/data/embeddings/test/`\n5. Validate retrieval quality with sample queries\n\n**After Test Success (Scale to Full 2024):**\n1. Process all 26,014 files in 2024\n2. Generate ~2.8M embeddings\n3. Store in `/app/data/embeddings/2024/` (organized by quarter)\n4. Set up ChromaDB or FAISS for similarity search\n5. Benchmark retrieval performance\n\n**Future (Full 1993-2024 Dataset):**\n1. Scale embedding generation to all 31 years\n2. Implement RAPTOR clustering on embeddings\n3. Generate hierarchical summaries\n4. Deploy production RAG system with Open WebUI\n\n---",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}