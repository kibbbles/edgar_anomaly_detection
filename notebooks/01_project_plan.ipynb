{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8241fd",
   "metadata": {},
   "source": "# SEC 10-K/10-Q Analysis - RAPTOR RAG Project Plan\n\n## Project Overview\nAI-powered system for analyzing complete SEC 10-K and 10-Q filings (1993-2024) using RAPTOR RAG (Recursive Adaptive Processing and Topical Organizational Retrieval). The system will create an enhanced knowledge base from financial filings that users can query interactively to identify year-over-year changes, patterns, and potential anomalies.\n\n**Data Coverage:** 1993-2024 (31 years of SEC EDGAR filings, ~51 GB)\n\n---\n\n## Core Architecture\n\n### Infrastructure\n- **Deployment**: AWS EC2 instance with GPU (in progress)\n- **Model Hosting**: Ollama for local LLM deployment\n- **User Interface**: Open WebUI for interactive queries\n- **Data Storage**: Cloud-based storage for processed embeddings and knowledge base\n\n### Architecture Diagram\n\n![System Architecture](diagrams/architecture.png)\n\n### RAPTOR RAG System\nUnlike traditional RAG systems that use simple similarity search, RAPTOR implements:\n- **Hierarchical Clustering**: Multi-level organization (global + local) using UMAP and Gaussian Mixture Models\n- **Recursive Summarization**: 3-level hierarchical summaries capturing both granular details and high-level concepts\n- **Enhanced Context Retrieval**: Cluster-aware retrieval providing richer context for LLM queries\n\n---\n\n## Technical Stack\n\n### NLP & ML\n- **LLM Model**: llama3-sec (Arcee AI, Llama-3-70B based, fine-tuned for SEC filings)\n  - Source: `arcee-ai/llama3-sec` on Ollama\n  - Trained on 72B tokens of SEC filing data (20B checkpoint available)\n  - Deployment: Via Ollama (`ollama pull arcee-ai/llama3-sec`)\n  - Fallback models: gpt-oss (13 GB), qwen2.5:1.5b (986 MB)\n- **RAPTOR Implementation**: Adapted from FinGPT's `FinancialReportAnalysis/utils/rag.py`\n  - Source: https://github.com/AI4Finance-Foundation/FinGPT\n  - Custom implementation in `src/models/raptor.py`\n- **Embeddings**: Sentence Transformers (`all-MiniLM-L6-v2`) for local, cost-free embedding generation\n- **Clustering**: UMAP (dimensionality reduction) + scikit-learn GMM\n- **LLM Interface**: Ollama (primary) or OpenAI API (testing/comparison)\n\n### Data Processing\n- **Chunking**: LangChain `RecursiveCharacterTextSplitter` (~2000 tokens/chunk)\n- **Vector Storage**: ChromaDB for efficient retrieval\n- **Data Format**: JSON/Parquet for structured storage\n\n### Libraries\n- `langchain`, `langchain_community` - LLM orchestration\n- `sentence-transformers` - Local embeddings\n- `umap-learn` - Dimensionality reduction\n- `scikit-learn` - Clustering algorithms (GMM)\n- `pandas`, `numpy` - Data manipulation\n- `requests` - SEC EDGAR API access\n- `ollama` - Python client for Ollama\n\n---\n\n## Data Scope\n\n### Current Data Holdings\n- **Time Period:** 1993-2024 (31 years)\n- **Data Size:** ~51 GB\n- **Data Location:** `data/external/`\n- **Filing Types:** Complete 10-K (annual reports) and 10-Q (quarterly filings)\n- **Processing Scope:** Full filing text (all sections)\n- **Analysis Focus:** Year-over-year changes, topic trends, boilerplate vs. substantive disclosure\n\n**Why Process Complete Filings:**\n- RAG enables users to ask questions about ANY section (not just risk factors)\n- RAPTOR clustering naturally organizes content by topic regardless of section\n- Maximizes system flexibility and future-proofs the knowledge base\n- Supports queries like: \"How did revenue recognition policies change?\" or \"Compare executive compensation across years\"\n\n---\n\n## RAPTOR Pipeline Flowchart\n\n![RAPTOR Pipeline](diagrams/raptor_pipeline.png)\n\n---\n\n## Data Processing Workflow\n\n![Data Processing Workflow](diagrams/data_processing_workflow.png)\n\n---\n\n## Implementation Phases\n\n### Phase 1: Model Research & Setup (Week 1) ✅ COMPLETE\n**Objectives:**\n- [x] Clarify FinGPT components: FinGPT-v3 (LLM model) vs RAPTOR (Python implementation)\n- [x] Set up Ollama and test local LLM deployment\n- [x] Evaluate and download appropriate models for SEC filing analysis\n- [x] Set up project structure (`src/`, `data/`, `notebooks/`, `dashboard/`)\n- [ ] Copy and adapt RAPTOR class from FinGPT's `rag.py` (deferred to Phase 3)\n- [ ] Create base `Raptor` class skeleton in `src/models/raptor.py` (deferred to Phase 3)\n\n**Completed Actions:**\n- ✅ Installed Ollama v0.12.5 on Windows\n- ✅ Downloaded qwen2.5:1.5b (986 MB) - lightweight model for testing\n- ✅ Downloaded gpt-oss (13 GB) - reasoning-capable model\n- ✅ Downloading llama3-sec (50 GB) - SEC-specific model by Arcee AI (in progress)\n- ✅ Verified Python ollama package integration\n- ✅ Tested model inference via command line and Python\n\n**Model Selection for Project:**\nPrimary model: **llama3-sec** (arcee-ai/llama3-sec)\n- Trained on 72B tokens of SEC filing data (currently at 20B checkpoint)\n- Based on Llama-3-70B architecture\n- Specialized for SEC data analysis, investment analysis, risk assessment\n- 50 GB download, 4-bit quantized for ~35-40 GB RAM usage\n- Status: Currently downloading\n\nFallback models:\n- **gpt-oss** (13 GB) - General purpose with reasoning capabilities\n- **qwen2.5:1.5b** (986 MB) - Lightweight for quick testing\n\n**Key Clarification:**\n- ✅ **FinGPT-v3** = Fine-tuned LLM model (downloadable from Hugging Face, runnable in Ollama)\n- ✅ **RAPTOR** = Python implementation for hierarchical clustering/summarization (we copy the code)\n- ✅ **fingpt-rag** = Deprecated project name (not a model), replaced by newer implementations\n- ✅ **llama3-sec** = Domain-specific SEC filing model (best fit for this project)\n\n---\n\n### Phase 2: Data Processing Pipeline (Week 2) - IN PROGRESS (Sample Testing)\n**Objectives:**\n- [ ] Extract filings from downloaded archives (1993-2024, ~51 GB)\n- [ ] Parse complete 10-K/10-Q text from HTML/XML/SGML formats\n- [ ] Implement document chunking (2000 token chunks with tiktoken)\n- [ ] Generate embeddings using local Sentence Transformers\n- [ ] Store structured data (chunks + metadata) in JSON/Parquet\n\n**Key Files:**\n- `src/data/filing_extractor.py` - Unzip archives, extract full filing text\n- `src/data/text_processor.py` - Clean text, chunk into 2000-token segments\n- `src/models/embedding_generator.py` - Embedding creation\n\n**Current Status:**\n- Working with sample filings to validate processing approach\n- Full pipeline implementation deferred until Phase 1 model setup is complete\n\n**Validation:**\n- Test on 3-5 sample filings from different time periods (1995, 2010, 2024)\n- Verify text extraction accuracy across HTML/XML/SGML formats\n- Confirm chunking preserves semantic coherence\n\n---\n\n### Phase 3: RAPTOR System Implementation (Week 3)\n**Objectives:**\n- [ ] Implement hierarchical clustering (adapted from FinGPT's RAPTOR):\n  - Global clustering (UMAP → GMM with BIC for optimal cluster count)\n  - Local clustering (secondary refinement within global clusters)\n- [ ] Build recursive summarization engine (3 levels deep)\n- [ ] Create enhanced knowledge base combining:\n  - Original document chunks\n  - Level 1 summaries (cluster summaries)\n  - Level 2 summaries (summary of summaries)\n  - Level 3 summaries (highest abstraction)\n- [ ] Implement cluster-aware retrieval mechanism\n\n**Key Methods in `Raptor` class (adapted from FinGPT):**\n```python\ndef global_cluster_embeddings(embeddings, dim, n_neighbors, metric=\"cosine\")\ndef local_cluster_embeddings(embeddings, dim, num_neighbors=10)\ndef get_optimal_clusters(embeddings, max_clusters=50)\ndef GMM_cluster(embeddings, threshold, random_state=0)\ndef perform_clustering(embeddings, dim, threshold)\ndef recursive_embed_cluster_summarize(texts, level=1, n_levels=3)\n```\n\n**Source Reference:**\n- Original implementation: https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n\n**Testing:**\n- Validate clustering quality on sample documents\n- Review generated summaries for coherence\n- Ensure topics are properly grouped (e.g., all revenue-related content clusters together)\n\n---\n\n### Phase 4: LLM Integration & Deployment (Week 4)\n**Objectives:**\n- [ ] Set up Ollama on EC2 instance with llama3-sec model\n- [ ] Deploy Open WebUI for user interaction\n- [ ] Integrate RAPTOR knowledge base with LLM query system\n- [ ] Implement query handling for diverse topics across filings\n- [ ] Create sample query templates for common use cases\n\n**Integration Workflow:**\n1. User submits query via Open WebUI\n2. RAPTOR retrieves relevant chunks + hierarchical summaries\n3. Context passed to Ollama LLM (llama3-sec)\n4. LLM generates response with supporting evidence\n5. Results displayed in WebUI\n\n**Example Queries:**\n- \"What cyber risks did Apple disclose in 2023?\"\n- \"How have revenue recognition policies evolved from 2010 to 2024?\"\n- \"Compare executive compensation disclosures between tech companies\"\n- \"Show boilerplate vs. substantive language in risk disclosures\"\n\n**Deliverables:**\n- Functional Open WebUI interface\n- End-to-end query processing pipeline\n- Documentation for common queries\n\n---\n\n## RAPTOR vs. Traditional RAG Comparison\n\n| Feature | Traditional RAG | RAPTOR RAG |\n|---------|----------------|------------|\n| Text Processing | Simple chunking | Recursive, hierarchical |\n| Clustering | None or basic | Multi-level (global + local) |\n| Summarization | None or single-level | Recursive, 3-level |\n| Context Selection | Similarity-based only | Cluster-aware + similarity |\n| Document Understanding | Flat representation | Hierarchical representation |\n| Knowledge Integration | Direct chunks only | Chunks + multi-level summaries |\n\n**Why RAPTOR for Financial Filings?**\n- Financial documents have hierarchical structure (sections, subsections, themes)\n- YoY analysis requires understanding both granular changes and high-level shifts\n- Boilerplate detection benefits from cluster analysis (repetitive language clusters together)\n- Complex queries need multi-level context (e.g., \"How did regulatory disclosures evolve?\")\n- Historical coverage (1993-2024) enables long-term trend analysis\n\n---\n\n## Success Metrics\n- [ ] Successfully process 90%+ of downloaded filings (1993-2024) into knowledge base\n- [ ] Clustering produces coherent, interpretable topic groups\n- [ ] Generated summaries accurately capture content at each hierarchical level\n- [ ] LLM queries return relevant, accurate responses with supporting evidence\n- [ ] System responds to queries in <10 seconds (including retrieval + generation)\n- [ ] Manual validation: Test 10 diverse queries across different topics and decades, verify accuracy\n\n---\n\n## Key Advantages of AI-First Approach\n1. **No Manual Feature Engineering**: LLM infers patterns from enhanced context (vs. building YoY diff algorithms)\n2. **Flexible Queries**: Users can ask arbitrary questions about ANY topic or section\n3. **Semantic Understanding**: Detects substantive changes even when wording differs\n4. **Scalable**: Adding new filings just requires re-running RAPTOR pipeline\n5. **Explainable**: LLM can cite specific sections supporting its conclusions\n6. **Historical Depth**: 31 years of data enables long-term trend analysis\n\n---\n\n## Technical Challenges & Mitigations\n\n### Challenge 1: Embedding Generation at Scale\n- **Issue**: Processing 31 years of complete filings (~51 GB) requires significant compute power\n- **Solution**: Use EC2 GPU instance, batch processing, cache embeddings, process in chronological chunks\n\n### Challenge 2: Model Selection & Deployment\n- **Issue**: Need to clarify what FinGPT components to use (model vs. implementation)\n- **Solution**: \n  - Use llama3-sec (SEC-specific) as primary model via Ollama\n  - Copy RAPTOR implementation from FinGPT's `rag.py`\n  - Maintain flexibility to swap models (gpt-oss, qwen2.5, etc.)\n\n### Challenge 3: Clustering Quality\n- **Issue**: Poorly defined clusters reduce summary quality\n- **Solution**: Use BIC for optimal cluster count, validate clusters manually on samples\n\n### Challenge 4: Context Window Limits\n- **Issue**: LLMs have token limits, can't ingest entire knowledge base\n- **Solution**: RAPTOR's hierarchical retrieval provides most relevant chunks + summaries\n\n### Challenge 5: Data Format Evolution\n- **Issue**: SEC filing formats changed significantly between 1993 and 2024 (SGML → HTML → XML)\n- **Solution**: Build robust parsing logic that handles multiple formats, test across time periods\n\n### Challenge 6: Processing Time\n- **Issue**: Unzipping and processing 51 GB of data could take significant time\n- **Solution**: Parallel processing where possible, start with subset (one year) to validate pipeline\n\n---\n\n## Repository Structure\n```\nedgar_anomaly_detection/\n├── data/\n│   ├── external/         # Downloaded filings 1993-2024 (~51 GB, gitignored)\n│   ├── processed/        # Extracted, chunked filings (gitignored)\n│   └── embeddings/       # Generated embeddings (gitignored)\n├── src/\n│   ├── data/\n│   │   ├── filing_extractor.py    # Extract full filing text\n│   │   └── text_processor.py       # Chunk complete filings\n│   ├── models/\n│   │   ├── raptor.py              # RAPTOR class (adapted from FinGPT)\n│   │   ├── embedding_generator.py\n│   │   └── clustering.py\n│   └── pipeline/\n│       └── knowledge_base_builder.py\n├── notebooks/\n│   ├── 01_project_plan.ipynb      # This file\n│   ├── 02_data_collection.ipynb\n│   └── 03_raptor_testing.ipynb\n├── dashboard/\n│   └── README.md                  # Open WebUI setup instructions\n├── .gitignore\n├── requirements.txt\n└── README.md\n```\n\n---\n\n## Next Steps\n1. ✅ Pull appropriate Ollama model (llama3-sec downloading)\n2. [ ] Test llama3-sec model with sample SEC filing queries\n3. [ ] Copy RAPTOR class from FinGPT GitHub to `src/models/raptor.py`\n4. [ ] Extract sample filings from different time periods to examine format evolution\n5. [ ] Build initial `filing_extractor.py` to handle HTML/XML/SGML parsing\n6. [ ] Coordinate with team on EC2 instance access and GPU availability\n\n---\n\n## References\n- **FinGPT GitHub:** https://github.com/AI4Finance-Foundation/FinGPT\n- **FinGPT RAPTOR Implementation:** https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n- **FinGPT Models on Hugging Face:** https://huggingface.co/AI4Finance-Foundation\n- **RAPTOR RAG Documentation:** https://deepwiki.com/AI4Finance-Foundation/FinGPT/5.1-raptor-rag-system\n- **SEC EDGAR API:** https://www.sec.gov/edgar/sec-api-documentation\n- **Ollama:** https://ollama.ai/\n- **Ollama Models Library:** https://ollama.com/library\n- **Arcee AI llama3-sec:** https://ollama.com/arcee-ai/llama3-sec\n- **Open WebUI:** https://github.com/open-webui/open-webui"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}