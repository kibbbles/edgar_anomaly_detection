{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SEC 10-K Risk Factor Analysis - Project Plan\n\n## Project Overview\nAI-powered system for analyzing SEC 10-K and 10-Q filings using RAPTOR RAG (Recursive Adaptive Processing and Topical Organizational Retrieval). The system will create an enhanced knowledge base from financial filings that users can query interactively to identify year-over-year changes, risk patterns, and potential fraud indicators.\n\n**Data Coverage:** 1993-2024 (31 years of SEC EDGAR filings)\n\n---\n\n## Core Architecture\n\n### Infrastructure\n- **Deployment**: AWS EC2 instance with GPU (in progress)\n- **Model Hosting**: Ollama for local LLM deployment\n- **User Interface**: Open WebUI for interactive queries\n- **Data Storage**: Cloud-based storage for processed embeddings and knowledge base\n\n### Architecture Diagram\n\n![System Architecture](diagrams/architecture.png)\n\n### RAPTOR RAG System\nUnlike traditional RAG systems that use simple similarity search, RAPTOR implements:\n- **Hierarchical Clustering**: Multi-level organization (global + local) using UMAP and Gaussian Mixture Models\n- **Recursive Summarization**: 3-level hierarchical summaries capturing both granular details and high-level themes\n- **Enhanced Context Retrieval**: Cluster-aware retrieval providing richer context for LLM queries\n\n---\n\n## Technical Stack\n\n### NLP & ML\n- **LLM Model**: FinGPT-v3 (Llama2-based, fine-tuned for financial analysis)\n  - Source: `AI4Finance-Foundation/FinGPT-v3` on Hugging Face\n  - Deployment: Via Ollama (`ollama pull hf.co/AI4Finance-Foundation/FinGPT-v3`)\n  - Alternative: Any Ollama-compatible model (Llama3, Mistral, etc.)\n- **RAPTOR Implementation**: Adapted from FinGPT's `FinancialReportAnalysis/utils/rag.py`\n  - Source: https://github.com/AI4Finance-Foundation/FinGPT\n  - Custom implementation in `src/models/raptor.py`\n- **Embeddings**: Sentence Transformers (`all-MiniLM-L6-v2`) for local, cost-free embedding generation\n- **Clustering**: UMAP (dimensionality reduction) + scikit-learn GMM\n- **LLM Interface**: Ollama (primary) or OpenAI API (testing/comparison)\n\n### Data Processing\n- **Chunking**: LangChain `RecursiveCharacterTextSplitter` (~2000 tokens/chunk)\n- **Vector Storage**: ChromaDB for efficient retrieval\n- **Data Format**: JSON/Parquet for structured storage\n\n### Libraries\n- `langchain`, `langchain_community` - LLM orchestration\n- `sentence-transformers` - Local embeddings\n- `umap-learn` - Dimensionality reduction\n- `scikit-learn` - Clustering algorithms (GMM)\n- `pandas`, `numpy` - Data manipulation\n- `requests` - SEC EDGAR API access\n\n---\n\n## Data Scope\n\n### Current Data Holdings\n- **Time Period:** 1993-2024 (31 years)\n- **Data Location:** `data/external/`\n- **Filing Types:** 10-K (annual reports) and 10-Q (quarterly filings)\n- **Target Sections:** \n  - Item 1A (Risk Factors) - primary focus\n  - MD&A (Management Discussion & Analysis)\n  - Other disclosure sections as needed\n- **Analysis Focus:** Year-over-year changes, new/removed risks, boilerplate vs. substantive disclosure\n\n---\n\n## RAPTOR Pipeline Flowchart\n\n![RAPTOR Pipeline](diagrams/raptor_pipeline.png)\n\n---\n\n## Data Processing Workflow\n\n![Data Processing Workflow](diagrams/data_processing_workflow.png)\n\n---\n\n## Implementation Phases\n\n### Phase 1: Model Research & Setup (Week 1)\n**Objectives:**\n- [x] Clarify FinGPT components: FinGPT-v3 (LLM model) vs RAPTOR (Python implementation)\n- [ ] Pull FinGPT-v3 model into Ollama for testing\n- [ ] Copy and adapt RAPTOR class from FinGPT's `rag.py`\n- [ ] Set up project structure (`src/`, `data/`, `notebooks/`, `dashboard/`)\n- [ ] Create base `Raptor` class skeleton in `src/models/raptor.py`\n\n**Deliverables:**\n- FinGPT-v3 running in Ollama\n- RAPTOR class adapted from FinGPT source\n- Project repository structure\n\n**Key Clarification:**\n- ✅ **FinGPT-v3** = Fine-tuned LLM model (downloadable from Hugging Face, runnable in Ollama)\n- ✅ **RAPTOR** = Python implementation for hierarchical clustering/summarization (we copy the code)\n- ✅ **fingpt-rag** = Deprecated project name (not a model), replaced by newer implementations\n\n---\n\n### Phase 2: Data Processing Pipeline (Week 2)\n**Objectives:**\n- [ ] Extract filings from downloaded archives (1993-2024)\n- [ ] Parse 10-K/10-Q HTML/XML to extract Item 1A and other sections\n- [ ] Implement document chunking (2000 token chunks with tiktoken)\n- [ ] Generate embeddings using local Sentence Transformers\n- [ ] Store structured data (chunks + metadata) in JSON/Parquet\n\n**Key Files:**\n- `src/data/filing_extractor.py` - Unzip and parse filings\n- `src/data/text_processor.py` - Chunking and cleaning\n- `src/models/embedding_generator.py` - Embedding creation\n\n**Validation:**\n- Test on 3-5 sample filings before scaling\n- Verify Item 1A extraction accuracy across different time periods\n\n---\n\n### Phase 3: RAPTOR System Implementation (Week 3)\n**Objectives:**\n- [ ] Implement hierarchical clustering (adapted from FinGPT's RAPTOR):\n  - Global clustering (UMAP → GMM with BIC for optimal cluster count)\n  - Local clustering (secondary refinement within global clusters)\n- [ ] Build recursive summarization engine (3 levels deep)\n- [ ] Create enhanced knowledge base combining:\n  - Original document chunks\n  - Level 1 summaries (cluster summaries)\n  - Level 2 summaries (summary of summaries)\n  - Level 3 summaries (highest abstraction)\n- [ ] Implement cluster-aware retrieval mechanism\n\n**Key Methods in `Raptor` class (adapted from FinGPT):**\n```python\ndef global_cluster_embeddings(embeddings, dim, n_neighbors, metric=\"cosine\")\ndef local_cluster_embeddings(embeddings, dim, num_neighbors=10)\ndef get_optimal_clusters(embeddings, max_clusters=50)\ndef GMM_cluster(embeddings, threshold, random_state=0)\ndef perform_clustering(embeddings, dim, threshold)\ndef recursive_embed_cluster_summarize(texts, level=1, n_levels=3)\n```\n\n**Source Reference:**\n- Original implementation: https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n\n**Testing:**\n- Validate clustering quality on sample documents\n- Review generated summaries for coherence\n\n---\n\n### Phase 4: LLM Integration & Deployment (Week 4)\n**Objectives:**\n- [ ] Set up Ollama on EC2 instance with FinGPT-v3 model\n- [ ] Deploy Open WebUI for user interaction\n- [ ] Integrate RAPTOR knowledge base with LLM query system\n- [ ] Implement query handling:\n  - YoY change detection queries\n  - Risk classification questions\n  - Boilerplate vs. substantive disclosure analysis\n- [ ] Create sample query templates for common use cases\n\n**Integration Workflow:**\n1. User submits query via Open WebUI\n2. RAPTOR retrieves relevant chunks + hierarchical summaries\n3. Context passed to Ollama LLM (FinGPT-v3)\n4. LLM generates response with supporting evidence\n5. Results displayed in WebUI\n\n**Deliverables:**\n- Functional Open WebUI interface\n- End-to-end query processing pipeline\n- Documentation for common queries\n\n---\n\n## RAPTOR vs. Traditional RAG Comparison\n\n| Feature | Traditional RAG | RAPTOR RAG |\n|---------|----------------|------------|\n| Text Processing | Simple chunking | Recursive, hierarchical |\n| Clustering | None or basic | Multi-level (global + local) |\n| Summarization | None or single-level | Recursive, 3-level |\n| Context Selection | Similarity-based only | Cluster-aware + similarity |\n| Document Understanding | Flat representation | Hierarchical representation |\n| Knowledge Integration | Direct chunks only | Chunks + multi-level summaries |\n\n**Why RAPTOR for Financial Filings?**\n- Financial documents have hierarchical structure (sections, subsections, themes)\n- YoY analysis requires understanding both granular changes and high-level shifts\n- Boilerplate detection benefits from cluster analysis (repetitive language clusters together)\n- Complex queries need multi-level context (e.g., \"How did cyber risk disclosures evolve?\")\n- Historical coverage (1993-2024) enables long-term trend analysis\n\n---\n\n## Success Metrics\n- [ ] Successfully process 90%+ of downloaded filings (1993-2024) into knowledge base\n- [ ] Clustering produces coherent, interpretable groups\n- [ ] Generated summaries accurately capture document content at each level\n- [ ] LLM queries return relevant, accurate responses with supporting evidence\n- [ ] System responds to queries in <10 seconds (including retrieval + generation)\n- [ ] Manual validation: Test 10 YoY comparison queries across different decades, verify accuracy\n\n---\n\n## Key Advantages of AI-First Approach\n1. **No Manual Feature Engineering**: LLM infers patterns from enhanced context (vs. building YoY diff algorithms)\n2. **Flexible Queries**: Users can ask arbitrary questions beyond predefined analyses\n3. **Semantic Understanding**: Detects substantive changes even when wording differs\n4. **Scalable**: Adding new filings just requires re-running RAPTOR pipeline\n5. **Explainable**: LLM can cite specific sections supporting its conclusions\n6. **Historical Depth**: 31 years of data enables long-term trend analysis\n\n---\n\n## Technical Challenges & Mitigations\n\n### Challenge 1: Embedding Generation at Scale\n- **Issue**: Processing 31 years of large documents requires significant compute power\n- **Solution**: Use EC2 GPU instance, batch processing, cache embeddings, process in chronological chunks\n\n### Challenge 2: Model Selection & Deployment\n- **Issue**: Need to clarify what FinGPT components to use (model vs. implementation)\n- **Solution**: \n  - Use FinGPT-v3 LLM model via Ollama (Hugging Face → Ollama)\n  - Copy RAPTOR implementation from FinGPT's `rag.py`\n  - Maintain flexibility to swap models (Llama3, Mistral, etc.)\n\n### Challenge 3: Clustering Quality\n- **Issue**: Poorly defined clusters reduce summary quality\n- **Solution**: Use BIC for optimal cluster count, validate clusters manually on samples\n\n### Challenge 4: Context Window Limits\n- **Issue**: LLMs have token limits, can't ingest entire knowledge base\n- **Solution**: RAPTOR's hierarchical retrieval provides most relevant chunks + summaries\n\n### Challenge 5: Data Format Evolution\n- **Issue**: SEC filing formats changed significantly between 1993 and 2024\n- **Solution**: Build robust parsing logic that handles HTML, SGML, and modern XML formats\n\n---\n\n## Repository Structure\n```\nedgar_anomaly_detection/\n├── data/\n│   ├── external/         # Downloaded filings 1993-2024 (gitignored)\n│   ├── processed/        # Extracted, chunked filings (gitignored)\n│   └── embeddings/       # Generated embeddings (gitignored)\n├── src/\n│   ├── data/\n│   │   ├── filing_extractor.py\n│   │   └── text_processor.py\n│   ├── models/\n│   │   ├── raptor.py           # RAPTOR class (adapted from FinGPT)\n│   │   ├── embedding_generator.py\n│   │   └── clustering.py\n│   └── pipeline/\n│       └── knowledge_base_builder.py\n├── notebooks/\n│   ├── 01_project_plan.ipynb   # This file\n│   ├── 02_data_collection.ipynb\n│   └── 03_raptor_testing.ipynb\n├── dashboard/\n│   └── README.md               # Open WebUI setup instructions\n├── .gitignore\n├── requirements.txt\n└── README.md\n```\n\n---\n\n## Next Steps\n1. Pull FinGPT-v3 model into Ollama: `ollama pull hf.co/AI4Finance-Foundation/FinGPT-v3`\n2. Copy RAPTOR class from FinGPT GitHub to `src/models/raptor.py`\n3. Test FinGPT-v3 on sample financial text queries\n4. Test embedding generation on 1-2 sample filings from different time periods (e.g., 1995, 2010, 2024)\n5. Coordinate with team on EC2 instance access and GPU availability\n\n---\n\n## References\n- **FinGPT GitHub:** https://github.com/AI4Finance-Foundation/FinGPT\n- **FinGPT RAPTOR Implementation:** https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n- **FinGPT Models on Hugging Face:** https://huggingface.co/AI4Finance-Foundation\n- **RAPTOR RAG Documentation:** https://deepwiki.com/AI4Finance-Foundation/FinGPT/5.1-raptor-rag-system\n- **SEC EDGAR API:** https://www.sec.gov/edgar/sec-api-documentation\n- **Ollama:** https://ollama.ai/\n- **Open WebUI:** https://github.com/open-webui/open-webui"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}