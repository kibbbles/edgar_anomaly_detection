{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SEC 10-K Risk Factor Analysis - Project Plan\n\n## Project Overview\nAI-powered system for analyzing SEC 10-K and 10-Q filings using RAPTOR RAG (Recursive Adaptive Processing and Topical Organizational Retrieval). The system will create an enhanced knowledge base from financial filings that users can query interactively to identify year-over-year changes, risk patterns, and potential fraud indicators.\n\n---\n\n## Core Architecture\n\n### Infrastructure\n- **Deployment**: AWS EC2 instance with GPU (in progress)\n- **Model Hosting**: Ollama for local LLM deployment\n- **User Interface**: Open WebUI for interactive queries\n- **Data Storage**: Cloud-based storage for processed embeddings and knowledge base\n\n### Architecture Diagram\n\n```mermaid\ngraph TB\n    subgraph \"Data Sources\"\n        A[SEC EDGAR API]\n        B[Downloaded ZIP Files<br/>10-K/10-Q Filings]\n    end\n\n    subgraph \"AWS EC2 Instance with GPU\"\n        C[Filing Extractor]\n        D[Text Processor<br/>Chunking & Cleaning]\n        E[RAPTOR System]\n        F[Embedding Generator<br/>Sentence Transformers]\n        G[Knowledge Base<br/>ChromaDB]\n        H[Ollama LLM<br/>FinGPT Model]\n        I[Open WebUI]\n    end\n\n    subgraph \"Storage\"\n        J[(Processed Data<br/>JSON/Parquet)]\n        K[(Vector Embeddings)]\n    end\n\n    subgraph \"Users\"\n        L[End Users]\n    end\n\n    A -->|Download Filings| B\n    B -->|Extract| C\n    C -->|Parse HTML/XML| D\n    D -->|Chunks + Metadata| F\n    F -->|Generate Embeddings| E\n    E -->|Hierarchical Clustering<br/>& Summarization| G\n    G -->|Store| K\n    D -->|Store| J\n\n    L -->|Submit Query| I\n    I -->|Retrieve Context| G\n    G -->|Relevant Chunks<br/>+ Summaries| H\n    H -->|Generate Response| I\n    I -->|Display Results| L\n\n    style E fill:#ff9800,stroke:#f57c00,stroke-width:3px\n    style H fill:#4caf50,stroke:#388e3c,stroke-width:2px\n    style I fill:#2196f3,stroke:#1976d2,stroke-width:2px\n```\n\n### RAPTOR RAG System\nUnlike traditional RAG systems that use simple similarity search, RAPTOR implements:\n- **Hierarchical Clustering**: Multi-level organization (global + local) using UMAP and Gaussian Mixture Models\n- **Recursive Summarization**: 3-level hierarchical summaries capturing both granular details and high-level themes\n- **Enhanced Context Retrieval**: Cluster-aware retrieval providing richer context for LLM queries\n\n---\n\n## Technical Stack\n\n### NLP & ML\n- **Base Model**: FinGPT (Hugging Face compatible version) or alternative financial LLM\n- **Embeddings**: Sentence Transformers (`all-MiniLM-L6-v2`) for local, cost-free embedding generation\n- **Clustering**: UMAP (dimensionality reduction) + scikit-learn GMM\n- **LLM Interface**: Ollama (local) or OpenAI API (for testing/comparison)\n\n### Data Processing\n- **Chunking**: LangChain `RecursiveCharacterTextSplitter` (~2000 tokens/chunk)\n- **Vector Storage**: ChromaDB or similar for efficient retrieval\n- **Data Format**: JSON/Parquet for structured storage\n\n### Libraries\n- `langchain`, `langchain_community` - LLM orchestration\n- `sentence-transformers` - Local embeddings\n- `umap-learn` - Dimensionality reduction\n- `scikit-learn` - Clustering algorithms\n- `pandas`, `numpy` - Data manipulation\n- `requests` - SEC EDGAR API access\n\n---\n\n## Data Scope\n- **Current Holdings**: ZIP folders containing 10-K and 10-Q filings (already downloaded)\n- **Target Sections**: \n  - Item 1A (Risk Factors) - primary focus\n  - Other sections as needed for comprehensive analysis\n- **Analysis Focus**: Year-over-year changes, new/removed risks, boilerplate vs. substantive disclosure\n\n---\n\n## RAPTOR Pipeline Flowchart\n\n```mermaid\nflowchart TD\n    Start([Start: Raw 10-K/10-Q Filing]) --> Extract[Extract Text from HTML/XML]\n    Extract --> Chunk[Chunk Document<br/>RecursiveCharacterTextSplitter<br/>2000 tokens/chunk]\n    Chunk --> Embed1[Generate Embeddings<br/>Sentence Transformers]\n    \n    Embed1 --> GlobalCluster[Global Clustering<br/>UMAP + GMM]\n    GlobalCluster --> LocalCluster[Local Clustering<br/>Refine within each cluster]\n    \n    LocalCluster --> Summarize1[Level 1 Summarization<br/>Cluster summaries]\n    Summarize1 --> Embed2[Embed Level 1 Summaries]\n    \n    Embed2 --> Cluster2[Level 2 Clustering<br/>UMAP + GMM]\n    Cluster2 --> Summarize2[Level 2 Summarization<br/>Summary of summaries]\n    \n    Summarize2 --> Embed3[Embed Level 2 Summaries]\n    Embed3 --> Cluster3[Level 3 Clustering<br/>UMAP + GMM]\n    Cluster3 --> Summarize3[Level 3 Summarization<br/>Highest abstraction]\n    \n    Summarize3 --> Combine[Combine All Levels<br/>Original chunks + L1/L2/L3 summaries]\n    Combine --> Store[Store in Knowledge Base<br/>ChromaDB]\n    Store --> End([Knowledge Base Ready])\n    \n    style GlobalCluster fill:#ffeb3b,stroke:#fbc02d,stroke-width:2px\n    style LocalCluster fill:#ffeb3b,stroke:#fbc02d,stroke-width:2px\n    style Summarize1 fill:#9c27b0,stroke:#7b1fa2,stroke-width:2px\n    style Summarize2 fill:#9c27b0,stroke:#7b1fa2,stroke-width:2px\n    style Summarize3 fill:#9c27b0,stroke:#7b1fa2,stroke-width:2px\n    style Store fill:#4caf50,stroke:#388e3c,stroke-width:3px\n```\n\n---\n\n## Data Processing Workflow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant FileSystem as File System<br/>(ZIP Archives)\n    participant Extractor as Filing Extractor\n    participant Parser as Text Processor\n    participant Embedder as Embedding Generator\n    participant RAPTOR as RAPTOR Engine\n    participant KB as Knowledge Base\n    participant Storage as Cloud Storage\n\n    User->>FileSystem: Access ZIP files\n    FileSystem->>Extractor: Read 10-K/10-Q archives\n    Extractor->>Extractor: Unzip and extract filings\n    Extractor->>Parser: Send raw HTML/XML\n    \n    Parser->>Parser: Parse Item 1A (Risk Factors)\n    Parser->>Parser: Clean and normalize text\n    Parser->>Parser: Chunk into 2000 token segments\n    Parser->>Storage: Save chunks as JSON/Parquet\n    \n    Parser->>Embedder: Send text chunks\n    Embedder->>Embedder: Generate embeddings<br/>(Sentence Transformers)\n    \n    Embedder->>RAPTOR: Send chunks + embeddings\n    \n    RAPTOR->>RAPTOR: Global clustering (UMAP + GMM)\n    RAPTOR->>RAPTOR: Local clustering refinement\n    RAPTOR->>RAPTOR: Level 1 summarization\n    RAPTOR->>RAPTOR: Recursive clustering (L2)\n    RAPTOR->>RAPTOR: Level 2 summarization\n    RAPTOR->>RAPTOR: Recursive clustering (L3)\n    RAPTOR->>RAPTOR: Level 3 summarization\n    \n    RAPTOR->>KB: Store enhanced knowledge base<br/>(chunks + summaries)\n    KB->>Storage: Persist vector embeddings\n    \n    Storage-->>User: Processing complete\n    \n    Note over RAPTOR,KB: Knowledge base now contains:<br/>- Original chunks<br/>- L1/L2/L3 summaries<br/>- Hierarchical structure\n```\n\n---\n\n## Implementation Phases\n\n### Phase 1: Model Research & Setup (Week 1)\n**Objectives:**\n- [ ] Research FinGPT models on Hugging Face (avoid outdated fingpt-rag from 2 years ago)\n- [ ] Select model compatible with Ollama deployment\n- [ ] Set up project structure (`src/`, `data/`, `notebooks/`, `dashboard/`)\n- [ ] Initialize Git repository with proper `.gitignore`\n- [ ] Create base `Raptor` class skeleton\n\n**Deliverables:**\n- Model selection document\n- Project repository structure\n- `Raptor` class foundation\n\n---\n\n### Phase 2: Data Processing Pipeline (Week 2)\n**Objectives:**\n- [ ] Extract filings from ZIP archives\n- [ ] Parse 10-K/10-Q HTML/XML to extract Item 1A and other sections\n- [ ] Implement document chunking (2000 token chunks with tiktoken)\n- [ ] Generate embeddings using local Sentence Transformers\n- [ ] Store structured data (chunks + metadata) in JSON/Parquet\n\n**Key Files:**\n- `src/data/filing_extractor.py` - Unzip and parse filings\n- `src/data/text_processor.py` - Chunking and cleaning\n- `src/models/embedding_generator.py` - Embedding creation\n\n**Validation:**\n- Test on 3-5 sample filings before scaling\n- Verify Item 1A extraction accuracy\n\n---\n\n### Phase 3: RAPTOR System Implementation (Week 3)\n**Objectives:**\n- [ ] Implement hierarchical clustering:\n  - Global clustering (UMAP → GMM with BIC for optimal cluster count)\n  - Local clustering (secondary refinement within global clusters)\n- [ ] Build recursive summarization engine (3 levels deep)\n- [ ] Create enhanced knowledge base combining:\n  - Original document chunks\n  - Level 1 summaries (cluster summaries)\n  - Level 2 summaries (summary of summaries)\n  - Level 3 summaries (highest abstraction)\n- [ ] Implement cluster-aware retrieval mechanism\n\n**Key Methods in `Raptor` class:**\n```python\ndef global_cluster_embeddings(embeddings, dim, n_neighbors, metric=\"cosine\")\ndef local_cluster_embeddings(embeddings, dim, num_neighbors=10)\ndef get_optimal_clusters(embeddings, max_clusters=50)\ndef GMM_cluster(embeddings, threshold, random_state=0)\ndef perform_clustering(embeddings, dim, threshold)\ndef recursive_embed_cluster_summarize(texts, level=1, n_levels=3)\n```\n\n**Testing:**\n- Validate clustering quality on sample documents\n- Review generated summaries for coherence\n\n---\n\n### Phase 4: LLM Integration & Deployment (Week 4)\n**Objectives:**\n- [ ] Set up Ollama on EC2 instance with selected FinGPT model\n- [ ] Deploy Open WebUI for user interaction\n- [ ] Integrate RAPTOR knowledge base with LLM query system\n- [ ] Implement query handling:\n  - YoY change detection queries\n  - Risk classification questions\n  - Boilerplate vs. substantive disclosure analysis\n- [ ] Create sample query templates for common use cases\n\n**Integration Workflow:**\n1. User submits query via Open WebUI\n2. RAPTOR retrieves relevant chunks + hierarchical summaries\n3. Context passed to Ollama LLM\n4. LLM generates response with supporting evidence\n5. Results displayed in WebUI\n\n**Deliverables:**\n- Functional Open WebUI interface\n- End-to-end query processing pipeline\n- Documentation for common queries\n\n---\n\n## RAPTOR vs. Traditional RAG Comparison\n\n| Feature | Traditional RAG | RAPTOR RAG |\n|---------|----------------|------------|\n| Text Processing | Simple chunking | Recursive, hierarchical |\n| Clustering | None or basic | Multi-level (global + local) |\n| Summarization | None or single-level | Recursive, 3-level |\n| Context Selection | Similarity-based only | Cluster-aware + similarity |\n| Document Understanding | Flat representation | Hierarchical representation |\n| Knowledge Integration | Direct chunks only | Chunks + multi-level summaries |\n\n**Why RAPTOR for Financial Filings?**\n- Financial documents have hierarchical structure (sections, subsections, themes)\n- YoY analysis requires understanding both granular changes and high-level shifts\n- Boilerplate detection benefits from cluster analysis (repetitive language clusters together)\n- Complex queries need multi-level context (e.g., \"How did cyber risk disclosures evolve?\")\n\n---\n\n## Success Metrics\n- [ ] Successfully process 90%+ of downloaded filings into knowledge base\n- [ ] Clustering produces coherent, interpretable groups\n- [ ] Generated summaries accurately capture document content at each level\n- [ ] LLM queries return relevant, accurate responses with supporting evidence\n- [ ] System responds to queries in <10 seconds (including retrieval + generation)\n- [ ] Manual validation: Test 10 YoY comparison queries, verify accuracy\n\n---\n\n## Key Advantages of AI-First Approach\n1. **No Manual Feature Engineering**: LLM infers patterns from enhanced context (vs. building YoY diff algorithms)\n2. **Flexible Queries**: Users can ask arbitrary questions beyond predefined analyses\n3. **Semantic Understanding**: Detects substantive changes even when wording differs\n4. **Scalable**: Adding new filings just requires re-running RAPTOR pipeline\n5. **Explainable**: LLM can cite specific sections supporting its conclusions\n\n---\n\n## Technical Challenges & Mitigations\n\n### Challenge 1: Embedding Generation at Scale\n- **Issue**: Processing hundreds of large documents requires compute power\n- **Solution**: Use EC2 GPU instance, batch processing, cache embeddings\n\n### Challenge 2: Model Selection\n- **Issue**: fingpt-rag outdated (2 years old), not on Hugging Face\n- **Solution**: Research alternative FinGPT models on Hugging Face with recent updates\n\n### Challenge 3: Clustering Quality\n- **Issue**: Poorly defined clusters reduce summary quality\n- **Solution**: Use BIC for optimal cluster count, validate clusters manually on samples\n\n### Challenge 4: Context Window Limits\n- **Issue**: LLMs have token limits, can't ingest entire knowledge base\n- **Solution**: RAPTOR's hierarchical retrieval provides most relevant chunks + summaries\n\n---\n\n## Repository Structure\n```\nedgar_anomaly_detection/\n├── data/\n│   ├── raw/              # ZIP files of 10-K/10-Q (gitignored)\n│   ├── processed/        # Extracted, chunked filings (gitignored)\n│   └── embeddings/       # Generated embeddings (gitignored)\n├── src/\n│   ├── data/\n│   │   ├── filing_extractor.py\n│   │   └── text_processor.py\n│   ├── models/\n│   │   ├── raptor.py           # Main RAPTOR class\n│   │   ├── embedding_generator.py\n│   │   └── clustering.py\n│   └── pipeline/\n│       └── knowledge_base_builder.py\n├── notebooks/\n│   ├── 01_project_plan.ipynb   # This file\n│   ├── 02_data_exploration.ipynb\n│   └── 03_raptor_testing.ipynb\n├── dashboard/\n│   └── README.md               # Open WebUI setup instructions\n├── .gitignore\n├── requirements.txt\n└── README.md\n```\n\n---\n\n## Next Steps\n1. Begin Phase 1: Research FinGPT models on Hugging Face\n2. Create `src/models/raptor.py` skeleton\n3. Test embedding generation on 1-2 sample filings\n4. Coordinate with team on EC2 instance access and GPU availability\n\n---\n\n## References\n- FinGPT Documentation: https://deepwiki.com/AI4Finance-Foundation/FinGPT/\n- RAPTOR RAG System: https://deepwiki.com/AI4Finance-Foundation/FinGPT/5.1-raptor-rag-system\n- SEC EDGAR API: https://www.sec.gov/edgar/sec-api-documentation\n- Ollama: https://ollama.ai/\n- Open WebUI: https://github.com/open-webui/open-webui"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}