{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8241fd",
   "metadata": {},
   "source": "# SEC 10-K/10-Q Analysis - RAPTOR RAG Project Plan\n\n## Project Overview\nAI-powered system for analyzing complete SEC 10-K and 10-Q filings (1993-2024) using RAPTOR RAG (Recursive Adaptive Processing and Topical Organizational Retrieval). The system will create an enhanced knowledge base from financial filings that users can query interactively to identify year-over-year changes, patterns, and potential anomalies.\n\n**Data Coverage:** 1993-2024 (31 years of SEC EDGAR filings, ~51 GB)\n\n---\n\n## Core Architecture\n\n### Infrastructure\n- **Development Environment**: Local Windows laptop with Ollama + Docker Desktop\n- **Production Deployment**: AWS EC2 instance (r6i.4xlarge: 128 GB RAM, 500 GB EBS storage)\n- **Containerization**: Docker Compose for service orchestration\n- **Model Hosting**: Ollama (containerized)\n- **User Interface**: Open WebUI (containerized)\n- **Vector Database**: ChromaDB (file-based, stored on EC2 EBS volume)\n- **Data Storage**: EC2 EBS volume for processed embeddings and knowledge base\n\n### Deployment Architecture\n\n**Development (Phase 2):**\n```\nLocal Laptop (32 GB RAM)\n├── Docker Compose\n│   ├── Ollama Container (gpt-oss 13 GB)\n│   ├── RAPTOR API Container (Python app)\n│   └── Open WebUI Container\n└── Local Volumes\n    ├── ./data/external (sample data)\n    ├── ./data/embeddings (ChromaDB)\n    └── ./models (Ollama models)\n```\n\n**Production (Phase 4):**\n```\nAWS EC2 (r6i.4xlarge - 128 GB RAM)\n├── Docker Compose\n│   ├── Ollama Container (llama3-sec 49 GB)\n│   ├── RAPTOR API Container (Python app)\n│   └── Open WebUI Container\n└── EBS Volume (500 GB)\n    ├── /data/external (51 GB SEC filings)\n    ├── /data/processed (chunked filings)\n    ├── /data/embeddings (ChromaDB vector DB)\n    └── /models (Ollama models)\n```\n\n### Architecture Diagram\n\n![System Architecture](diagrams/architecture.png)\n\n### RAPTOR RAG System\nUnlike traditional RAG systems that use simple similarity search, RAPTOR implements:\n- **Hierarchical Clustering**: Multi-level organization (global + local) using UMAP and Gaussian Mixture Models\n- **Recursive Summarization**: 3-level hierarchical summaries capturing both granular details and high-level concepts\n- **Enhanced Context Retrieval**: Cluster-aware retrieval providing richer context for LLM queries\n\n---\n\n## Technical Stack\n\n### NLP & ML\n\n**Development Model (Phase 2):**\n- **gpt-oss** (13 GB) - General purpose with reasoning capabilities\n- Suitable for: Local testing, rapid iteration, pipeline validation\n- RAM requirement: ~16-20 GB\n\n**Production Model (Phase 4):**\n- **llama3-sec** (Arcee AI, Llama-3-70B based, fine-tuned for SEC filings)\n  - Source: `arcee-ai/llama3-sec` on Ollama\n  - Trained on 72B tokens of SEC filing data (20B checkpoint available)\n  - Deployment: Via Ollama in Docker container\n  - Size: 49 GB download, 4-bit quantized\n  - RAM requirement: ~35-40 GB (requires EC2 with 128 GB)\n  - Specialized for: SEC data analysis, investment analysis, risk assessment\n\n**Fallback Models:**\n- qwen2.5:1.5b (986 MB) - Lightweight for quick testing\n\n**RAPTOR Implementation:**\n- Adapted from FinGPT's `FinancialReportAnalysis/utils/rag.py`\n- Source: https://github.com/AI4Finance-Foundation/FinGPT\n- Custom implementation in `src/models/raptor.py`\n\n**Embeddings & Clustering:**\n- Sentence Transformers (`all-MiniLM-L6-v2`) for local, cost-free embedding generation\n- UMAP (dimensionality reduction) + scikit-learn GMM for clustering\n- LLM Interface: Ollama Python client\n\n### Infrastructure & Deployment\n\n**Containerization:**\n- Docker + Docker Compose for all services\n- Reproducible environments (dev → production)\n- Service isolation and orchestration\n\n**AWS EC2 Configuration (Production):**\n- **Instance Type**: r6i.4xlarge (memory-optimized)\n  - 16 vCPUs\n  - 128 GB RAM (sufficient for llama3-sec 70B model)\n  - ~$0.80-1.00/hour (~$600-750/month if running 24/7)\n- **Storage**: 500 GB EBS gp3 volume\n  - 51 GB raw data + processed embeddings + models\n- **OS**: Ubuntu 22.04 LTS\n- **Security**: VPC with restricted security groups, SSH key access\n\n**Data Processing:**\n- Chunking: LangChain `RecursiveCharacterTextSplitter` (2000-4000 tokens/chunk)\n- Vector Storage: ChromaDB (file-based, stored on EBS volume)\n- Data Format: JSON/Parquet for structured storage\n\n### Libraries\n- `langchain`, `langchain_community` - LLM orchestration\n- `sentence-transformers` - Local embeddings\n- `umap-learn` - Dimensionality reduction\n- `scikit-learn` - Clustering algorithms (GMM)\n- `pandas`, `numpy` - Data manipulation\n- `requests` - SEC EDGAR API access\n- `ollama` - Python client for Ollama\n- `docker`, `docker-compose` - Containerization\n\n---\n\n## Data Scope\n\n### Current Data Holdings\n- **Time Period:** 1993-2024 (31 years)\n- **Data Size:** ~51 GB\n- **Data Location:** `data/external/`\n- **Filing Types:** Complete 10-K (annual reports) and 10-Q (quarterly filings)\n- **Processing Scope:** Full filing text (all sections)\n- **Analysis Focus:** Year-over-year changes, topic trends, boilerplate vs. substantive disclosure\n\n**Why Process Complete Filings:**\n- RAG enables users to ask questions about ANY section (not just risk factors)\n- RAPTOR clustering naturally organizes content by topic regardless of section\n- Maximizes system flexibility and future-proofs the knowledge base\n- Supports queries like: \"How did revenue recognition policies change?\" or \"Compare executive compensation across years\"\n\n---\n\n## RAPTOR Pipeline Flowchart\n\n![RAPTOR Pipeline](diagrams/raptor_pipeline.png)\n\n---\n\n## Data Processing Workflow\n\n![Data Processing Workflow](diagrams/data_processing_workflow.png)\n\n---\n\n## Implementation Phases\n\n### Phase 1: Model Research & Setup (Week 1) ✅ COMPLETE\n**Objectives:**\n- [x] Clarify FinGPT components: FinGPT-v3 (LLM model) vs RAPTOR (Python implementation)\n- [x] Set up Ollama and test local LLM deployment\n- [x] Evaluate and download appropriate models for SEC filing analysis\n- [x] Set up project structure (`src/`, `data/`, `notebooks/`, `dashboard/`)\n- [ ] Copy and adapt RAPTOR class from FinGPT's `rag.py` (deferred to Phase 3)\n- [ ] Create base `Raptor` class skeleton in `src/models/raptor.py` (deferred to Phase 3)\n\n**Completed Actions:**\n- ✅ Installed Ollama v0.12.5 on Windows\n- ✅ Downloaded qwen2.5:1.5b (986 MB) - lightweight model for testing\n- ✅ Downloaded gpt-oss (13 GB) - reasoning-capable model for development\n- ✅ Verified Python ollama package integration\n- ✅ Tested model inference via command line and Python\n\n**Model Selection Strategy:**\n- **Development (Phase 2):** gpt-oss (13 GB) on local laptop\n- **Production (Phase 4):** llama3-sec (49 GB) on AWS EC2\n- **Rationale:** Start with smaller model for faster iteration, scale to specialized model in production\n\n**Key Clarification:**\n- ✅ **FinGPT-v3** = Fine-tuned LLM model (downloadable from Hugging Face, runnable in Ollama)\n- ✅ **RAPTOR** = Python implementation for hierarchical clustering/summarization (we copy the code)\n- ✅ **fingpt-rag** = Deprecated project name (not a model), replaced by newer implementations\n- ✅ **llama3-sec** = Domain-specific SEC filing model (best fit for production)\n- ✅ **gpt-oss** = General purpose reasoning model (best fit for development)\n\n---\n\n### Phase 2: Data Processing Pipeline (Week 2) - IN PROGRESS (Sample Testing)\n**Objectives:**\n- [ ] Extract filings from sample archives\n- [ ] Parse complete 10-K/10-Q text from HTML/XML/SGML formats\n- [ ] Implement document chunking (test 2000-4000 token ranges)\n- [ ] Generate embeddings using local Sentence Transformers\n- [ ] Store structured data (chunks + metadata) in JSON/Parquet\n- [ ] Set up Docker Compose for local development environment\n\n**Key Files:**\n- `src/data/filing_extractor.py` - Unzip archives, extract full filing text\n- `src/data/text_processor.py` - Clean text, chunk into configurable token segments\n- `src/models/embedding_generator.py` - Embedding creation\n- `docker-compose.dev.yml` - Development environment setup\n\n**Development Setup (Docker Compose):**\n```yaml\nservices:\n  ollama:\n    image: ollama/ollama\n    volumes:\n      - ./models:/root/.ollama\n    ports:\n      - \"11434:11434\"\n  \n  raptor-api:\n    build: ./src\n    volumes:\n      - ./data:/app/data\n      - ./src:/app/src\n    environment:\n      - OLLAMA_HOST=ollama:11434\n      - MODEL_NAME=gpt-oss\n    depends_on:\n      - ollama\n  \n  webui:\n    image: ghcr.io/open-webui/open-webui\n    ports:\n      - \"3000:8080\"\n    environment:\n      - OLLAMA_BASE_URL=http://ollama:11434\n    depends_on:\n      - ollama\n```\n\n**Current Status:**\n- Working with sample filings to validate processing approach\n- Testing chunk sizes: 200, 500, 1000, 2000, 3000, 4000 tokens\n- Using gpt-oss (13 GB) for development\n- Full pipeline implementation with complete dataset deferred to Phase 4\n\n**Validation:**\n- Test on 3-5 sample filings from different time periods (1995, 2010, 2024)\n- Verify text extraction accuracy across HTML/XML/SGML formats\n- Confirm chunking preserves semantic coherence\n- Compare chunk quality across different token sizes\n\n---\n\n### Phase 3: RAPTOR System Implementation (Week 3)\n**Objectives:**\n- [ ] Copy RAPTOR class from FinGPT to `src/models/raptor.py`\n- [ ] Implement hierarchical clustering (adapted from FinGPT's RAPTOR):\n  - Global clustering (UMAP → GMM with BIC for optimal cluster count)\n  - Local clustering (secondary refinement within global clusters)\n- [ ] Build recursive summarization engine (3 levels deep)\n- [ ] Create enhanced knowledge base combining:\n  - Original document chunks\n  - Level 1 summaries (cluster summaries)\n  - Level 2 summaries (summary of summaries)\n  - Level 3 summaries (highest abstraction)\n- [ ] Implement cluster-aware retrieval mechanism\n- [ ] Test on sample data with gpt-oss\n\n**Key Methods in `Raptor` class (adapted from FinGPT):**\n```python\ndef global_cluster_embeddings(embeddings, dim, n_neighbors, metric=\"cosine\")\ndef local_cluster_embeddings(embeddings, dim, num_neighbors=10)\ndef get_optimal_clusters(embeddings, max_clusters=50)\ndef GMM_cluster(embeddings, threshold, random_state=0)\ndef perform_clustering(embeddings, dim, threshold)\ndef recursive_embed_cluster_summarize(texts, level=1, n_levels=3)\n```\n\n**Source Reference:**\n- Original implementation: https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n\n**Testing:**\n- Validate clustering quality on sample documents\n- Review generated summaries for coherence\n- Ensure topics are properly grouped (e.g., all revenue-related content clusters together)\n- Benchmark with gpt-oss before scaling to production\n\n---\n\n### Phase 4: Production Deployment on AWS EC2 (Week 4-5)\n**Objectives:**\n- [ ] Provision AWS EC2 instance (r6i.4xlarge, 128 GB RAM, 500 GB storage)\n- [ ] Set up Docker + Docker Compose on EC2\n- [ ] Deploy Ollama container with llama3-sec model (49 GB)\n- [ ] Deploy RAPTOR API container with full pipeline\n- [ ] Deploy Open WebUI container for user interaction\n- [ ] Process complete 51 GB dataset into knowledge base\n- [ ] Implement query handling for diverse topics across filings\n- [ ] Create sample query templates for common use cases\n- [ ] Set up monitoring and logging\n\n**EC2 Setup Steps:**\n1. Launch r6i.4xlarge instance (Ubuntu 22.04)\n2. Attach 500 GB EBS gp3 volume\n3. Install Docker + Docker Compose\n4. Clone repository to EC2\n5. Pull llama3-sec model via Ollama\n6. Start services with `docker-compose -f docker-compose.prod.yml up -d`\n7. Process full 51 GB dataset\n8. Configure security groups (restrict access)\n\n**Production Docker Compose:**\n```yaml\nservices:\n  ollama:\n    image: ollama/ollama\n    volumes:\n      - /data/models:/root/.ollama\n    deploy:\n      resources:\n        limits:\n          memory: 60G\n  \n  raptor-api:\n    build: ./src\n    volumes:\n      - /data:/app/data\n    environment:\n      - OLLAMA_HOST=ollama:11434\n      - MODEL_NAME=llama3-sec\n      - CHUNK_SIZE=2000\n    depends_on:\n      - ollama\n  \n  webui:\n    image: ghcr.io/open-webui/open-webui\n    ports:\n      - \"443:8080\"\n    environment:\n      - OLLAMA_BASE_URL=http://ollama:11434\n    depends_on:\n      - ollama\n```\n\n**Integration Workflow:**\n1. User submits query via Open WebUI (web interface)\n2. RAPTOR retrieves relevant chunks + hierarchical summaries from ChromaDB\n3. Context passed to Ollama LLM (llama3-sec) via API\n4. LLM generates response with supporting evidence\n5. Results displayed in WebUI with citations\n\n**Example Queries:**\n- \"What cyber risks did Apple disclose in 2023?\"\n- \"How have revenue recognition policies evolved from 2010 to 2024?\"\n- \"Compare executive compensation disclosures between tech companies\"\n- \"Show boilerplate vs. substantive language in risk disclosures\"\n\n**Cost Estimation:**\n- EC2 r6i.4xlarge: ~$0.80-1.00/hour\n- EBS gp3 500GB: ~$40/month\n- Data transfer: Minimal (queries only)\n- **Total**: ~$600-800/month for 24/7 operation\n- **Optimization**: Stop instance when not in use, use spot instances for batch processing\n\n**Deliverables:**\n- Functional production system on EC2\n- Dockerized, reproducible deployment\n- Full 51 GB dataset processed into knowledge base\n- Open WebUI interface accessible via HTTPS\n- Documentation for deployment and maintenance\n\n---\n\n## Docker Strategy\n\n### Development vs. Production\n\n**Development (Local Laptop):**\n- Purpose: Build and test pipeline with sample data\n- Model: gpt-oss (13 GB) - fits in 32 GB RAM\n- Data: Sample filings (~1-5 GB)\n- Services: Ollama + RAPTOR API + Open WebUI\n- Command: `docker-compose -f docker-compose.dev.yml up`\n\n**Production (AWS EC2):**\n- Purpose: Full-scale deployment with complete dataset\n- Model: llama3-sec (49 GB) - requires 128 GB RAM\n- Data: Complete 51 GB SEC filings (1993-2024)\n- Services: Same as dev, with production configs\n- Command: `docker-compose -f docker-compose.prod.yml up -d`\n\n### Benefits of Docker Approach:\n\n1. **Reproducibility**: Same environment dev → production\n2. **Isolation**: Services don't conflict (different Python versions, dependencies)\n3. **Portability**: Works on laptop, EC2, teammate's machine\n4. **Scalability**: Easy to add services (monitoring, caching, etc.)\n5. **Version Control**: Infrastructure as code (`Dockerfile`, `docker-compose.yml`)\n6. **Easy Deployment**: `git pull && docker-compose up` deploys updates\n\n### Repository Structure with Docker\n```\nedgar_anomaly_detection/\n├── data/\n│   ├── external/         # Downloaded filings (gitignored)\n│   ├── processed/        # Chunked filings (gitignored)\n│   └── embeddings/       # ChromaDB files (gitignored)\n├── src/\n│   ├── Dockerfile        # RAPTOR API container definition\n│   ├── requirements.txt  # Python dependencies\n│   ├── data/\n│   │   ├── filing_extractor.py\n│   │   └── text_processor.py\n│   ├── models/\n│   │   ├── raptor.py\n│   │   ├── embedding_generator.py\n│   │   └── clustering.py\n│   └── api/\n│       └── main.py       # FastAPI server for RAPTOR\n├── notebooks/\n│   └── 01_project_plan.ipynb\n├── docker-compose.dev.yml   # Development setup\n├── docker-compose.prod.yml  # Production setup (EC2)\n├── .dockerignore\n├── .gitignore\n├── requirements.txt\n└── README.md\n```\n\n---\n\n## RAPTOR vs. Traditional RAG Comparison\n\n| Feature | Traditional RAG | RAPTOR RAG |\n|---------|----------------|------------|\n| Text Processing | Simple chunking | Recursive, hierarchical |\n| Clustering | None or basic | Multi-level (global + local) |\n| Summarization | None or single-level | Recursive, 3-level |\n| Context Selection | Similarity-based only | Cluster-aware + similarity |\n| Document Understanding | Flat representation | Hierarchical representation |\n| Knowledge Integration | Direct chunks only | Chunks + multi-level summaries |\n\n**Why RAPTOR for Financial Filings?**\n- Financial documents have hierarchical structure (sections, subsections, themes)\n- YoY analysis requires understanding both granular changes and high-level shifts\n- Boilerplate detection benefits from cluster analysis (repetitive language clusters together)\n- Complex queries need multi-level context (e.g., \"How did regulatory disclosures evolve?\")\n- Historical coverage (1993-2024) enables long-term trend analysis\n\n---\n\n## Success Metrics\n- [ ] Successfully process 90%+ of downloaded filings (1993-2024) into knowledge base\n- [ ] Clustering produces coherent, interpretable topic groups\n- [ ] Generated summaries accurately capture content at each hierarchical level\n- [ ] LLM queries return relevant, accurate responses with supporting evidence\n- [ ] System responds to queries in <10 seconds (including retrieval + generation)\n- [ ] Manual validation: Test 10 diverse queries across different topics and decades, verify accuracy\n- [ ] Docker deployment: Services start successfully on both dev and production\n- [ ] EC2 deployment: System runs stably for 7+ days without intervention\n\n---\n\n## Key Advantages of AI-First Approach\n1. **No Manual Feature Engineering**: LLM infers patterns from enhanced context (vs. building YoY diff algorithms)\n2. **Flexible Queries**: Users can ask arbitrary questions about ANY topic or section\n3. **Semantic Understanding**: Detects substantive changes even when wording differs\n4. **Scalable**: Adding new filings just requires re-running RAPTOR pipeline\n5. **Explainable**: LLM can cite specific sections supporting its conclusions\n6. **Historical Depth**: 31 years of data enables long-term trend analysis\n7. **Reproducible**: Docker ensures consistent environment across deployments\n8. **Cost-Effective**: EC2 instance can be stopped when not in use\n\n---\n\n## Technical Challenges & Mitigations\n\n### Challenge 1: Model Size vs. Available RAM\n- **Issue**: llama3-sec (49 GB) requires 35-40 GB RAM, local laptop has 32 GB\n- **Solution**: \n  - Development: Use gpt-oss (13 GB) on laptop\n  - Production: Deploy llama3-sec on AWS EC2 r6i.4xlarge (128 GB RAM)\n  - Benefits: Faster iteration locally, best quality in production\n\n### Challenge 2: Embedding Generation at Scale\n- **Issue**: Processing 31 years of complete filings (~51 GB) requires significant compute power\n- **Solution**: Use EC2 instance, batch processing, cache embeddings, process in chronological chunks\n\n### Challenge 3: Infrastructure Complexity\n- **Issue**: Managing multiple services (Ollama, RAPTOR API, WebUI, ChromaDB)\n- **Solution**: \n  - Docker Compose orchestrates all services\n  - Single command deployment: `docker-compose up`\n  - Services isolated and independently scalable\n\n### Challenge 4: Clustering Quality\n- **Issue**: Poorly defined clusters reduce summary quality\n- **Solution**: Use BIC for optimal cluster count, validate clusters manually on samples\n\n### Challenge 5: Context Window Limits\n- **Issue**: LLMs have token limits, can't ingest entire knowledge base\n- **Solution**: RAPTOR's hierarchical retrieval provides most relevant chunks + summaries\n\n### Challenge 6: Data Format Evolution\n- **Issue**: SEC filing formats changed significantly between 1993 and 2024 (SGML → HTML → XML)\n- **Solution**: Build robust parsing logic that handles multiple formats, test across time periods\n\n### Challenge 7: Processing Time\n- **Issue**: Unzipping and processing 51 GB of data could take significant time\n- **Solution**: Parallel processing where possible, start with subset (one year) to validate pipeline\n\n### Challenge 8: AWS Costs\n- **Issue**: Running r6i.4xlarge 24/7 costs ~$600-800/month\n- **Solution**: \n  - Stop instance when not in use\n  - Use spot instances for batch processing (60-90% discount)\n  - Process data once, serve queries on-demand\n\n---\n\n## Next Steps\n1. ✅ Download and test gpt-oss model locally\n2. [ ] Set up Docker Desktop on local laptop\n3. [ ] Create `docker-compose.dev.yml` for local development\n4. [ ] Test Ollama container with gpt-oss\n5. [ ] Copy RAPTOR class from FinGPT GitHub to `src/models/raptor.py`\n6. [ ] Complete Phase 2 with sample data and gpt-oss\n7. [ ] Provision AWS EC2 r6i.4xlarge instance\n8. [ ] Deploy llama3-sec on EC2 for production\n\n---\n\n## References\n- **FinGPT GitHub:** https://github.com/AI4Finance-Foundation/FinGPT\n- **FinGPT RAPTOR Implementation:** https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_FinancialReportAnalysis/utils/rag.py\n- **FinGPT Models on Hugging Face:** https://huggingface.co/AI4Finance-Foundation\n- **RAPTOR RAG Documentation:** https://deepwiki.com/AI4Finance-Foundation/FinGPT/5.1-raptor-rag-system\n- **SEC EDGAR API:** https://www.sec.gov/edgar/sec-api-documentation\n- **Ollama:** https://ollama.ai/\n- **Ollama Docker:** https://hub.docker.com/r/ollama/ollama\n- **Ollama Models Library:** https://ollama.com/library\n- **Arcee AI llama3-sec:** https://ollama.com/arcee-ai/llama3-sec\n- **Open WebUI:** https://github.com/open-webui/open-webui\n- **Docker Documentation:** https://docs.docker.com/\n- **AWS EC2 Instance Types:** https://aws.amazon.com/ec2/instance-types/"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}